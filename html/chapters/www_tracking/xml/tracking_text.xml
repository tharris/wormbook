<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.4CR2//EN"
                         "docbookx.dtd"
[
<!ENTITY fig1 SYSTEM 'WormBook_Gottschalk_tracking_fig01.jpg' NDATA JPG>
<!ENTITY fig1legend SYSTEM 'WormBook_Gottschalk_tracking_fig1leg.xml'>
<!ENTITY WormBook_Gottschalk_tracking_refs SYSTEM 'WormBook_Gottschalk_tracking_ref.xml'>
<!ENTITY pubmed_url 'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids='>
<!ENTITY crossref_url 'http://dx.doi.org/'>
]>
<article id="WormBook_Gottschalk_tracking_id">
<articleinfo>
<title>Keeping track of worm trackers<footnote>
<para>Edited by Oliver Hobert. Last revised March 2, 2012. Published September 10, 2012. This chapter should be cited as: Husson, S. J. et al. Keeping track of worm trackers (September 10, 2012), <emphasis>WormBook</emphasis>, ed. The <emphasis>C. elegans</emphasis> Research Community, WormBook, doi/10.1895/wormbook.1.156.1, <ulink url="http://www.wormbook.org">http://www.wormbook.org</ulink>.</para>
<para><emphasis role="bold">Copyright:</emphasis> &#x000A9; 2012 Steven J. Husson, Wagner Steuer Costa, Cornelia Schmitt and Alexander Gottschalk. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</para>
</footnote>
</title>
<author>
<othername>Steven J. Husson<superscript>2</superscript><superscript>*</superscript>, Wagner Steuer Costa<superscript>1</superscript><superscript>*</superscript>, Cornelia Schmitt<superscript>1</superscript>, Alexander Gottschalk<superscript>1</superscript><remark><footnote><para>To whom correspondence should be addressed. E-mail: <ulink url="mailto:a.gottschalk@em.uni-frankfurt.de">a.gottschalk@em.uni-frankfurt.de</ulink>; phone: +496979842518; fax: +496979876342518</para></footnote></remark>
<remark><footnote><para><superscript>*</superscript>both authors contributed equally</para></footnote></remark></othername>
<affiliation><orgname><superscript>1</superscript>Buchman Institute for Molecular Life Sciences (BMLS), and Institute of Biochemistry, Goethe-University, Max von Laue Strasse 15, D-60438 Frankfurt, Germany</orgname></affiliation>
<affiliation><orgname><superscript>2</superscript>Katholieke Universiteit Leuven, Research group of Functional Genomics and Proteomics, Naamsestraat 59, B-3000 Leuven, Belgium</orgname></affiliation>
</author>
</articleinfo>
<abstract>
<title><emphasis role="bold">Abstract</emphasis></title>
<para><emphasis role="bold"><emphasis>C. elegans</emphasis> is used extensively as a model system in the neurosciences due to its well defined nervous system. However, the seeming simplicity of this nervous system in anatomical structure and neuronal connectivity, at least compared to higher animals, underlies a rich diversity of behaviors. The usefulness of the worm in genome-wide mutagenesis or RNAi screens, where thousands of strains are assessed for phenotype, emphasizes the need for computational methods for automated parameterization of generated behaviors. In addition, behaviors can be modulated upon external cues like temperature, O<subscript>2</subscript> and CO<subscript>2</subscript> concentrations, mechanosensory and chemosensory inputs. Different machine vision tools have been developed to aid researchers in their efforts to inventory and characterize defined behavioral &#x201C;outputs&#x201D;. Here we aim at providing an overview of different worm-tracking packages or video analysis tools designed to quantify different aspects of locomotion such as the occurrence of directional changes (turns, omega bends), curvature of the sinusoidal shape (amplitude, body bend angles) and velocity (speed, backward or forward movement).</emphasis></para>
</abstract>
<sect1 id="sec1">
<title>Introduction</title>
<para><emphasis>C. elegans</emphasis> is an outstanding model organism for the study of neuronal circuits at the systems level. Exactly 302 neurons coordinate different behaviors such as feeding, mating, egg-laying, defecation, swimming and many subtle forms of locomotion on a solid surface. Due to its experimental amenability, the nematode has been an ideal animal for examining the genetic basis of behavior. Numerous phenotype-driven (forward and reverse) genetic screens have been performed, in search of defined behavioral abnormalities that can be assigned to specific genes. However, the effects of specific mutations on behavioral changes under study are often poorly described using imprecise terminology. In addition, as the phenotypes are difficult to quantify, lack of numerical data hinders robust statistical analysis. These screens mostly provide an informative description of the phenotype like &#x201C;Unc&#x201D; (uncoordinated) or similar descriptions (<link linkend="bib4">Brenner, 1974</link><!-- PMID: 4366476 -->). However, an uncoordinated worm can be &#x201C;coiling&#x201D;, &#x201C;kinky&#x201D;, &#x201C;sluggish&#x201D;, &#x201C;loopy&#x201D;, &#x201C;slow&#x201D; or might not move at all (<link linkend="bib18">Hodgkin, 1983</link><!-- PMID: 17246100 -->). These observations and phenotypical assignments are generally made by the experimenter and therefore involve the risk of subjectivity and non-uniformity, and also fail to address issues of phenotypic penetrance and degree of severity. Moreover, precise specification of the different aspects of locomotion that are affected, such as velocity, amplitude of the sinusoidal movement, angles of body bends and turning frequency cannot be easily provided through visual inspection by an individual researcher. The emergence of possibilities for tracking cells (particularly neurons; <link linkend="bib15">Faumont et al., 2011</link><!-- PMID: 21969859 -->), as well as optogenetic technologies that use light to gain exogenous control of defined cells (e.g., activation by the depolarizing Channelrhodopsin-2 [ChR2] and inhibition by the hyperpolarizing Halorhodopsin [NpHR] (<link linkend="bib3">Boyden et al., 2005</link><!-- PMID: 16116447 -->; <link linkend="bib12">Deisseroth, 2011</link><!-- PMID: 21191368 -->; <link linkend="bib22">Liewald et al., 2008</link><!-- PMID: 18794862 -->; <link linkend="bib26">Nagel et al., 2003</link><!-- PMID: 14615590 -->; <link linkend="bib25">Nagel et al., 2005</link><!-- PMID: 16360690 -->; <link linkend="bib39">Stirman et al., 2011</link><!-- PMID: 21240278 -->; <link linkend="bib45">Zhang et al., 2007</link><!-- PMID: 17410168 -->; <link linkend="bib21">Leifer et al., 2011</link><!-- PMID: 21240279 -->), has generated an even more pressing demand for neurobiologists to have robust computational methods for the quantification of behavior.</para>
<para>To address this problem, different machine vision approaches for automated behavioral analysis have been developed recently. Here we focus on software (and, to some extent, hardware) tools that quantitatively analyze locomotion behavior. We aim to provide a descriptive and currently comprehensive overview of different tracking systems and software developed by the worm community. We will discuss obvious advantages and disadvantages of the respective systems, including some &#x201C;how-to&#x0027;s&#x201D; to the extent that we can judge this either from our own experience or from the published work describing the systems. This review focuses mainly on the &#x201C;input&#x201D; and the &#x201C;output&#x201D; of behavior tracking systems: how many worms can be analyzed with the respective tool, and which behavioral parameters can be analyzed (<link linkend="table1">Table 1</link>). An in-depth description of the various programs/codes of the diversity of video analysis tools is beyond the focus of this review; these will rather be treated as &#x201C;black boxes&#x201D; and the reader is referred to the original publications. We will first provide a short history of worm tracking and mention how different video analysis tools have been used to quantitatively analyze <emphasis>C. elegans</emphasis> behavior in the past, to illustrate how the field has evolved. Next, we will give an overview of the major approaches available to-date and how, or if, these systems can be combined with optogenetic strategies that require precisely timed and synchronized illumination of the animal(s) with various colors of light.</para>
<table id="table1" frame="all" rules="all" cellpadding="0" cellspacing="0" width="75">
<title><emphasis role="bold">Table 1.</emphasis> Comparison of tracking systems</title>
<tgroup cols="9">
<colspec colnum="1" colname="col1"/>
<colspec colnum="2" colname="col2"/>
<colspec colnum="3" colname="col3"/>
<colspec colnum="4" colname="col4"/>
<colspec colnum="5" colname="col5"/>
<colspec colnum="6" colname="col6"/>
<colspec colnum="7" colname="col7"/>
<colspec colnum="8" colname="col8"/>
<colspec colnum="9" colname="col9"/>
<thead>
<row valign="bottom" rowsep="1">
<entry align="center">Name</entry>
<entry align="center">Worm Tracker 2.0 (Schafer lab)</entry>
<entry align="center">Nemo (Tavernarakis lab)</entry>
<entry align="center">The Parallel Worm Tracker (Goodman lab)</entry>
<entry align="center">OptoTracker (Gottschalk lab)</entry>
<entry align="center">Multimodal illumination and tracking system (Lu lab)</entry>
<entry align="center">CoLBeRT (Samuel lab)</entry>
<entry align="center">The Multi Worm Tracker (Kerr lab)</entry>
<entry align="center">Opto-mechanical system for virtual environments (Lockery lab)</entry>
</row>
</thead>
<tbody>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Single/Multi Worm</emphasis></entry>
<entry align="center">Single</entry>
<entry align="center">Single</entry>
<entry align="center">&#x003C;50</entry>
<entry align="center">&#x003C;50</entry>
<entry align="center">Single</entry>
<entry align="center">Single</entry>
<entry align="center">&#x003C;120</entry>
<entry align="center">Single</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Adaptable</emphasis></entry>
<entry align="center">Yes, supports x-y stages by three different vendors, as well as other camera systems (i.e. USB cameras)</entry>
<entry align="center">Yes, code open for changes, supports other camera systems (i.e. USB cameras)</entry>
<entry align="center">Yes, code open for changes, supports other camera systems (i.e. USB cameras)</entry>
<entry align="center">Yes, code open for changes, supports other camera systems (i.e. USB cameras)</entry>
<entry align="center">Yes, code open for changes, supports any projector and LabVIEW Vision compatible camera systems (i.e. USB cameras)</entry>
<entry align="center">Yes, code open for changes</entry>
<entry align="center">Yes, code open for changes, supports LabVIEW Vision compatible camera systems</entry>
<entry align="center">NA</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Optogenetic aplication</emphasis></entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes &#x2013; 3 wavelengths</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Illumination type</emphasis></entry>
<entry align="center">NA</entry>
<entry align="center">NA</entry>
<entry align="center">NA</entry>
<entry align="center">Whole field</entry>
<entry align="center">patterned; intensity adjustable &#x2013; each wavelength independently</entry>
<entry align="center">patterned</entry>
<entry align="center">Whole field</entry>
<entry align="center">patterned, intensity adjustable</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">X-Y Stage control</emphasis></entry>
<entry align="center">Yes</entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Measured parameters</emphasis></entry>
<entry align="center">Skeleton and outline</entry>
<entry align="center">Skeleton and outline</entry>
<entry align="center">Centroid</entry>
<entry align="center">Centroid</entry>
<entry align="center">Skeleton and outline</entry>
<entry align="center">Skeleton and outline</entry>
<entry align="center">Skeleton and outline</entry>
<entry align="center">Bright spot</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Camera resolution/support for other resolution (pixel)</emphasis></entry>
<entry align="center">1280 &#x00D7; 1024/Yes</entry>
<entry align="center">800 &#x00D7; 600/Yes</entry>
<entry align="center">640 &#x00D7; 480/No, downsized if greater</entry>
<entry align="center">640 &#x00D7; 480/No, downsized if greater</entry>
<entry align="center">320 &#x00D7; 240/Yes, but reduced fps at higher resolutions</entry>
<entry align="center">1280 &#x00D7; 1024 /NA</entry>
<entry align="center">2352 &#x00D7; 1728/No</entry>
<entry align="center">4 quandrants photomultipliertube</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Camera frequency/other supported (frames per second)</emphasis></entry>
<entry align="center">30/Yes</entry>
<entry align="center">40/Yes</entry>
<entry align="center">15/Yes</entry>
<entry align="center">15/Yes</entry>
<entry align="center">25/Yes</entry>
<entry align="center">50/Yes</entry>
<entry align="center">31/No</entry>
<entry align="center">NA-PMT</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Video stored</emphasis></entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">GUI</emphasis></entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Microscope required</emphasis></entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
<entry align="center">Yes</entry>
<entry align="center">No</entry>
<entry align="center">Yes</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Required Hardware</emphasis><!--<link linkend="tbl1fn1">--><emphasis role="bold"><superscript>&#x002A;</superscript></emphasis><!--</link>--></entry>
<entry align="center">X-Y Stage, camera</entry>
<entry align="center">Camera</entry>
<entry align="center">Camera</entry>
<entry align="center">Camera, light source with shutter, filters</entry>
<entry align="center">X-Y Stage, camera, projector, filters</entry>
<entry align="center">X-Y Stage, Laser, DMD Array, frame grabber, camera</entry>
<entry align="center">Camera, frame grabber, background light</entry>
<entry align="center">PMTand centering device</entry>
</row>
<row valign="top" rowsep="1">
<entry align="center"><emphasis role="bold">Required software</emphasis></entry>
<entry align="center">Java, ffdshow, MATLAB or <ulink url="http://www.wormbase.org/db/get?name=MCR;class=Cell" role="_blank">MCR</ulink></entry>
<entry align="center">MATLAB (R13) &#x002B; Image Processing Toolbox</entry>
<entry align="center">MATLAB (R13) &#x002B; Image Acquisition and Image Processing Toolbox</entry>
<entry align="center">MATLAB (R13) &#x002B; Image Acquisition and Image Processing Toolbox</entry>
<entry align="center">LabVIEW (&#x002B; Vision)</entry>
<entry align="center">MindControl (custom, C), MATLAB R2010a</entry>
<entry align="center">LabVIEW (&#x002B; Vision), C&#x002B;&#x002B; (custom), Java</entry>
<entry align="center">NA</entry>
</row>
<row valign="top">
<entry align="center"><emphasis role="bold">Cost estimation excluding software, computer and microscope (US&#x0024;)</emphasis></entry>
<entry align="center">3,500</entry>
<entry align="center">350</entry>
<entry align="center">350</entry>
<entry align="center">1600</entry>
<entry align="center">10,000</entry>
<entry align="center">16,000</entry>
<entry align="center">7,000</entry>
<entry align="center">Commercial version available (PhotoTrack, ASI)</entry>
</row>
<row valign="top"><!-- id="tbl1fn1" -->
<entry align="justify" namest="col1" nameend="col9">
<para>&#x002A; Some cameras require a frame grabber and PCI card to communicate with LabVIEW or MATLAB; USB- or fire-wire cameras should work w/o these</para>
<para>The authors thank Jeffrey N. Stirman for advice on assembling this table</para>
</entry>
</row>
</tbody>
</tgroup>
</table>
<sect2 id="sec2-1">
<title>Quantitative description of behavioral phenotypes using machine vision</title>
<para>Several machine vision programs follow a similar data processing strategy that first involves extraction of individual pictures from each frame of the movie file (<link linkend="figure1"><emphasis role="bold">Figure 1</emphasis></link>). The shape of the worm is then extracted from the background by a thresholding procedure. This operation allocates pixels to worm or background according to whether the intensity exceeds a defined threshold value thereby generating a two-color binary image (black and white). The next step is to depict the &#x201C;skeleton&#x201D; or &#x201C;spine&#x201D; of the animals from tail to head, often referred to as skeletonization of the worm shape (however, some systems do not use skeletonization, but segmentation of the worm shape). The one pixel thick line-image of the skeleton is further subdivided into different segments to allow computation of various parameters such as the center of mass (of the entire worm or for each segment) often referred to as the &#x201C;centroid&#x201D;, angles between two adjacent segments as a measure for body curvature, etc. In general, the velocities of individual worms are calculated as the rate of change in the location of their centroid or points along their skeleton over time, measured across the sequence of individual frames of the movie. Irrespective of the tracking program used, the key to success is to optimize the video quality such that the worms can be easily recognized as high contrast objects (dark) on a pale background (or <emphasis>vice versa</emphasis>). One should also take into account that the camera resolution, magnification used, and the quality of the imaging conditions jointly determine the accuracy of the measurements. When programmed for tracking several worms simultaneously, most trackers have the option for particle size exclusion. Through this option, dust particles are excluded, colliding worms are ignored and new tracks are automatically assigned once they separate again. This procedure is easier to implement and requires a much smaller amount of computation than keeping track of both animals.</para>
<literallayout></literallayout>
<mediaobject id="figure1" xreflabel="Figure 1">
<imageobject>
<imagedata entityref="fig1" width="4.8in" align="center"/>
</imageobject>
<textobject>
<phrase>figure 1</phrase>
</textobject> &fig1legend;</mediaobject>
<para>Although this method can also be applied for tracking <emphasis>C. elegans</emphasis> movement in a liquid environment, it is not optimal for quantification of swimming behavior. This fact led to the development of a covariance-based method, where the animals&#x2019; morphology is not measured but rather similarities between frames are searched and used for motion frequency calculation (see section 3).</para>
</sect2>
<sect2 id="sec2-2">
<title>History of <emphasis>C. elegans</emphasis> tracking systems</title>
<para>To our knowledge, the first video system, capable of tracking the movement of about 25 animals in real time at 1 Hz, was developed by Dusenbery and used to study chemotaxis (<link linkend="bib14">Dusenbery, 1985</link><!-- PMID: 4017556 -->). This software was programmed in BASIC09. Later, Dusenbery and colleagues devised a system that could track even 100s of animals, based on NIH Image software (<link linkend="bib13">Dhawan et al., 1999</link><!-- PMID: 10616193 -->). About the same time, another system, capable of tracking 50 animals, was used to characterize the neuropeptide Y receptor <ulink url="http://www.wormbase.org/db/get?name=NPR-1;class=Protein" role="_blank">NPR-1</ulink> and its role in aggregation behavior in the <ulink url="http://www.wormbase.org/resources/person/WBPerson42#01--9">Cori Bargmann</ulink> lab (<link linkend="bib11">de Bono and Bargmann, 1998</link><!-- PMID: 9741632 -->). Videos were analyzed using the &#x201C;DIAS&#x201D; software program that was initially developed to study basic crawling behaviors of amoeboid cells (<link linkend="bib33">Soll, 1995</link><!-- PMID: 8522423 -->). The speed of the objects under study was calculated between successive frames or as average speed over a longer period of time.</para>
<para>To study the role of pirouettes in chemotaxis behavior, another tracking system was developed and used to record the position, speed and turning rate of individual worms in the <ulink url="http://www.wormbase.org/resources/person/WBPerson383#01--9">Shawn Lockery</ulink> lab (<link linkend="bib28">Pierce-Shimomura et al., 1999</link><!-- PMID: 10531458 -->). The tracking system consisted of a computer-controlled motorized stage and a video camera mounted on a compound microscope. The system located the centroid of a worm under study and recorded x and y coordinates at a sampling rate of about 1 Hz. The worm was re-centered when it reached the edge of the field of view and the distance that the stage moved was recorded.</para>
<para>A similar tracking system was developed and used in the <ulink url="http://www.wormbase.org/resources/person/WBPerson554#01--9">William Schafer</ulink> lab to analyze egg-laying behavior (<link linkend="bib17">Hardaker et al., 2001</link><!-- PMID: 11745666 -->; <link linkend="bib44">Waggoner et al., 1998</link><!-- PMID: 9697864 -->). This prototype worm tracking system was further refined, in a joint venture between the <ulink url="http://www.wormbase.org/resources/person/WBPerson554#01--9">Schafer</ulink> and <ulink url="http://www.wormbase.org/resources/person/WBPerson625#01--9">Paul Sternberg</ulink> labs, for automated collection and analysis of <emphasis>C. elegans</emphasis> locomotion data. These systems were able to parameterize and classify different behavioral phenotypes of <emphasis>unc</emphasis> mutants by classification and regression tree (CART) algorithms. The tracker hardware and programming environment was estimated to cost about 10,000 US&#x0024; (excluding the requisite microscope, lighting and optics), software was coded in &#x201C;C&#x201D; programming language and it could operate at 2 Hz. In order to make worm-tracking accessible for general use in the <emphasis>C. elegans</emphasis> community, the system, with improved software, was described as a ready-to-use imaging system for standardized quantitative analysis of <emphasis>C. elegans</emphasis> behavior, complete with a parts-list, software packages and code to download and install (<link linkend="bib16">Feng et al., 2004</link><!-- PMID: 15331023 -->; <link linkend="bib9">Cronin et al., 2005</link><!-- PMID: 15698479 -->). This &#x201C;Wormtracker 1.0&#x201D; used a Cohu monochrome CCD camera (460 &#x00D7; 380 pixels) and a Daedal motorize stage controlled by a National Instruments controller and could operate at 30 Hz. Alternatively, video acquisition was done through a video cassette recorder and the movie then digitized afterwards. Software consists of four basic modules: (1) the Tracker, (2) a Converter to process raw images into a morphological skeleton, (3) a Lineup module to order backbone points from head to tail and (4) a &#x201C;Miner&#x201D; module for parameter extraction. The latter module analyzes specific features that define important parameters related to locomotion and morphology of the worm such as body posture, bending angles, movement and locomotion waveform. A total of 59 distinct features are measured, and the software is written with C/C++, LabVIEW 7.0 and MATLAB 13. Cronin and co-workers further described the metrics and application of their joint venture system with a toxicological assay as an example (<link linkend="bib9">Cronin et al., 2005</link><!-- PMID: 15698479 -->). The software was further refined in the <ulink url="http://www.wormbase.org/resources/person/WBPerson625#01--9">Sternberg</ulink> lab and can be downloaded as the Caltech Nematode Movement Analysis System (<ulink url="http://wormlab.caltech.edu/publications/download.html">http://wormlab.caltech.edu/publications/download.html</ulink>). As the system originally described by <link linkend="bib16">Feng et al. (2004)</link><!-- PMID: 15331023 --> was difficult to transfer to other labs, it has unfortunately not been widely used. An updated &#x201C;Wormtracker 2.0&#x201D; has been made available by the <ulink url="http://www.wormbase.org/resources/person/WBPerson554#01--9">Schafer</ulink> lab on the MRC-LMB website, including instructions on how to build and use the hardware, as well as software packages both for operation of the hardware, and for analysis of the obtained videos (<ulink url="http://www.mrc-lmb.cam.ac.uk/wormtracker/">http://www.mrc-lmb.cam.ac.uk/wormtracker/</ulink>). The system makes use of a digital microscope-type USB-camera (&#x201C;Dino-lite&#x201D;) that is able to acquire macro movies without the need for a compound microscope (see next paragraph).</para>
<para>Furthermore, there are various computational approaches for tracking and feature extraction of <emphasis>C. elegans</emphasis> in liquid environments. A system for quantifying the position, trajectory and body shape of worm populations in fluid environments has been developed by the <ulink url="http://www.wormbase.org/resources/person/WBPerson145#01--9">Monica Driscoll</ulink> lab (<link linkend="bib42">Tsechpenakis et al., 2008</link><!-- PMID: 18440900 -->), while the <ulink url="http://www.wormbase.org/resources/person/WBPerson1430#01--9">David Sattelle</ulink> lab presented a rapid method for automated counting of thrashing frequencies (<link linkend="bib5">Buckingham and Sattelle, 2009</link><!-- PMID: 19619274 -->). Another approach to quantify worm activity monitors the scattering of an infrared beam through a liquid culture of worms, and was used in the <ulink url="http://www.wormbase.org/resources/person/WBPerson8725#01--9">Diego Golombek</ulink> lab to measuring circadian rhythms (<link linkend="bib32">Simonetta and Golombek, 2007</link><!-- PMID: 17207862 -->). Moreover, the <ulink url="http://www.wormbase.org/resources/person/WBPerson1833#01--9">Randy Blakely</ulink> lab created a MATLAB script for automatic analysis of worm inactivity in liquid environment using a fast Fourier transform (FFT) to measure movement frequency (<link linkend="bib24">Matthies et al., 2006</link><!-- PMID: 16763028 -->).</para>
<para>If studying the involvement of a neuron or class of neurons in a particular behavior is of interest, optogenetic tools like ChR2 or NpHR for activating and silencing the cells acutely is a promising approach, particularly if combined with behavioral tracking. However, achieving single-cell expression of optogenetic tools is challenging, even when using recombinase-based approaches (<link linkend="bib10">Davis et al., 2008</link><!-- PMID: 18369447 -->; <link linkend="bib23">Macosko et al., 2009</link><!-- PMID: 19349961 -->). If single neuron expression cannot be obtained, restricting light to the region of the body where the cell of interest is localized may overcome this problem. High spatial and temporal precision has to be achieved in order to selectively address the cell of interest. This problem was tackled and solved by both the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Hang Lu</ulink> and <ulink url="http://www.wormbase.org/resources/person/WBPerson4184#013--9">Aravi Samuel</ulink> labs (<link linkend="bib21">Leifer et al., 2011</link><!-- PMID: 21240279 -->; <link linkend="bib39">Stirman et al., 2011</link><!-- PMID: 21240278 -->). Both systems were developed to illuminate distinct body regions, harboring the neurons of interest, in freely behaving animals. The respective neurons are, at least currently, defined and targeted by their anatomical position. Usually an area significantly larger than the size of the neuron&#x0027;s cell body is illuminated. This ensures that the neuron of interest is always illuminated for the defined period, even if the animal is moving quickly. In the future, it is conceivable that fluorescent markers expressed in a defined pattern (or within the cell of interest) may be used to address a specific cell. To some extent, the opto-mechanical tracker by the <ulink url="http://www.wormbase.org/resources/person/WBPerson383#013--9">Lockery</ulink> lab provides an approach to such devices (<link linkend="bib15">Faumont et al., 2011</link><!-- PMID: 21969859 -->).</para>
</sect2>
</sect1>
<sect1 id="sec1-2">
<title>Worm trackers</title>
<sect2 id="sec2-3">
<title>Nemo (<emphasis role="underline">Ne</emphasis>matode <emphasis role="underline">mo</emphasis>vement)</title>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson649#013--9">Nektarios Tavernarakis</ulink> group developed a simple yet powerful tool for analyzing <emphasis role="underline">ne</emphasis>matode <emphasis role="underline">mo</emphasis>vement (Nemo) without the need of a tracking device (<link linkend="bib43">Tsibidis and Tavernarakis, 2007</link><!-- PMID: 17941975 -->). Nemo is a modular software (written in MATLAB, release 13 or higher), that allows the user to specify which operation should occur on their data (software can be downloaded as supplementary material to <link linkend="bib43">Tsibidis and Tavernarakis, 2007</link><!-- PMID: 17941975 -->; <ulink url="http://www.biomedcentral.com/content/supplementary/1471-2202-8-86-s2.zip">http://www.biomedcentral.com/content/supplementary/1471-2202-8-86-s2.zip</ulink>). A GUI was designed to facilitate the processing and interpretation of the data and can be used to generate graphs and histograms of the computed parameters. We tested videos taken with different cameras at various settings and we could analyze the data without having to change the software. The software works with indexed images as input. These have to be obtained by the user through a 3<superscript>rd</superscript> party program like VirtualDub (<ulink url="http://virtualdub.sourceforge.net/">http://virtualdub.sourceforge.net/</ulink>). Although Nemo might be used with all image resolutions and magnifications, it is advisable to enhance these in order to reduce the error rate later in the quantification processes. First, all images are converted to gray scale and then a low-pass filter is applied in order to reduce noise. This image processing sequence allows Nemo to quantify color (RGB) images. However, care must be taken, since the resulting gray values must have a good worm to background contrast after processing. Nemo then searches in the first video frame for a single distinct object (i.e. the imaged worm) and computes its perimeter and skeleton using standard MATLAB Image Analysis Toolbox functions. In the following frames, only a region adjacent to the last position of the worm is computed, avoiding time consuming operations. Nemo also provides an algorithm to clear the skeleton from small branches. The skeleton is subdivided into a user-specified number of lines, representing segments of the worm. The coordinates of the center of mass of all lines as well as of the whole worm are recorded. In addition, the system is laid out such that reference points on the plate are taken into account to determine when the plate had to be moved to keep the worm within the field of view. With this information, the dataset is used to characterize the worm&#x0027;s speed, waveform (of the whole animal, or of only parts of the body), angles between two segments, thickness, distance between head and tail and trajectory.</para>
<para>Installing Nemo is straightforward and well-documented in the associated Readme.pdf file and the Algorithms.pdf document. Every function is clearly described, allowing non-MATLAB proficient users to understand how the program works.</para>
</sect2>
<sect2 id="sec2-4">
<title>Worm Tracker 2.0</title>
<para>The Worm Tracker 2.0 was released unofficially to the worm community in the beginning of 2007 by the <ulink url="http://www.wormbase.org/resources/person/WBPerson554#013--9">Schafer</ulink> lab and is frequently updated. The current release can be downloaded from the MRC-LMB website (<ulink url="http://www.mrc-lmb.cam.ac.uk/wormtracker/">http://www.mrc-lmb.cam.ac.uk/wormtracker/</ulink>). This single worm tracker operates with a Dino-lite digital microscope and camera (<ulink url="http://www.dino-lite.eu/">http://www.dino-lite.eu/</ulink>), practically rendering a compound microscope unnecessary. The tracker supports motorized stages of a variety of vendors as well as other cameras mounted on conventional microscopes, allowing one to adapt an existing microscope setup to the Worm Tracker 2.0 with ease. The software is fully based on graphical user interfaces (GUI) and is self-explanatory. The dedicated web page presents information ranging from the hardware needed to software installation all the way to protocols on how to optimize the NGM plates for recording videos with Worm Tracker 2.0. Additionally, the group also released a free Worm Analysis Toolbox for MATLAB, specifically developed to analyze videos taken with the Worm Tracker 2.0.</para>
<para>The tracker acquires the image stream from the camera and recognizes the worm and its centroid. It controls the motorized stage to position the worm&#x0027;s centroid at the center of the image as soon as the worm reaches previously selected boundaries in the field of view. The position of the stage is recorded in a separate file as well as the timing of the stage movement. This information is used in the Worm Analysis Toolbox to identify the frames where the stage moved. The blurred images caused by stage movement are dropped from the analysis, which is a drawback compared to systems that continuously re-center the animal with small motion increments, and may lead to loss of (some) information. However, the system also allows moving the microscope instead of the stage, thus leaving the worm completely non-agitated, in case vibrations due to stage movement, which may be sensed by the animal, are a concern. The stage&#x0027;s immobility also permits tracking of single worms swimming. The small form factor, due to the lack of a microscope, and low acquisition cost makes this system a good choice for locomotion studies without embedded optogenetic stimulus application.</para>
<para>The Worm Analysis Toolbox can be used (off-line) for automated segmentation of the worm&#x0027;s image and subsequent feature extraction. The current release supports analysis of the worm&#x0027;s area, length, width, thickness, transparency and the brightness of head and tail. In addition, the Toolbox allows visual confirmation of the extracted data as well as debugging in case of errors with three helper tools. All functions are widely commented in a user manual.</para>
<para>Installation of the Worm Tracker 2.0 requires a Java environment, while the Toolbox requires either a MATLAB installation or a MATLAB Compiler Runtime Environment. The latter can be downloaded for free with the Worm Analysis Toolbox. The group released an example folder for the Analysis Toolbox containing a video and all tracked data, allowing one to test the program prior to purchasing the required hardware.</para>
<para>Although the Worm Tracker 2.0 is, at the time of writing, a work in progress, it can already be reliably used by the worm community. The software is free to use (provided acceptance of a software release agreement issued by the MRC) and the cost of the hardware needed is the smallest among the single worm trackers with automated stage control. The ease of installation and operation are further reasons to consider this system in laboratories that wish to start automatic behavioral assays. Unfortunately, however, at least the current version of the tracker does not support synchronized control of other devices (for example by using TTL pulses). Therefore, in the current form, this system is not recommended when combining worm tracking with optogenetic tools that require accurate programming of illumination protocols to be synchronized with the videos taken.</para>
</sect2>
<sect2 id="sec2-5">
<title>The Parallel worm tracker and OptoTracker</title>
<para>The Parallel worm tracker (PWT) was designed by the <ulink url="http://www.wormbase.org/resources/person/WBPerson209#013--9">Miriam Goodman</ulink> lab as a high-throughput platform to analyze the locomotion (mainly centroid speed) of up to 50 worms in parallel, for example, enabling quantification of drug-induced paralysis (<link linkend="bib30">Ramot et al., 2008</link><!-- PMID: 18493300 -->). The overall setup is similar to Nemo and all software packages are implemented in MATLAB (<ulink url="http://wormsense.stanford.edu/tracker/">http://wormsense.stanford.edu/tracker/</ulink>). Video capture is performed using the VideoCapture module, which is compatible with any camera capable of communicating with the MATLAB Image Acquisition Toolbox. Thereafter tracking is performed off-line by the WormTracker module. The tracker records the centroid position of tens of worms in sequential movie frames extracted from uncompressed grayscale (8 bit) .avi format video files with a resolution of 640 &#x00D7; 480 pixels. If two animals collide, the tracking of each animal is terminated. New tracks are assigned to the animals once they separate. WormTracker only stores those tracks for analysis that persist for more than a certain amount of frames. Next, the WormAnalyzer package provides tools for analysis and display of generated tracks. It is capable of automatic detection of turning events or pirouettes, as described earlier by <link linkend="bib6">Chalasani et al. (2007)</link><!-- PMID: 17972877 -->, measures the speed of individual worms and can use these data to measure the fraction of worms that are paralyzed by drug application. The preferences for each module are stored in an Excel file and analyzed data can be exported as figures or tables.</para>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson2633#013--9">Alexander Gottschalk</ulink> lab, i.e. the authors of this review, is interested in combining worm tracking with optogenetics-assisted modulation of neuronal activity. Thus, the parallel worm tracker software was implemented with a program that allows controlling an electronic shutter, blocking out light from, e.g., an HBO (Hg = mercury B = luminance O = unforced cooling) light source, through the LPT (parallel) port of the computer by sending a TTL (Transistor-Transistor Logic) pulse. In this way, a series of predefined light pulses can be applied to the worms for optogenetics-based behavioral studies. A user-friendly interface for this &#x201C;OptoTracker&#x201D; has also been generated, in which individual users can load the different modules (VideoCapture, WormTracker, WormAnalyzer and the additional Shutter module) with their saved preferences, to export acquired raw data into an Excel file for further characterization.</para>
<para>The source code for the parallel worm tracker and a user manual can be downloaded from <ulink url="http://wormsense.stanford.edu/tracker/">http://wormsense.stanford.edu/tracker/</ulink>, while the OptoTracker variant can be found on the <ulink url="http://www.wormbase.org/resources/person/WBPerson2633#013--9">Gottschalk</ulink> lab website (<ulink url="http://www.biochem.uni-frankfurt.de/index.php?id=236">http://www.biochem.uni-frankfurt.de/index.php?id=236</ulink>), together with an installation and user manual. This system provides a simple, though efficient, solution for multi worm tracking and only requires MATLAB software, including the Image Acquisition and the Image Processing Toolboxes, a digital video camera and a microscope (if at all).</para>
</sect2>
<sect2 id="sec2-6">
<title>The Multi Worm Tracker</title>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson317#013--9">Rex Kerr</ulink> and <ulink url="http://www.wormbase.org/resources/person/WBPerson509#013--9">Catharine Rankin</ulink> labs recently described a multi-worm tracking system that was used to analyze spontaneous movement on food, chemotaxis and habituation of response to tap stimulation. The software package consists of real-time image-analysis software, called &#x201C;Multi-Worm Tracker (MWT)&#x201D;, and an additional program, &#x201C;Choreography&#x201D;, for off-line analysis of different behavioral parameters (<link linkend="bib40">Swierczek et al., 2011</link><!-- PMID: 21642964 -->). The MWT is used to provide basic features of the worm including its position and outline, whereas Choreography has to be employed to extract additional features after selecting the appropriate objects. The system can conveniently track up to 120 animals per plate. More animals can be reliably tracked when dropping frames during the real time processing of the MWT; conversely, one might expect to have more worms tracked by increasing the processing power of the computer used.</para>
<para>The core of the hardware system is a high-end digital camera (Falcon 4M30, 4 Megapixel (2352 &#x00D7; 1728), 31 fps, 10 bit digital camera equipped with a 25 mm modular focus block) that renders the use of a microscope system unnecessary. A special frame grabber is used to acquire the uncompressed video stream during experiments. As the camera streams the video at full rate of 7.2 gigabytes per minute, only the tracking files are stored during an experiment. It is recommended to use a stand for the camera and to build a stage to hold the Petri dish in front of the camera lens. The camera&#x0027;s high resolution can visualize the animals with 24 &#x00B5;m/pixel resolution, which reduces the measurement errors due to pixel flickering. The tracking procedure searches for animals in the first frame of a movie and draws a box around these. In the following frames, only the area of the boxes will be tracked; all other pixels are skipped. The position of the worm is stored and the box around the animal is refreshed for the next frame. After a defined amount of frames, a subpart of the whole image is searched for new worms entering the field of view, creating new boxes where needed. These subparts are cycled through without decreasing the tracking capability. It is important to achieve homogenous background lighting as parts of the field of view will not be addressed due to over- or underexposure when unevenly illuminated. It is also crucial to use synchronized worms: since animal recognition is performed through particle size analysis, all pictured animals should have the same size. When two animals collide, their size is added and counted as one particle. These animals are ignored by the MWT until they separate and are again identified as single worms by the animal search algorithm. This might be an issue when tracking higher number or animals simultaneously for longer periods of time.</para>
<para>All required software and documentation has been published in Sourceforge (<ulink url="http://sourceforge.net/projects/mwt/">http://sourceforge.net/projects/mwt/</ulink>). The package is coded in C++ and based on LabVIEW (MWT) and JAVA (Choreography). Unfortunately, Choreography has no GUI and works with JAVA commands. In addition to multi-worm tracking and analysis of the data, the software also allows the control of up to three stimulus-presenting systems. This permits multi-worm tracking while giving a computer-controlled mechanical tap to the plate or presenting a &#x201C;puff&#x201D; of air over the NGM Petri dish. The system can also be used to challenge animals with a light pulse (e.g. delivered by a ring of LEDs), for whole field optogenetics experiments (Adriel and Rankin, unpublished). Furthermore, this system may be used to quantify high-throughput swimming assays.</para>
</sect2>
<sect2 id="sec2-7">
<title>Multimodal illumination and tracking system for optogenetic analyses of circuit function</title>
<para>The Tracker developed recently in the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Lu</ulink> lab resolves an hitherto existing problem in optical stimulus delivery for optogenetic manipulation of animal behavior: patterned illumination with various wavelengths at the same time to target several distinct optogenetic tools in the same animal, addressing different nodes of a neuronal network (<link linkend="bib39">Stirman et al., 2011</link><!-- PMID: 21240278 -->; <link linkend="bib38">Stirman et al., 2012</link><!-- PMID: 22240583 -->). The system combines an inverted microscope and a commercially available video projector for multi-color illumination of physically separated cells. At the same time, the position of the worm is tracked by a movable x-y stage and various behavioral parameters, such as velocity and body curvature, can be analyzed. The spatial resolution of the presented system has been calculated to 14 &#x00B5;m/pixel at 25 Hz, depending on the objective used.</para>
<para>The software saves two video streams: the originally acquired video of the behaving animal, and a parallel video stream with information regarding the pattern of light used to stimulate optogenetic tools expressed in particular neurons. There is the option to merge both videos, where the area being optically activated is marked in the color of the channel used for stimulation (red, green, blue, or combinations of these 3 colors). The calibration, steering and analysis programs are coded in LabVIEW and all required software can be downloaded as supplements accompanying the paper (<link linkend="bib39">Stirman et al., 2011</link><!-- PMID: 21240278 -->; <ulink url="http://www.nature.com/nmeth/journal/v8/n2/extref/nmeth.1555-S10.zip">http://www.nature.com/nmeth/journal/v8/n2/extref/nmeth.1555-S10.zip</ulink>). A step-by-step protocol to make the essential optical changes to the off-the-shelf LCD projector is also available, as well as instructions on how to use the different software packages required (<link linkend="bib38">Stirman et al., 2012</link><!-- PMID: 22240583 -->). Briefly, the multimodal illumination and tracking system consists of three main LabVIEW programs. The first one is used for calibration prior to measurements. The second program performs the real-time tracking and patterned multimodal illumination while recording the movies. The third software package (consisting of different sub-programs) is used for post processing, i.e., head encoding, complete video analysis, and analysis of multiple data files in batch mode and presentation.</para>
<para>The major advantage of the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Lu</ulink> system is that up to three different colors of light, each with 256 independent levels of intensity, can be used simultaneously which is essential to combine different optical tools with shifted action spectra; for example, ChR2 (major activation peak at 460 nm), Mac (peak at 535 nm) and NpHR (peak at 590 nm) (<link linkend="bib7">Chow et al., 2010</link><!-- PMID: 20054397 -->; <link linkend="bib26">Nagel et al., 2003</link><!-- PMID: 14615590 -->; <link linkend="bib45">Zhang et al., 2007</link><!-- PMID: 17410168 -->). The applied wavelength depends on the installed band pass filter; therefore, changing the stimulation color is as convenient and cost-effective as possible. Similarly important is the fact that the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Lu</ulink> system can be assembled from a relatively cheap, off-the-shelf commercial video projector (less than 3000 US&#x0024; for a projector, band pass filters and the LabVIEW license). The software uses a common USB camera capable of communication with the LabVIEW Vision add-on and 25 Hz acquisition.</para>
</sect2>
<sect2 id="sec2-8">
<title>CoLBeRT: control locomotion and behavior in real time</title>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson4184#013--9">Samuel</ulink> lab published a system, &#x201C;CoLBeRT&#x201D;, for controlling locomotion and behavior in real time. CoLBeRT uses a digital micromirror device (DMD, from Texas Instruments) to reflect a diode-pumped solid-state laser in order to achieve selective illumination (<link linkend="bib21">Leifer et al., 2011</link><!-- PMID: 21240279 -->). High light intensities come with the drawback of only one color of light being used at the same time, hampering the simultaneous use of different optogenetic tools, at least in the published version of the system. The tracking and illumination setup operates at 50 Hz and is excellent for analyzing body curvature. The DMD has a spatial limit of 5 &#x00B5;m for the minimal area that may be accessed through CoLBeRT. This minimal area is larger for fast moving animals, i.e. about 30 &#x00B5;m for swimming worms. The <ulink url="http://www.wormbase.org/resources/person/WBPerson4184#013--9">Samuel</ulink> lab used viscous solutions to slow down the locomotion of the animals under study, allowing higher spatial accuracy in directing light at the respective cells.</para>
<para>The &#x201C;MindControl&#x201D; software used to track a worm and create illumination patterns in real time is written in the &#x201C;C&#x201D; programming language and is also available together with documentation through &#x201C;github&#x201D; (<ulink url="http://github.com/samuellab/mindcontrol and https://github.com/samuellab/mindcontrol-analysis">http://github.com/samuellab/mindcontrol and https://github.com/samuellab/mindcontrol-analysis</ulink>). MindControl stores two video sequences, an original stream and one with annotations regarding the optogenetic stimulation. During an experiment, a GUI allows one to change the optogenetic stimulation in real time as well as delivering manual stimulations. The raw data is stored in YAML format (a human-readable format to serialize/store data). This dataset is then processed in MATLAB to retrieve a quantitative analysis of the experiment, for instance, with kymographs (a graph of spatial position vs. time) of the worm&#x0027;s locomotion.</para>
<para>In conclusion, this system is the fastest real-time single worm tracker to date, capable of spatially restricted optogenetic manipulations. The disadvantage of CoLBeRT, however, is that only one color of light can be used at the same time. Also, the acquisition cost is considerably higher compared to the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Lu</ulink> system (see <link linkend="table1">Table 1</link>).</para>
</sect2>
<sect2 id="sec2-9">
<title>The opto-mechanical system for imaging or manipulation of neuronal activity in freely moving animals</title>
<para>Non-invasive neuronal manipulation <emphasis>via</emphasis> optogenetics in freely moving animals, while simultaneously tracking evoked behaviors (as discussed above), has been a significant step forward in the analysis of neural network function. In addition to optogenetics-assisted manipulation, imaging of neuronal activity in untethered, freely moving animals is a major technical challenge when combined with tracking and quantification of locomotory behavior. The image-free opto-mechanical system developed in the <ulink url="http://www.wormbase.org/resources/person/WBPerson383#013--9">Lockery</ulink> lab promises to address both approaches (<link linkend="bib15">Faumont et al., 2011</link><!-- PMID: 21969859 -->). This system can be used to create virtual environments by optogenetic activation of sensory neurons, or to image activity in identified neurons at high magnification. The system uses two light paths with different magnifications. The first path, with lower magnification, is used for behavioral analysis and records the image of the animal in a standard gray-scale movie. The second path has a higher magnification (typically 63x-100x) and is used for Ca<superscript>2+</superscript>-imaging and the actual tracking procedure. For this purpose, a beam splitter redirects a small amount (20%) of the light at the Ca<superscript>2+</superscript>-imaging camera to a four-quadrant photomultiplier tube (PMT). The four analog signal intensities are directed to a motorized stage controller, which regulates the speed of the servo motors in order to center the brightest spot to the center of the PMT. This approach thus requires a trackable bright spot, for instance, a cell expressing a fluorescent protein marker. As no software processing is required for stage control, i.e., this part is an all-analog system, this is the fastest tracking system available to date, allowing one to track neurons in animals thrashing in liquid. The combination of two recordings with different magnifications allows worm tracking in parallel with Ca<superscript>2+</superscript>-imaging in single neurons, which is a feature not commonly seen in tracking systems. The system can also create a so-called &#x201C;virtual environment&#x201D; by projecting light into a user-defined pattern. This projection can be used to control activity of (sensory) neurons expressing optogenetic tools, e.g., mimicking an aversive stimulus by specifically expressing channelrhodopsin in, and photoactivating the, polymodal aversive neuron ASH. The instructions needed to build the centering device have been published (<link linkend="bib15">Faumont et al., 2011</link><!-- PMID: 21969859 -->) and a commercial version is available (PhotoTrack, Applied Scientific Instrumentation).</para>
</sect2>
<sect2 id="sec2-10">
<title>Further systems allowing tracking and Ca<superscript>2+</superscript> imaging in semi-restrained or freely behaving animals</title>
<para>As briefly described above, when one is interested in the neuronal basis of behavior, one ideally wants to track and quantify behavior, and at the same time record the activity of neurons involved in generating the behavior. Thus, several systems have been described that allow tracking of semi-restrained or freely behaving animals, and recording Ca<superscript>2+</superscript> signals in transgenic neurons. These systems have been successfully used, but, to our knowledge, not described in sufficient technical detail to be adopted by others. Thus, we can just mention them here and refer the reader to the respective researchers if they are interested in setting up these systems themselves.</para>
<para>A first approach somewhat achieving this goal was described by the <ulink url="http://www.wormbase.org/resources/person/WBPerson4184#013--9">Samuel</ulink> lab (<link linkend="bib8">Clark et al, 2007</link><!-- PMID: 17553981 -->). They imaged fluorescent signals from the AFD thermosensory neuron expressing the ratiometric Ca<superscript>2+</superscript> sensor cameleon. This was done at intermediate magnification in animals whose tails were glued, but whose heads were free to move within a thermal gradient. They also tracked animals freely moving in such a gradient, at low magnification, by tracking the fluorescent neuron, in this case using a joystick-controlled x-y translational stage. Later the track of the animal was extracted from the recorded stage (and relative neuron) positions.</para>
<para>A different system, tracking an animal moving on an open NGM plate automatically, and acquiring Ca<superscript>2+</superscript> signals from a neuron of interest, was developed by the <ulink url="http://www.wormbase.org/resources/person/WBPerson11210#013--9">Didier Chatenay</ulink> and <ulink url="http://www.wormbase.org/resources/person/WBPerson554#013--9">Schafer</ulink> labs, who imaged the AVA backward command motor neuron in freely moving animals (<link linkend="bib2">Ben Arous et al., 2010</link><!-- PMID: 20096306 -->). This system uses two cameras, one for acquiring an image of the animal, which is used to track locomotion behavior and to re-center the stage (using a low magnification objective), and another to record fluorescent signals of the AVA neuron expressing the cameleon sensor (using higher magnification). The software operates at roughly 7 Hz, and is based on an ImageJ script, thus no costly commercial software package is required.</para>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson720?query=zhen#013--9">Mei Zhen</ulink> lab developed a similar system that tracks fluorescent cells and animal behavior, based on freely available software (MicroManager and ImageJ), and operating at up to 20 Hz (<link linkend="bib20">Kawano et al., 2011</link><!-- PMID: 22099460 -->). This system allows one to image multiple command (or &#x201C;premotor&#x201D;) interneurons as well as ventral cord motor neurons expressing cameleon sensors in movement-restricted animals (under a cover slip, effectively slowing down but not preventing locomotion).</para>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson2026#013--9">Shawn Xu</ulink> and <ulink url="http://www.wormbase.org/resources/person/WBPerson4687#013--9">Zhaoyang Feng</ulink> labs devised a tracker to study whether motor activity decline might be used as a lifespan predictor (<link linkend="bib19">Hsu et al., 2009</link><!-- PMID: 18255194 -->). Their system is based on a stereomicroscope with a digital camera and a motorized stage. Custom software tracks the animal at 2 Hz for five minutes. The software was briefly described in the original publication, but was not published for further use. This system was further developed to allow Ca<superscript>2+</superscript> imaging, termed CARIBN (Ca<superscript>2+</superscript> ratiometric imaging of behaving nematodes), and allows tracking as well as Ca<superscript>2+</superscript> imaging using GCaMP3 (<link linkend="bib29">Piggott et al., 2011</link><!-- PMID: 22078887 -->). They use DsRed (non-responsive to Ca<superscript>2+</superscript>) as a control for motion or focusing artifacts. In addition, the system may be suited for optogenetics experiments, while imaging Ca<superscript>2+</superscript> at the same time. However, as the Ca<superscript>2+</superscript> imaging light is also used for ChR2 activation, measurement of baseline fluorescence for Ca<superscript>2+</superscript> imaging is not possible and must be controlled in a separate experiment by imaging additional animals not expressing ChR2. The CARIBN II system adds the option of controlling the z-axis, allowing automatic focusing of the pictured neurons, as well as z-sectioning (<link linkend="bib46">Zheng et al., 2012</link><!-- PMID: 22108336 -->). The latter function allows CARIBN II to image multiple neurons concomitantly. Both versions of CARIBN are available upon request.</para>
</sect2>
<sect2 id="sec2-11">
<title>Behavioral arenas</title>
<para>Conventional trackers for freely moving animals on solid surfaces do not allow one to present odors in a spatially and temporally controlled manner. To quantitatively understand chemosensory behaviors, the <ulink url="http://www.wormbase.org/resources/person/WBPerson42?query=bargmann#013--9">Bargmann</ulink> group recently described a microfluidics device allowing creation of precise spatiotemporal chemical environments while monitoring the resulting behavioral output (<link linkend="bib1">Albrecht and Bargmann, 2011</link><!-- PMID: 21666667 -->). This &#x201C;behavioral arena&#x201D; consists of a 4 cm<superscript>2</superscript> polydimethylsiloxane (PDMS) surface containing a structured micropost array (hexagonally arranged, 200 &#x00B5;m diameter pillars, separated by 100 &#x00B5;m) through which the nematodes can crawl. The arena height is set to 70 &#x00B5;m, which is roughly the diameter of a young adult animal. These parameters match the wavelength of normal crawling behavior on an agar substrate. The microfluidic chip has different inlets for stimulus inflow, a worm loading port with variable entry points and an outflow channel. Furthermore, the device boundaries are smooth in order to minimize the animal&#x0027;s tendency to explore sharp corners. The different stimulus inlets are controlled by valves, allowing different configurations of odor stimulation by generating gradients that mix two odor concentrations or through temporal control by timed opening of a valve. The worm entry point to the arena is variable depending on the device used. The system is equipped with a camera for recording the animal&#x0027;s behavior during stimulus presentation. The image analysis is performed offline, using MATLAB code that is partially based on the parallel worm tracker (<link linkend="bib30">Ramot et al., 2008</link><!-- PMID: 18493300 -->). The system performs automated behavioral classification based on the identification of five primary locomotory states: forward locomotion (straight or curved), pause, reversal, pirouette reverse (the reversal before an omega turn) and pirouette forward (the subsequent resolution of the omega turn). Data can be presented in stimulus-aligned ethograms in which the five states are color-coded and plotted over time.</para>
<para>The single-layer PDMS chips can be easily re-designed and the microfluidics system works with standard Luer valves. Second-generation devices promise high-throughput behavioral analysis. Concomitant separate population measurement is achieved by dividing the arena with worm barriers. A 2- and a 4-arena multiplexed device have been designed with multiple fluid inlets. These allow up to four different populations to be challenged with four unique stimuli simultaneously during one experiment.</para>
</sect2>
<sect2 id="sec2-12">
<title>The WormLab, a commercially available worm tracker</title>
<para>MicroBrightField Inc. developed a commercially available worm tracker called WormLab (<ulink url="http://www.mbfbioscience.com/wormlab">http://www.mbfbioscience.com/wormlab</ulink>). At the time of writing, the software is available for pre-purchase, with the option for a complete system including microscope, video camera and motorized stage. The software features include tracking of selected worms through their centroid, head or tail markers. The analysis comprises the worm&#x0027;s velocity, position, area, direction and wavelength, as well as the track&#x0027;s length.</para>
</sect2>
</sect1>
<sect1 id="sec1-3">
<title>Worm trackers optimized for liquid environments</title>
<para><emphasis>C. elegans</emphasis> thrashing and swimming behavior have been effectively tracked by morphological analysis, but this approach requires high computing power. The <ulink url="http://www.wormbase.org/resources/person/WBPerson1430#013--9">Sattelle</ulink> lab published a new approach to quantify thrashing assays without morphometry (<link linkend="bib5">Buckingham and Sattelle, 2009</link><!-- PMID: 19619274 -->). Their software is based on covariance analysis. First, the background is extracted from the images through a technique employing Principal Component Analysis (the background is represented by the maximum covariance and is therefore coded by the first principal component). Then a covariance matrix is computed for all frames. This matrix shows frames that are statistically significantly similar to each other. Counting the amount of frames between two similar ones allows one to identify the time needed to complete a full swing during thrashing, which ultimately leads to the thrashing frequency. This system was conceived for high-throughput analysis of worm swimming behavior, but requires one worm per video to be analyzed. The <ulink url="http://www.wormbase.org/resources/person/WBPerson4687#013--9">Feng</ulink> lab further improved the system with a program capable of controlling a motorized stage (<link linkend="bib46">Zheng et al., 2011</link><!-- PMID: 22108336 -->). This software automatically records a movie of each well in a multi-well plate with parameters set by the user. The improved system combines efficient thrashing assay analysis with high-throughput screening. The source code is written in C and compiled in LabWindows (NI, Austin, TX, USA). The thrashing analysis core program does not require any specific hardware. The only requirement for the video is that only one animal is depicted. The system is easily deployed and the instructions for the hardware needed for the high-throughput measurements are available upon request by the authors.</para>
<para>Furthermore, the <ulink url="http://www.wormbase.org/resources/person/WBPerson1833#013--9">Blakely</ulink> lab created a system for automatic analysis of worm (in)activity in fluids. A MATLAB script automatically analyzes the thrashing frequency of a single worm through a Fast-Fourier transform of the movement frequency (<link linkend="bib24">Matthies et al., 2006</link><!-- PMID: 16763028 -->). Although the software is not published online, the authors do share it upon request.</para>
</sect1>
<sect1 id="sec1-4">
<title>Additional analysis tools for quantifying <emphasis>C. elegans</emphasis> behavior</title>
<para>The following tools do not contain instructions for controlling x-y stages, thus they should be considered as stand-alone video analysis tools that require videos or images as input.</para>
<sect2 id="sec2-13">
<title>Eigenworms: Low-dimensional superposition of principal components</title>
<para>Greg J. Stephens from the <ulink url="http://www.wormbase.org/resources/person/WBPerson8536#013--9">William Ryu</ulink> lab showed that the space of shapes adopted by the worm can be described with just four &#x201C;elementary shapes&#x201D;, or &#x201C;eigenworms&#x201D; that provide a quantitative description of worm behavior, accounting for 95% of the variance in N2 shapes (<link linkend="bib35">Stephens et al., 2008</link><!-- PMID: 18389066 -->). As the worm&#x0027;s shape determines its motion, characterization of the shape dynamics provides insights into locomotion behavior. Variations along the eigenworms thus offer a quantitative characterization of classical locomotion parameters such as forward crawling, reversals, omega bends etc. For this work, they built a homemade tracking system and used MATLAB to capture and process the images in order to calculate the eigenworms. Images of worms were first skeletonized to a single-pixel thick backbone that was segmented into 101 parts such that 100 angles between these segments could be calculated in order to deduce the four eigenworms or &#x201C;modes&#x201D;. The first two modes are sinusoidal-like oscillations that describe the orthogonal phases of a wave along the body. The third mode is related to the curvature and is thus used to identify or describe turns or omega bends. The fourth mode contributes to the shape of the head and tail region of the worm. One should interpret this approach as a projection or reduction of motor behaviors onto four templates or parameters with variable strengths. Mapping the dynamics of the shape space to the trajectory of the moving worm can reveal subtle differences in locomotion (<link linkend="bib36">Stephens et al., 2010</link><!-- PMID: 21103370 -->; <link linkend="bib34">Stephens et al., 2011</link><!-- PMID: 21502536 -->).</para>
<para>The approach to calculate the eigenworms can easily be executed as a stand-alone MATLAB-based program and virtually any movie file can be analyzed (after thresholding and transformation into individual frames by other video processing programs like VirtualDub). The use of a tracking system is not required as one is only interested in the &#x201C;space shape&#x201D; of the worms. When using this software tool, one can just move the plate by hand to keep the worm in the center of the field of view. The backbone length also represents an accurate calculation of the length of the worm. This approach was used when measuring body contractions or elongations evoked by depolarization of muscle cells or cholinergic neurons <emphasis>via</emphasis> optogenetic tools (<link linkend="bib22">Liewald et al., 2008</link><!-- PMID: 18794862 -->). Thereafter, a microfluidics device was developed by the <ulink url="http://www.wormbase.org/resources/person/WBPerson3658#013--9">Lu</ulink> lab for high-throughput automation of body length measurements to investigate synaptic transmission (<link linkend="bib37">Stirman et al., 2010</link><!-- PMID: 20538016 -->), utilizing the algorithm devised by Stephens to analyze worm length (<link linkend="bib35">Stephens et al., 2008</link><!-- PMID: 18389066 -->).</para>
</sect2>
<sect2 id="sec2-14">
<title>An analysis tool for the description of bending angles during swimming or crawling</title>
<para>Body bends during crawling and swimming behaviors are best displayed through kymographs of the worm&#x0027;s body angles (or curvature) with respect to time. Although many trackers have an option to analyze these features, it is unnecessary to install an expensive system if one wishes only to address these aspects of locomotion. The <ulink url="http://www.wormbase.org/resources/person/WBPerson1263#013--9">Steven McIntire</ulink> lab created video analysis software capable of displaying worm bending as kymographs/curvature matrices (<link linkend="bib27">Pierce-Shimomura et al., 2008</link><!-- PMID: 19074276 -->). The software is programmed as a custom image analysis algorithm in ImagePro (Media Cybernetics). Videos are recorded with a resolution of 2.9 &#x00B5;m/pixel at a frequency of 30 Hz. The software recognizes the animal and describes it through its midline. This skeleton is subdivided into 13 segments and the angles between them are color-coded to form an image of the angles over time. The columns created for each frame of the video are connected to form the curvature matrix. This method is advantageous when displaying <emphasis>C. elegans</emphasis> body curvature changes during locomotion, since apprehension and comparison of curvature matrices is intuitive.</para>
</sect2>
<sect2 id="sec2-15">
<title>The Worm Analysis System</title>
<para>The open source Worm Analysis System implements the FARSIGHT Toolkit with a fully integrated GUI (<ulink url="http://farsight-toolkit.org/wiki/Worm_Analysis_System">http://farsight-toolkit.org/wiki/Worm_Analysis_System</ulink> and <ulink url="http://farsight-toolkit.org/wiki/Worm_Features%26Events">http://farsight-toolkit.org/wiki/Worm_Features%26Events</ulink>). The software analyzes movie files of multiple worms from which different parameters can be calculated, like the worm&#x0027;s length, width, curvature, area and speed (<link linkend="bib31">Roussel et al., 2007</link><!-- PMID: 17926677 -->). It also describes the worm&#x0027;s state, as in forward motion, omega bend or pause. At the time of writing, the developers are working on a solution for collision detection. The software is capable of tracking two or more contacting nematodes, even if they partially overlap. The software has been optimized for usage of the graphical processing unit (GPU) during computation. One must note that implementation of this tracker requires more programming skills in comparison to the other systems available.</para>
</sect2>
<sect2 id="sec2-16">
<title>The Multi-Environment Model Estimation for Motility Analysis</title>
<para>The <ulink url="http://www.wormbase.org/resources/person/WBPerson11461#013--9">Josue Sznitman</ulink> Lab recently described a new strategy for image recognition: the Multi-Environment Model Estimation (MEME) for <emphasis>C. elegans</emphasis> motility analysis (<link linkend="bib41">Sznitman et al., 2010</link><!-- PMID: 20661478 -->). The software is coded in MATLAB, all functions are accessed through a GUI and it is available upon request from the authors. MEME is off-line image analysis software capable of recognizing worm body boundaries in image conditions that would not be tolerated by threshold-based worm trackers. As output, MEME &#x201C;skeletonizes&#x201D; the worm and saves images of the skeleton as well as a MATLAB file containing the x-y coordinates of the nematode over time. The software relies on the idea of Mixture of Gaussians (MOG). Briefly, MOG methods describe each pixel&#x0027;s intensity in an image as a variable with a Gaussian distribution. The background of an image can be recognized by analyzing the Gaussian distributions of all pixels, which requires a &#x201C;background only&#x201D; image (readers are referred to the paper by <link linkend="bib41">Sznitman et al., (2010)</link><!-- PMID: 20661478 -->, for details of the method). The MEME strategy is more reliable when recognizing worms in microfluidic chips than the common thresholding methods. The MEME software requires a sequence of images as input, which has to be manually extracted by third-party software such as VirtualDub. The software does not control the image acquisition. MEME runs under the MATLAB R2009b release with the Image Processing Toolbox.</para>
</sect2>
</sect1>
<sect1 id="sec1-5">
<title>Possible future developments</title>
<para>Regarding future development of Worm Trackers, one might expect advances in three fields. The first field is data acquisition. Currently, some tracking systems can depict freely behaving animals with a resolution reaching 10 &#x00B5;m per pixel for whole animals, or they can track single (fluorescent) cells in a region of interest. Multi-channel acquisition allows trackers to not only depict the animals&#x2019; behavior, but also make use of fluorescent reporters to correlate behavior and, for instance, second messenger signals (e.g. Ca<superscript>2+</superscript>). Parallel acquisition with different magnifications (e.g., using low and high magnification objectives above and below the specimen plane) allows focusing on distinct behavioral aspects. The second field is dedicated to stimulus application. Although many systems allow some sort of stimulus application during imaging, the nature of the stimulus is limited. Combination of optical, mechanical and thermal stimulation in real time is likely to boost <emphasis>C. elegans</emphasis> research. These aspects lead to the third field of innovation&#x2014;modularity. For the time being, researchers working with worm trackers are probably confronted with more than one system in order to address all research questions. As can be grasped from this review, several labs have developed solutions to the same question, which is good on one hand, as different ideas are being developed, and different aspects can be tackled. On the other hand, these systems generally are not compatible and thus it would be desirable to have a common basic system that can be expanded individually, where new modules are being shared on an open access basis (this is relatively straightforward for software, but less easy for hardware development). Modularity of worm trackers would allow one to build upon a framework and enhance already existing systems. Most trackers are not bound to a specific hardware configuration, although the first configuration of the new system might still be painstakingly difficult. In the future, such hardware ties will play a lesser role. One might expect to have qualities of many described worm trackers combined into such a framework, allowing a much broader approach for <emphasis>C. elegans</emphasis> tracking and enriching research in the worm community.</para>
</sect1>
<sect1 id="sec1-6">
<title>Conclusion</title>
<para>Due especially to the well-defined nervous system of <emphasis>C. elegans</emphasis>, neurobiologists in the worm community aim at a comprehensive functional description of neuronal networks, assessing information flow from sensory neurons through different circuit layers and motor circuits that define a prevalent behavioral response. Due to the availability of various assays and optogenetic tools, precise behavioral parameterization is required to allow straightforward (statistical) analysis and comparisons of the data. To this end, many tracking systems have been developed, each with their individual strengths and applicability. The current overview aims at providing a guideline to keep track of all the tracking systems and we hope that this WormBook chapter facilitates the search for a specific setup to fulfill individual needs.</para>
</sect1>
<sect1 id="sec1-7">
<title>References</title>
<para/>
&WormBook_Gottschalk_tracking_refs;
</sect1>
</article>