<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   
      <title>A biologist's guide to statistical thinking and analysis</title>
      <meta name="generator" content="DocBook XSL Stylesheets V1.67.2">
      <link rel="start" href="#Fay_and_Gerow_JM_final_text" title="A biologist's guide to statistical thinking and analysis">
      <link rel="next" href="#sec1" title="1.&nbsp;The basics">
      <!--#include virtual="/ssi/header_article.html" -->
   </head>
   <body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<!--#include virtual="/ssi/boilerplate.html" -->
<div class="html_to_pdf_link">
	<a href="statisticalanalysis.pdf">Download PDF version</a>
</div>
      <div class="article" lang="en">
         <div class="titlepage">
            <div>
               <div>
                  <h1 class="title"><a name="Fay_and_Gerow_JM_final_text"></a>A biologist's guide to statistical thinking and analysis<sup><a name="d0e6" href="#ftn.d0e6">*</a></sup></h1>
               </div>
               <div>
                  <div class="author"><span class="author"><span class="othername">David S. Fay<sup>1</sup><em><span class="remark"><sup><a name="d0e29" href="#ftn.d0e29">&sect;</a></sup></span></em> and Ken Gerow<sup>2</sup></span><br></span><span class="affiliation"><span class="orgname"><sup>1</sup>Department of Molecular Biology, College of Agriculture and Natural Resources, University of Wyoming, Laramie WY 82071, USA;
                           </span></span><span class="affiliation"><span class="orgname"><sup>2</sup>Department of Statistics, College of Arts and Sciences, University of Wyoming, Laramie WY 82071; </span></span></div>
               </div>
            </div>
            <hr>
         </div>
         <div class="toc">

	   <div id="section_locator"><a href="/toc_wormmethods.html">
	         <b>This chapter is in WormBook section:</b><br />
		       > WormMethods<br />>> Basic methods for <em>C. elegans</em>
		</a></div><hr />
		
            <p><b>Table of Contents</b></p>
            <dl>
               <dt><span class="sect1"><a href="#sec1">1. The basics</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec1-1">1.1. Introduction</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-2">1.2. Quantifying variation in population or sample data</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-3">1.3. Quantifying statistical uncertainty</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-4">1.4. Confidence intervals</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-5">1.5. What is the best way to report variation in data?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-6">1.6. A quick guide to interpreting different indicators of variation</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-7">1.7. The coefficient of variation</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-8">1.8. <span class="emphasis"><em>P</em></span>-values</a></span></dt>
                     <dt><span class="sect2"><a href="#sec1-9">1.9. Why 0.05?</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec2">2. Comparing two means</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec2-1">2.1. Introduction</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-2">2.2. Understanding the t-test: a brief foray into some statistical theory</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-3">2.3. One- versus two-sample tests</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-4">2.4. One versus two tails</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-5">2.5. Equal or non-equal variances</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-6">2.6. Are the data normal enough?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-7">2.7. Is there a minimum acceptable sample size?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-8">2.8. Paired versus unpaired tests</a></span></dt>
                     <dt><span class="sect2"><a href="#sec2-9">2.9. The critical value approach</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec3">3. Comparisons of more than two means</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec3-1">3.1. Introduction</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-2">3.2. Safety through repetition</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-3">3.3. The family-wise error rate</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-4">3.4. Bonferroni-type corrections</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-5">3.5. False discovery rates</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-6">3.6. Analysis of variance</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-7">3.7. Summary of multiple comparisons methods</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-8">3.8. When are multiple comparison adjustments not required?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec3-9">3.9. A philosophical argument for making no adjustments for multiple comparisons</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec4">4. Probabilities and Proportions</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec4-1">4.1. Introduction</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-2">4.2. Calculating simple probabilities</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-3">4.3. Calculating more-complex probabilities</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-4">4.4. The Poisson distribution</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-5">4.5. Intuitive methods for calculating probabilities</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-6">4.6. Conditional probability: calculating probabilities when events are not independent</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-7">4.7. Binomial proportions</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-8">4.8. Calculating confidence intervals for binomial proportions</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-9">4.9. Tests for differences between two binomial proportions</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-10">4.10. Tests for differences between more than one binomial proportion</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-11">4.11. Probability calculations for binomial proportions</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-12">4.12. Probability calculations when sample sizes are large relative to the population size</a></span></dt>
                     <dt><span class="sect2"><a href="#sec4-13">4.13. Tests for differences between multinomial proportions</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec5">5. Relative differences, ratios, and correlations</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec5-1">5.1. Comparing relative versus incremental differences</a></span></dt>
                     <dt><span class="sect2"><a href="#sec5-2">5.2. Ratio of means versus mean of ratios</a></span></dt>
                     <dt><span class="sect2"><a href="#sec5-3">5.3. Log scales</a></span></dt>
                     <dt><span class="sect2"><a href="#sec5-4">5.4. Correlation and modeling</a></span></dt>
                     <dt><span class="sect2"><a href="#sec5-5">5.5. Modeling and regression</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec6">6. Additional considerations and guidelines</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="sect2"><a href="#sec6-1">6.1. When is a sample size too small?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-2">6.2. Statistical power</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-3">6.3. Can a sample size be too large?</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-4">6.4. Dealing with outliers</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-5">6.5. Nonparametric tests</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-6">6.6. A brief word about survival</a></span></dt>
                     <dt><span class="sect2"><a href="#sec6-7">6.7. Fear not the bootstrap</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="sect1"><a href="#sec8">7. Acknowledgments</a></span></dt>
               <dt><span class="sect1"><a href="#sec9">8. References</a></span></dt>
               <dt><span class="sect1"><a href="#sec10">9. Appendix A: Microsoft Excel tools</a></span></dt>
               <dt><span class="sect1"><a href="#sec11">10. Appendix B: Recomended reading</a></span></dt>
               <dt><span class="sect1"><a href="#sec12">11. Appendix C: Useful programs for statistical calculations</a></span></dt>
               <dt><span class="sect1"><a href="#sec13">12. Appendix D: Useful websites for statistical calculations</a></span></dt>
            </dl>
         </div>
         <div class="abstract">
            <p class="title"><span class="bold"><strong>Abstract</strong></span></p>
            <p>The proper understanding and use of statistical tools are essential to the scientific enterprise. This is true both at the
               level of designing one's own experiments as well as for critically evaluating studies carried out by others. Unfortunately,
               many researchers who are otherwise rigorous and thoughtful in their scientific approach lack sufficient knowledge of this
               field. This methods chapter is written with such individuals in mind. Although the majority of examples are drawn from the
               field of <span class="emphasis"><em>Caenorhabditis elegans</em></span> biology, the concepts and practical applications are also relevant to those who work in the disciplines of molecular genetics
               and cell and developmental biology. Our intent has been to limit theoretical considerations to a necessary minimum and to
               use common examples as illustrations for statistical analysis. Our chapter includes a description of basic terms and central
               concepts and also contains in-depth discussions on the analysis of means, proportions, ratios, probabilities, and correlations.
               We also address issues related to sample size, normality, outliers, and non-parametric approaches.
            </p>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec1"></a>1.&nbsp;The basics
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-1"></a>1.1.&nbsp;Introduction
                        </h3>
                     </div>
                  </div>
               </div>
               <p>At the first group meeting that I attended as a new worm postdoc (1997, D.S.F.), I heard the following opinion expressed by
                  a senior scientist in the field: &#8220;If I need to rely on statistics to prove my point, then I'm not doing the right experiment.&#8221;
                  In fact, reading this statement today, many of us might well identify with this point of view. Our field has historically
                  gravitated toward experiments that provide clear-cut &#8220;yes&#8221; or &#8220;no&#8221; types of answers. Yes, mutant <span class="emphasis"><em>X</em></span> has a phenotype. No, mutant <span class="emphasis"><em>Y</em></span> does not genetically complement mutant <span class="emphasis"><em>Z</em></span>. We are perhaps even a bit suspicious of other kinds of data, which we perceive as requiring excessive hand waving. However,
                  the realities of biological complexity, the sometimes-necessary intrusion of sophisticated experimental design, and the need
                  for quantifying results may preclude black-and-white conclusions. Oversimplified statements can also be misleading or at least
                  overlook important and interesting subtleties. Finally, more and more of our experimental approaches rely on large multi-faceted
                  datasets. These types of situations may not lend themselves to straightforward interpretations or facile models. Statistics
                  may be required.
               </p>
               <p>The intent of these sections will be to provide <span class="emphasis"><em>C. elegans</em></span> researchers with a practical guide to the application of statistics using examples that are relevant to our field. Namely,
                  which common situations require statistical approaches and what are some of the appropriate methods (i.e., tests or estimation
                  procedures) to carry out? Our intent is therefore to aid worm researchers in applying statistics to their own work, including
                  considerations that may inform experimental design. In addition, we hope to provide reviewers and critical readers of the
                  worm scientific literature with some criteria by which to interpret and evaluate statistical analyses carried out by others.
                  At various points we suggest some general guidelines, which may lead to somewhat more uniformity in how our field conducts
                  and presents statistical findings. Finally, we provide some suggestions for additional readings for those interested in a
                  more systematic and in-depth coverage of the topics introduced (<a href="#sec10" title="9.&nbsp;Appendix A: Microsoft Excel tools">Appendix A</a>).
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-2"></a>1.2.&nbsp;Quantifying variation in population or sample data
                        </h3>
                     </div>
                  </div>
               </div>
               <p>There are numerous ways to describe and present the variation that is inherent to most data sets. <span class="emphasis"><em>Range</em></span> (defined as the largest value minus the smallest) is one common measure and has the advantage of being simple and intuitive.
                  Range, however, can be misleading because of the presence of <span class="emphasis"><em>outliers</em></span>, and it tends to be larger for larger sample sizes even without unusual data values<span class="emphasis"><em>. Standard deviation</em></span> (SD) is the most common way to present variation in biological data. It has the advantage that nearly everyone is familiar
                  with the term and that its units are identical to the units of the sample measurement. Its disadvantage is that few people
                  can recall what it actually means.
               </p>
               <p><a href="#figure1">Figure 1</a> depicts <span class="emphasis"><em>density curves</em></span> of brood sizes in two different <span class="emphasis"><em>populations</em></span> of self-fertilizing hermaphrodites. Both have identical average brood sizes of 300. However, the population in <a href="#figure1">Figure 1B</a> displays considerably more inherent variation than the population in <a href="#figure1">Figure 1A</a>. Looking at the density curves, we would predict that 10 randomly selected values from the population depicted in <a href="#figure1">Figure 1B</a> would tend to show a wider range than an equivalent set from the more tightly distributed population in <a href="#figure1">Figure 1A</a>. We might also note from the shape and symmetry of the density curves that both populations are <span class="emphasis"><em>Normally<sup><a name="fn1" href="#ftn.fn1">1</a></sup></em></span> <span class="emphasis"><em>distributed</em></span> (this is also referred to as a <span class="emphasis"><em>Gaussian distribution</em></span>). In reality, most biological data do not conform to a perfect bell-shaped curve, and, in some cases, they may profoundly
                  deviate from this ideal. Nevertheless, in many instances, the distribution of various types of data can be roughly approximated
                  by a normal distribution. Furthermore, the normal distribution is a particularly useful concept in classical statistics (more
                  on this later) and in this example is helpful for illustrative purposes.
               </p>
               <div class="mediaobject" align="center"><a name="figure1"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig1.jpg"><img src="statisticalanalysis_fig1_s.jpg" border="2" align="middle" alt=" figure 1"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 1. Two normal distributions.&nbsp;</b></p>
                  </div>
               </div>
               <p>The vertical red lines in <a href="#figure1">Figure 1A</a> and <a href="#figure1">1B</a> indicate one SD to either side of the mean. From this, we can see that the population in <a href="#figure1">Figure 1A</a> has a SD of 20, whereas the population in <a href="#figure1">Figure 1B</a> has a SD of 50. A useful rule of thumb is that roughly 67% of the values within a normally distributed population will reside
                  within one SD to either side of the mean. Correspondingly, 95% of values reside within two<sup><a name="fn2" href="#ftn.fn2">2</a></sup> SDs, and more than 99% reside within three SDs to either side of the mean. Thus, for the population in <a href="#figure1">Figure 1A</a>, we can predict that about 95% of hermaphrodites produce brood sizes between 260 and 340, whereas for the population in <a href="#figure1">Figure 1B</a>, 95% of hermaphrodites produce brood sizes between 200 and 400.
               </p>
               <p>Often we can never really know the true mean or SD of a population because we cannot usually observe the entire population.
                  Instead, we must use a sample to make an educated guess. In the case of experimental laboratory science, there is often no
                  limit to the number of animals that we could theoretically test or the number of experimental repeats that we could perform.
                  Admittedly, use of the term &#8220;populations&#8221; in this context can sound rather forced. It's awkward for us to think of a theoretical
                  collection of bands on a western blot or a series of cycle numbers from a qRT-PCR experiment as a population, but from the
                  standpoint of statistics, that's exactly what they are. Thus, our populations tend to be mythical in nature as well as infinite.
                  Moreover, even the most sadistic advisor can only expect a finite number of biological or technical repeats to be carried
                  out. The data that we ultimately analyze are therefore always just a tiny proportion of the population, real or theoretical,
                  from whence they came.
               </p>
               <p>It is important to note that increasing our sample size will not predictably increase or decrease the amount of variation
                  that we are ultimately likely to record. What can be stated is that a larger sample size will tend to give a sample SD that
                  is a more accurate estimate of the population SD. In the same vein, a larger sample size will also provide a more accurate
                  estimation of other <span class="emphasis"><em>parameters</em></span>, such as the population mean.
               </p>
               <p>In some cases, standard numerical summaries (e.g., mean and SD) may not be sufficient to fully or accurately describe the
                  data. In particular, these measures usually<sup><a name="fn3" href="#ftn.fn3">3</a></sup> tell you nothing about the shape of the underlying distribution. <a href="#figure2">Figure 2</a> illustrates this point; Panels A and B show the duration (in seconds) of vulval muscle cell contractions in two populations
                  of <span class="emphasis"><em>C. elegans</em></span>. The data from both panels have nearly identical means and SDs, but the data from panel A are clearly bimodal, whereas the
                  data from Panel B conform more to a normal distribution<sup><a name="fn4" href="#ftn.fn4">4</a></sup>. One way to present this observation would be to show the actual histograms (in a figure or supplemental figure). Alternatively,
                  a somewhat more concise depiction, which still gets the basic point across, is shown by the individual data plot in Panel
                  C. In any case, presenting these data simply as a mean and SD without highlighting the difference in distributions would be
                  potentially quite misleading, as the populations would appear to be identical.
               </p>
               <div class="mediaobject" align="center"><a name="figure2"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig2.jpg"><img src="statisticalanalysis_fig2_s.jpg" border="2" align="middle" alt=" figure 2"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 2. Two distributions with similar means and SDs.&nbsp;</b>Panels A and B show histograms of simulated data of vulval muscle cell contraction durations derived from underlying populations
                        with distributions that are either bimodal (A) or normal (B). Note that both populations have nearly identical means and SDs,
                        despite major differences in the population distributions. Panel C displays the same information shown in the two histograms
                        using individual data plots. Horizontally arrayed sets of dots represent repeat values.
                     </p>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-3"></a>1.3.&nbsp;Quantifying statistical uncertainty
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Before you become distressed about what the title of this section actually means, let's be clear about something. Statistics,
                  in its broadest sense, effectively does two things for us&#8212;more or less simultaneously. (1) Statistics provides us with useful
                  quantitative descriptors for summarizing our data. This includes fairly simple stuff such as means and proportions. It also
                  includes more complex statistics such as the correlation between related measurements, the slope of a linear regression, and
                  the odds ratio for mortality under differing conditions. These can all be useful for interpreting our data, making informed
                  conclusions, and constructing hypotheses for future studies. However, statistics gives us something else, too. (2) Statistics
                  also informs us about the accuracy of the very estimates that we've made. What a deal! Not only can we obtain predictions
                  for the population mean and other parameters, we also estimate how accurate those predictions really are. How this comes about
                  is part of the &#8220;magic&#8221; of statistics, which as stated shouldn't be taken literally, even if it appears to be that way at times.
               </p>
               <p>In the preceding section we discussed the importance of SD as a measure for describing natural variation within an entire
                  population of worms. We also touched upon the idea that we can calculate statistics, such as SD, from a sample that is drawn
                  from a larger population. Intuition also tells us that these two values, one corresponding to the population, the other to
                  the sample, ought to generally be similar in magnitude, if the sample size is large. Finally, we understand that the larger
                  the sample size, the closer our sample statistic will be to the true population statistic. This is true not only for the SD
                  but also for many other statistics as well.
               </p>
               <p>It is now time to discuss SD in another context that is central to the understanding of statistics. We do this with a thought
                  experiment. Imagine that we determine the brood size for six animals that are randomly selected from a larger population.
                  We could then use these data to calculate a sample mean, as well as a sample SD, which would be based on a sample size of
                  <span class="emphasis"><em>n</em></span> = 6. Not being satisfied with our efforts, we repeat this approach every day for 10 days, each day obtaining a new mean and
                  new SD (<a href="#table1" title="Table 1. Ten random samples (trials) of brood sizes.">Table 1</a>). At the end of 10 days, having obtained ten different means, we can now use each sample mean as though it were a single
                  data point to calculate a new mean, which we can call <span class="emphasis"><em>the mean of the means</em></span>. In addition, we can calculate the SD of these ten mean values, which we can refer to for now as the <span class="emphasis"><em>SD of the means</em></span>. We can then pose the following question: will the SD calculated using the ten means generally turn out to be a larger or
                  smaller value (on average) than the SD calculated from each sample of six random individuals? This is not merely an idiosyncratic
                  question posed for intellectual curiosity. The notion of the <span class="emphasis"><em>SD of the mean</em></span> is critical to statistical inference. Read on.
               </p>
               <div class="table"><a name="table1"></a><p class="title">Table 1. Ten random samples (trials) of brood sizes.</p>
                  <table summary="Table 1. Ten random samples (trials) of brood sizes." border="1">
                     <colgroup>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                     </colgroup>
                     <thead>
                        <tr valign="bottom">
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>Trial</strong></span></th>
                           <th colspan="6" align="center" valign="bottom"><span class="bold"><strong>Brood Sizes<sup>a</sup></strong></span></th>
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>Sample Mean</strong></span></th>
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>Sample SD</strong></span></th>
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>SE<sup>b</sup> of Mean</strong></span></th>
                        </tr>
                        <tr valign="top">
                           <th align="center" valign="top"><span class="bold"><strong>1</strong></span></th>
                           <th align="center" valign="top"><span class="bold"><strong>2</strong></span></th>
                           <th align="center" valign="top"><span class="bold"><strong>3</strong></span></th>
                           <th align="center" valign="top"><span class="bold"><strong>4</strong></span></th>
                           <th align="center" valign="top"><span class="bold"><strong>5</strong></span></th>
                           <th align="center" valign="top"><span class="bold"><strong>6</strong></span></th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>1</strong></span></td>
                           <td align="left" valign="top">218</td>
                           <td align="left" valign="top">259</td>
                           <td align="left" valign="top">271</td>
                           <td align="left" valign="top">320</td>
                           <td align="left" valign="top">266</td>
                           <td align="left" valign="top">392</td>
                           <td align="left" valign="top">287.67</td>
                           <td align="left" valign="top">60.59</td>
                           <td align="left" valign="top">24.73</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>2</strong></span></td>
                           <td align="left" valign="top">370</td>
                           <td align="left" valign="top">237</td>
                           <td align="left" valign="top">307</td>
                           <td align="left" valign="top">358</td>
                           <td align="left" valign="top">295</td>
                           <td align="left" valign="top">318</td>
                           <td align="left" valign="top">314.17</td>
                           <td align="left" valign="top">47.81</td>
                           <td align="left" valign="top">19.52</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>3</strong></span></td>
                           <td align="left" valign="top">324</td>
                           <td align="left" valign="top">264</td>
                           <td align="left" valign="top">343</td>
                           <td align="left" valign="top">269</td>
                           <td align="left" valign="top">304</td>
                           <td align="left" valign="top">223</td>
                           <td align="left" valign="top">287.83</td>
                           <td align="left" valign="top">44.13</td>
                           <td align="left" valign="top">18.02</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>4</strong></span></td>
                           <td align="left" valign="top">341</td>
                           <td align="left" valign="top">343</td>
                           <td align="left" valign="top">277</td>
                           <td align="left" valign="top">374</td>
                           <td align="left" valign="top">302</td>
                           <td align="left" valign="top">308</td>
                           <td align="left" valign="top">324.17</td>
                           <td align="left" valign="top">34.93</td>
                           <td align="left" valign="top">14.26</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>5</strong></span></td>
                           <td align="left" valign="top">293</td>
                           <td align="left" valign="top">362</td>
                           <td align="left" valign="top">296</td>
                           <td align="left" valign="top">384</td>
                           <td align="left" valign="top">270</td>
                           <td align="left" valign="top">307</td>
                           <td align="left" valign="top">318.67</td>
                           <td align="left" valign="top">44.32</td>
                           <td align="left" valign="top">18.10</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>6</strong></span></td>
                           <td align="left" valign="top">366</td>
                           <td align="left" valign="top">301</td>
                           <td align="left" valign="top">209</td>
                           <td align="left" valign="top">336</td>
                           <td align="left" valign="top">254</td>
                           <td align="left" valign="top">295</td>
                           <td align="left" valign="top">293.50</td>
                           <td align="left" valign="top">56.25</td>
                           <td align="left" valign="top">22.96</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>7</strong></span></td>
                           <td align="left" valign="top">325</td>
                           <td align="left" valign="top">240</td>
                           <td align="left" valign="top">304</td>
                           <td align="left" valign="top">294</td>
                           <td align="left" valign="top">260</td>
                           <td align="left" valign="top">310</td>
                           <td align="left" valign="top">288.83</td>
                           <td align="left" valign="top">32.34</td>
                           <td align="left" valign="top">13.20</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>8</strong></span></td>
                           <td align="left" valign="top">334</td>
                           <td align="left" valign="top">327</td>
                           <td align="left" valign="top">310</td>
                           <td align="left" valign="top">346</td>
                           <td align="left" valign="top">320</td>
                           <td align="left" valign="top">233</td>
                           <td align="left" valign="top">311.67</td>
                           <td align="left" valign="top">40.43</td>
                           <td align="left" valign="top">16.56</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>9</strong></span></td>
                           <td align="left" valign="top">339</td>
                           <td align="left" valign="top">256</td>
                           <td align="left" valign="top">240</td>
                           <td align="left" valign="top">329</td>
                           <td align="left" valign="top">230</td>
                           <td align="left" valign="top">361</td>
                           <td align="left" valign="top">292.50</td>
                           <td align="left" valign="top">56.89</td>
                           <td align="left" valign="top">23.22</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>10</strong></span></td>
                           <td align="left" valign="top">235</td>
                           <td align="left" valign="top">300</td>
                           <td align="left" valign="top">271</td>
                           <td align="left" valign="top">300</td>
                           <td align="left" valign="top">281</td>
                           <td align="left" valign="top">253</td>
                           <td align="left" valign="top">273.33</td>
                           <td align="left" valign="top">25.96</td>
                           <td align="left" valign="top">10.60</td>
                        </tr>
                        <tr valign="top">
                           <td colspan="7" align="right" valign="top">Mean of values</td>
                           <td align="left" valign="top"><span class="bold"><strong>299.2</strong></span></td>
                           <td align="left" valign="top"><span class="bold"><strong>44.36</strong></span></td>
                           <td align="left" valign="top"><span class="bold"><strong>18.11</strong></span></td>
                        </tr>
                        <tr valign="top">
                           <td colspan="7" align="left" valign="top">SD of means</td>
                           <td align="left" valign="top"><span class="bold"><strong>16.66</strong></span></td>
                           <td valign="top">&nbsp;</td>
                           <td valign="top">&nbsp;</td>
                        </tr>
                        <tr valign="top">
                           <td colspan="10" align="left" valign="top">
                              <p><sup>a</sup>For each trial, <span class="emphasis"><em>n</em></span> = 6 worms were assayed for brood size.
                              </p>
                              
                              <p><sup>b</sup>SE, standard error. When applied to a mean value, also abbreviated as SEM.
                              </p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               <p>Thinking about this, we may realize that the ten mean values, being averages of six worms, will tend to show less total variation
                  than measurements from individual worms. In other words, the variation between means should be less than the variation between
                  individual data values. Moreover, the average of these means will generally be closer to the true population mean than would
                  a mean obtained from just six random individuals. In fact, this idea is born out in <a href="#table1" title="Table 1. Ten random samples (trials) of brood sizes.">Table 1</a>, which used random sampling from a theoretical population (with a mean of 300 and SD of 50) to generate the sample values.
                  We can therefore conclude sample means will generally exhibit less variation than that seen among individual samples. Furthermore,
                  we can consider what might happen if we were to take daily samples of 20 worms instead of 6. Namely, the larger sample size
                  would result in an even tighter cluster of mean values. This in turn would produce an even smaller SD of the means than from
                  the experiment where only six worms were analyzed each day. Thus, increasing sample size will consistently lead to a smaller
                  SD of the means. Note however, as discussed above, increasing sample size will not predictably lead to a smaller or larger
                  SD for any given sample.
               </p>
               <p>It turns out that this concept of calculating the SD of multiple means (or other statistical parameters) is a very important
                  one. The good news is that rather than having to actually collect samples for ten or more days, statistical theory gives us
                  a short cut that allows us to estimate this value based on only a single day's effort. What a deal! Rather than calling this
                  value the &#8220;SD of the means&#8221;, as might make sense, the field has historically chosen to call this value the &#8220;<span class="emphasis"><em>standard error of the mean</em></span>&#8221; (SEM). In fact, whenever a SD is calculated for a statistic (e.g., the slope from a regression or a proportion), it is called
                  the <span class="emphasis"><em>standard error</em></span> (SE) of that statistic. SD is a term generally reserved for describing variation within a sample or population only. Although
                  we will largely avoid the use of formulas in this review, it is worth knowing that we can estimate the SEM from a single sample
                  of <span class="emphasis"><em>n</em></span> animals using the following equation:
               </p>
               <div class="mediaobject" align="center"><a name="unfig1"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig1.jpg" align="middle" width="90"></td>
                     </tr>
                  </table>
               </div>
               <p>From this relatively simple formula<sup><a name="fn5" href="#ftn.fn5">5</a></sup>, we can see that the greater the SD of the sample, the greater the SEM will be. Conversely, the larger our sample size, the
                  smaller the SEM will be. Looking back at <a href="#table1" title="Table 1. Ten random samples (trials) of brood sizes.">Table 1</a>, we can also see that the SEM estimate for each daily sample is reasonably<sup><a name="fn6" href="#ftn.fn6">6</a></sup> close, on average, to what we obtained by calculating the observed SD of the means from 10 days. This is not an accident.
                  Rather, chalk one up for statistical theory.
               </p>
               <p>Obviously, having a smaller SEM value reflects more precise estimates of the population mean. For that reason, scientists
                  are typically motivated to keep SEM values as low as possible. In the case of experimental biology, variation within our samples
                  may be due to inherent biological variation or to technical issues related to the methods we use. The former we probably can't
                  control very much. The latter we may be able to control to some extent, but probably not completely. This leaves increasing
                  sample size as a direct route to decreasing SE estimates and thus to improving the precision of the parameter estimates. However,
                  increasing sample size in some instances may not be a practical or efficient use of our time. Furthermore, because the denominator
                  in SE equations typically involves the square root of sample size, increasing sample size will have diminishing returns. In
                  general, a quadrupling of sample size is required to yield a halving of the SEM. Moreover, as discussed elsewhere in this
                  chapter, supporting very small differences with very high sample sizes might lead us to make convincing-sounding statistical
                  statements regarding biological effects of no real importance, which is not something we should aspire to do.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-4"></a>1.4.&nbsp;Confidence intervals
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Although SDs and SEs are all well and good, what we typically want to know is the accuracy of our parameter estimates. It
                  turns out that SEs are the key to calculating a more directly useful measure, a <span class="emphasis"><em>confidence interval</em></span> (CI). Although, the transformation of SEs into CIs isn't necessarily that complex, we will generally want to let computers
                  or calculators perform this conversion for us. That said, for sample means derived from sample sizes greater than about ten,
                  a 95% CI will usually span about two SEMs above and below the mean<sup><a name="fn7" href="#ftn.fn7">7</a></sup>. When pressed for a definition, most people might say that with a 95% CI, we are 95% certain that the true value of the mean or slope (or whatever parameter
                  we are estimating) is between the upper and lower limits of the given CI. Proper statistical semantics would more accurately
                  state that a 95% CI procedure is such that 95% of properly calculated intervals from appropriately random samples will contain
                  the true value of the parameter. If you can discern the difference, fine. If not, don't worry about it.
               </p>
               <p>One thing to keep in mind about CIs is that, for a given sample, a higher confidence level (e.g., 99%) will invoke intervals
                  that are wider than those created with a lower confidence level (e.g., 90%). Think about it this way. With a given amount
                  of information (i.e., data), if you wish to be more confident that your interval contains the parameter, then you need to
                  make the interval wider. Thus, less confidence corresponds to a narrower interval, whereas higher confidence requires a wider
                  interval. Generally for CIs to be useful, the range shouldn't be too great. Another thing to realize is that there is really
                  only one way to narrow the range associated with a given confidence level, and that is to increase the sample size. As discussed
                  above, however, diminishing returns, as well as basic questions related to biological importance of the data, should figure
                  foremost in any decision regarding sample size.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-5"></a>1.5.&nbsp;What is the best way to report variation in data?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Of course, the answer to this will depend on what you are trying to show and which measures of variation are most relevant
                  to your experiment. Nevertheless, here is an important news flash: with respect to means, the SEM is often not the most informative
                  parameter to display. This should be pretty obvious by now. SD is a good way to go if we are trying to show how much variation
                  there is within a population or sample. CIs are highly informative if we are trying to make a statement regarding the accuracy
                  of the estimated population mean. In contrast, SEM does neither of these things directly, yet remains very popular and is
                  often used inappropriately (<a href="#bib16">Nagele, 2003</a>). Some statisticians have pointed out that because SEM gives the smallest of the error bars, authors may often chose SEMs
                  for aesthetic reasons. Namely, to make their data appear less variable or to convince readers of a difference between values
                  that might not otherwise appear to be very different. In fairness, SEM is a perfectly legitimate descriptor of variation<sup><a name="fn8" href="#ftn.fn8">8</a></sup>. In contrast to CIs, the size of the SEM is not an artifact of the chosen confidence level. Furthermore, unlike the CI, the
                  validity of the SEM does not require assumptions that relate to statistical normality<sup><a name="fn9" href="#ftn.fn9">9</a></sup>. However, because the SEM is often less directly informative to readers, presenting either SDs or CIs can be strongly recommended
                  for most data. Furthermore, if the intent of a figure is to compare means for differences or a lack thereof, CIs are the clear
                  choice.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-6"></a>1.6.&nbsp;A quick guide to interpreting different indicators of variation
                        </h3>
                     </div>
                  </div>
               </div>
               <p><a href="#figure3">Figure 3</a> shows a bar graph containing identical (artificial) data plotted with the SD, SEM, and CI to indicate variation. Note that
                  the SD is the largest value, followed by the CI and SEM. This will be the case for all but very small sample sizes (in which
                  case the CI could be wider than two SDs). Remember: SD is variation among individuals, SE is the variation for a theoretical
                  collection of sample means (acquired in an identical manner to the real sample), and CI is a rescaling of the SE so as to
                  be able to impute confidence regarding the value of the population mean. With larger sample sizes, the SE and CI will shrink,
                  but there is no such tendency for the SD, which tends to remain the same but can also increase or decrease somewhat in a manner
                  that is not predictable.
               </p>
               <div class="mediaobject" align="center"><a name="figure3"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig3.jpg"><img src="statisticalanalysis_fig3_s.jpg" border="2" align="middle" alt=" figure 3"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 3. Illustration of SD, SE, and CI as measures of variability.&nbsp;</b></p>
                  </div>
               </div>
               <p><a href="#figure4">Figure 4</a> shows two different situations for two artificial means: one in which bars do not overlap (<a href="#figure4">Figure 4A</a>), and one in which they do, albeit slightly (<a href="#figure4">Figure 4B</a>). The following are some general guidelines for interpreting error bars, which are summarized in <a href="#table2" title="Table 2. General guidelines for interpreting error bars.">Table 2</a>. (1) With respect to SD, neither overlapping bars nor an absence of overlapping bars can be used to infer whether or not
                  two sample means are significantly different. This again is because SD reflects individual variation and you simply cannot
                  infer anything about significance of differences for the means. End of story. (2) With respect to SEM, overlapping bars (<a href="#figure4">Figure 4B</a>) can be used to infer that the sample means are not significantly different<span class="emphasis"><em>.</em></span> However, the absence of overlapping bars (<a href="#figure4">Figure 4A</a>) cannot be used to infer that the sample means are different. (3) With respect to CIs, the absence of overlapping bars (<a href="#figure4">Figure 4A</a>) can be used to infer that the sample means are statistically different<sup>6</sup>. If the CI bars do overlap (<a href="#figure4">Figure 4B</a>), however, the answer is &#8220;maybe&#8221;. Here is why. The correct measure for comparing two means is in fact the SE <span class="emphasis"><em>of the difference between the means</em></span>. In the case of equal SEMs, as illustrated in <a href="#figure4">Figure 4</a>, the SE of the difference is &#8764;1.4 times the SEM. To be significantly different,<sup><a name="fn10" href="#ftn.fn10">10</a></sup> then, two means need to be separated by about twice the SE of the difference (2.8 SEMs). In contrast, visual separation using
                  the CI bars requires a difference of four times the SEM (remember that CI &#8764; 2 &times; SEM above and below the mean), which is larger
                  than necessary to infer a difference between means. Therefore, a slight overlap can be present even when two means differ
                  significantly. If they overlap a lot (e.g., the CI for Mean 1 includes Mean 2), then the two means are for sure not significantly
                  different. If there is any uncertainty (i.e., there is some slight overlap), determination of significance is not possible;
                  the test needs to be formally carried out.
               </p>
               <div class="mediaobject" align="center"><a name="figure4"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig4.jpg"><img src="statisticalanalysis_fig4_s.jpg" border="2" align="middle" alt=" figure 4"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 4. Comparing means using visual measures of precision.&nbsp;</b></p>
                  </div>
               </div>
               <div class="table"><a name="table2"></a><p class="title">Table 2. General guidelines for interpreting error bars.</p>
                  <table summary="Table 2. General guidelines for interpreting error bars." border="1">
                     <colgroup>
                        <col>
                        <col>
                        <col>
                     </colgroup>
                     <thead>
                        <tr valign="bottom">
                           <th align="center" valign="bottom"><span class="bold"><strong>Error bar type</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Overlapping error bars</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Non-overlapping error bars</strong></span></th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr valign="top">
                           <td align="left" valign="top">SD</td>
                           <td align="left" valign="top">no inference</td>
                           <td align="left" valign="top">no inference</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">SEM</td>
                           <td align="left" valign="top">sample means are <span class="underline">not significantly different</span></td>
                           <td align="left" valign="top">no inference</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">CI</td>
                           <td align="left" valign="top">sample means <span class="underline">may or may not be significantly different</span></td>
                           <td align="left" valign="top">sample means <span class="underline">are significantly different</span></td>
                        </tr>
                     </tbody>
                  </table>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-7"></a>1.7.&nbsp;The coefficient of variation
                        </h3>
                     </div>
                  </div>
               </div>
               <p>In some cases, it may be most relevant to describe the <span class="emphasis"><em>relative variation</em></span> within a sample or population. Put another way, knowing the sample SD is really not very informative unless we also know
                  the sample mean. Thus, a sample with a SD = 50 and mean = 100 shows considerably more relative variation than a sample with
                  SD = 100 but mean = 10,000. To indicate the level of variation relative to the mean, we can report the coefficient of variation
                  (CV). In the case of sample means (<img src="statisticalanalysis_unfig2.jpg" width="9">), this can be calculated as follows:</p>
               <div class="mediaobject" align="center"><a name="unfig3"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig3.jpg" align="middle" width="90"></td>
                     </tr>
                  </table>
               </div>
               <p>Thus, low CVs indicate relatively little variation within the sample, and higher CVs indicate more variation. In addition,
                  because units will cancel out in this equation, CV is a unitless expression. This is actually advantageous when comparing
                  relative variation between parameters that are described using different scales or distinct types of measurements. Note, however,
                  that in situations where the mean value is zero (or very close to zero), the CV could approach infinity and will not provide
                  useful information. A similar warning applies in cases when data can be negative. The CV is most useful and meaningful only
                  for positively valued data. A variation on the CV is its use as applied to a statistic (rather than to individual variation).
                  Then its name has to reflect the statistic in question; so, for example, 
                  <img src="statisticalanalysis_unfig4.jpg" width="100" align="middle">
                  . For another example (the role of 
			<img src="statisticalanalysis_unfig5.jpg" width="9">
 may be confusing here), suppose one has estimated a proportion (mortality, for instance), and obtained an estimate labeled
 			<img src="statisticalanalysis_unfig6.jpg" width="9">
                and its SE, labeled 
                <img src="statisticalanalysis_unfig7.jpg" width="40" align="middle">
                . Then <img src="statisticalanalysis_unfig8.jpg" width="120" align="middle">
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-8"></a>1.8.&nbsp;<span class="emphasis"><em>P</em></span>-values
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Most statistical tests culminate in a statement regarding the <span class="emphasis"><em>P</em></span>-value, without which reviewers or readers may feel shortchanged. The <span class="emphasis"><em>P</em></span>-value is commonly defined as the probability of obtaining a result (more formally a <span class="emphasis"><em>test statistic</em></span>) that is at least as extreme as the one observed, assuming that the <span class="emphasis"><em>null hypothesis</em></span> is true. Here, the specific null hypothesis will depend on the nature of the experiment. In general, the null hypothesis
                  is the statistical equivalent of the &#8220;innocent until proven guilty&#8221; convention of the judicial system. For example, we may
                  be testing a mutant that we suspect changes the ratio of male-to-hermaphrodite cross-progeny following mating. In this case,
                  the null hypothesis is that the mutant does not differ from wild type, where the sex ratio is established to be 1:1. More
                  directly, the null hypothesis is that the sex ratio in mutants is 1:1. Furthermore, the complement of the null hypothesis,
                  known as the <span class="emphasis"><em>experimental</em></span> or <span class="emphasis"><em>alternative hypothesis</em></span>, would be that the sex ratio in mutants is different than that in wild type or is something other than 1:1. For this experiment,
                  showing that the ratio in mutants is <span class="emphasis"><em>significantly</em></span> different than 1:1 would constitute a finding of interest. Here, use of the term &#8220;significantly&#8221; is short-hand for a particular
                  technical meaning, namely that the result is <span class="emphasis"><em>statistically significant</em></span>, which in turn implies only that the observed difference appears to be real and is not due only to random chance in the sample(s).
                  <span class="underline">Whether or not a result that is statistically significant is also <span class="emphasis"><em>biologically significant</em></span> is another question</span>. Moreover, the term significant is not an ideal one, but because of long-standing convention, we are stuck with it. Statistically
                  <span class="emphasis"><em>plausible</em></span> or statistically <span class="emphasis"><em>supported</em></span> may in fact be better terms.
               </p>
               <p>Getting back to <span class="emphasis"><em>P</em></span>-values, let's imagine that in an experiment with mutants, 40% of cross-progeny are observed to be males, whereas 60% are
                  hermaphrodites. A statistical significance test then informs us that for this experiment, <span class="emphasis"><em>P</em></span> = 0.25. We interpret this to mean that even if there was no actual difference between the mutant and wild type with respect
                  to their sex ratios, we would still expect to see deviations as great, or greater than, a 6:4 ratio in 25% of our experiments.
                  Put another way, if we were to replicate this experiment 100 times, random chance would lead to ratios at least as extreme
                  as 6:4 in 25 of those experiments. Of course, you may well wonder how it is possible to extrapolate from one experiment to
                  make conclusions about what (approximately) the next 99 experiments will look like. (Short answer: There is well-established
                  statistical theory behind this extrapolation that is similar in nature to our discussion on the SEM.) In any case, a large
                  <span class="emphasis"><em>P</em></span>-value, such as 0.25, is a red flag and leaves us unconvinced of a difference. It is, however, possible that a true difference
                  exists but that our experiment failed to detect it (because of a small sample size, for instance). In contrast, suppose we
                  found a sex ratio of 6:4, but with a corresponding <span class="emphasis"><em>P</em></span>-value of 0.001 (this experiment likely had a much larger sample size than did the first). In this case, the likelihood that
                  pure chance has conspired to produce a deviation from the 1:1 ratio as great or greater than 6:4 is very small, 1 in 1,000
                  to be exact. Because this is very unlikely, we would conclude that the null hypothesis is not supported and that mutants really
                  do differ in their sex ratio from wild type. Such a finding would therefore be described as statistically significant on the
                  basis of the associated low <span class="emphasis"><em>P</em></span>-value.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec1-9"></a>1.9.&nbsp;Why 0.05?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>There is a long-standing convention in biology that <span class="emphasis"><em>P</em></span>-values that are &#8804;0.05 are considered to be significant, whereas <span class="emphasis"><em>P</em></span>-values that are &gt;0.05 are not significant<sup><a name="fn11" href="#ftn.fn11">11</a></sup>. Of course, common sense would dictate that there is no rational reason for anointing any specific number as a universal
                  cutoff, below or above which results must either be celebrated or condemned. Can anyone imagine a convincing argument by someone
                  stating that they will believe a finding if the <span class="emphasis"><em>P</em></span>-value is 0.04 but not if it is 0.06? Even a <span class="emphasis"><em>P</em></span>-value of 0.10 suggests a finding for which there is <span class="emphasis"><em>some</em></span> chance that it is real.
               </p>
               <p>So why impose &#8220;cutoffs&#8221;, which are often referred to as the <span class="emphasis"><em>chosen &#945; level</em></span>, of any kind? Well, for one thing, it makes life simpler for reviewers and readers who may not want to agonize over personal
                  judgments regarding every <span class="emphasis"><em>P</em></span>-value in every experiment. It could also be argued that, much like speed limits, there needs to be an agreed-upon cutoff.
                  Even if driving at 76 mph isn't much more dangerous than driving at 75 mph, one does have to consider public safety. In the
                  case of science, the apparent danger is that too many false-positive findings may enter the literature and become dogma. Noting
                  that the imposition of a reasonable, if arbitrary, cutoff is likely to do little to prevent the publication of dubious findings
                  is probably irrelevant at this point.
               </p>
               <p>The key is not to change the chosen cutoff&#8212;we have no better suggestion<sup><a name="fn12" href="#ftn.fn12">12</a></sup> than 0.05. The key is for readers to understand that there is nothing special about 0.05 and, most importantly, to look beyond
                  <span class="emphasis"><em>P</em></span>-values to determine whether or not the experiments are well controlled and the results are of biological interest. It is
                  also often more informative to include actual <span class="emphasis"><em>P</em></span>-values rather than simply stating <span class="emphasis"><em>P</em></span> &#8804; 0.05; a result where <span class="emphasis"><em>P</em></span> = 0.049 is roughly three times more likely to have occurred by chance than when <span class="emphasis"><em>P</em></span> = 0.016, yet both are typically reported as <span class="emphasis"><em>P</em></span> &#8804; 0.05. Moreover, reporting the results of statistical tests as <span class="emphasis"><em>P</em></span> &#8804; 0.05 (or any number) is a holdover to the days when computing exact <span class="emphasis"><em>P</em></span>-values was much more difficult. Finally, if a finding is of interest and the experiment is technically sound, reviewers need
                  not skewer a result or insist on authors discarding the data just because <span class="emphasis"><em>P</em></span> &#8804; 0.07. Judgment and common sense should always take precedent over an arbitrary number.
               </p>
            </div>
         </div>
            <div class="footnotes"><br><hr width="100" align="left">
            <div class="footnote">
               <p><sup><a name="ftn.fn1" href="#fn1">1</a></sup>In theory, we could always capitalize &#8220;Normal&#8221; to emphasize its role as the name of a distribution, not a reference to &#8220;normal&#8221;,
                  meaning usual or typical. However, most texts don't bother and so we won't either.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn2" href="#fn2">2</a></sup>A useful addendum: Four SDs captures the range of most (here, formally 95%) data values; it turns out this is casually true
                  for the distribution for most real-life variables (i.e., not only those that are normally distributed). Most (but not quite
                  all) of the values will span a range of approximately four SDs.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn3" href="#fn3">3</a></sup>For example, in many instances, data values are known to be composed of only non-negative values. In that instance, if the
                  coefficient of variation (SD/mean) is greater than &#8764;0.6, this would indicate that the distribution is skewed right.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn4" href="#fn4">4</a></sup>Indeed the data from Panel B was generated from a normal distribution. However, you can see that the distribution of the sample
                  won't necessarily be perfectly symmetric and bell-shape, though it is close. Also note that just because the distribution
                  in Panel A is bimodal does not imply that classical statistical methods are inapplicable. In fact, a simulation study based
                  on those data showed that the distribution of the sample mean was indeed very close to normal, so a usual t-based confidence
                  interval or test would be valid. This is so because of the large sample size and is a predictable consequence of the Central
                  Limit Theorem (see <a href="#sec2" title="2.&nbsp;Comparing two means">Section 2</a> for a more detailed discussion).
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn5" href="#fn5">5</a></sup>We note that the SE formula shown here is for the SE of a mean from a random sample. Changing the sample design (e.g., using
                  stratified sampling) or choosing a different statistic requires the use of a different formula.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn6" href="#fn6">6</a></sup>Our simulation had only ten random samples of size six. Had we used a much larger number of trials (e.g., 100 instead of 10),
                  these two values would have been much closer to each other.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn7" href="#fn7">7</a></sup>This calculation (two times the SE) is sometimes called the margin of error for the CI.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn8" href="#fn8">8</a></sup>Indeed, given the ubiquity of &#8220;95%&#8221; as a usual choice for confidence level, and applying the concept in Footnote 2, a quick-and-dirty
                  &#8220;pretty darn sure&#8221; (PDS) CI can be constructed by using 2 times the SE as the margin of error. This will approximately coincide
                  with a 95% CI under many circumstances, as long as the sample size is not small.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn9" href="#fn9">9</a></sup>The requirement for normality in the context of various tests will be discussed in later sections.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn10" href="#fn10">10</a></sup>Here meaning by a statistical test where the P-value cutoff or &#8220;alpha level&#8221; (&#945;) is 0.05.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn11" href="#fn11">11</a></sup>R.A. Fisher, a giant in the field of statistics, chose this value as being meaningful for the agricultural experiments with
                  which he worked in the 1920s.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.fn12" href="#fn12">12</a></sup>Although one of us is in favor of 0.056, as it coincides with his age (modulo a factor of 1000).
               </p>
            </div>
         </div>
      </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec2"></a>2.&nbsp;Comparing two means
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-1"></a>2.1.&nbsp;Introduction
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Many studies in our field boil down to generating means and comparing them to each other. In doing so, we try to answer questions
                  such as, &#8220;Is the average brood size of mutant <span class="emphasis"><em>x</em></span> different from that of wild type?&#8221; or &#8220;Is the expression of gene <span class="emphasis"><em>y</em></span> in embryos greater at 25 &deg;C than at 20 &deg;C?&#8221; Of course we will never really obtain identical values from any two experiments.
                  This is true even if the data are acquired from a single population; the sample means will always be different from each other,
                  even if only slightly. The pertinent question that statistics can address is whether or not the differences we inevitably
                  observe reflect a real difference in the <span class="emphasis"><em>populations</em></span> from which the samples were acquired. Put another way, are the differences detected by our experiments, which are necessarily
                  based on a limited sample size, likely or not to result from chance effects of sampling (i.e., <span class="emphasis"><em>chance sampling</em></span>). If chance sampling can account for the observed differences, then our results will <span class="emphasis"><em>not</em></span> be deemed <span class="emphasis"><em>statistically significant</em></span><sup><a name="fn13" href="#ftn.fn13">13</a></sup>. In contrast, if the observed differences are unlikely to have occurred by chance, then our results may be considered significant
                  in so much as statistics are concerned. Whether or not such differences are <span class="emphasis"><em>biologically significant</em></span> is a separate question reserved for the judgment of biologists.
               </p>
               <p>Most biologists, even those leery of statistics, are generally aware that the venerable <span class="emphasis"><em>t-</em></span>test (a.k.a., Student's <span class="emphasis"><em>t</em></span>-test)<sup><a name="fn14" href="#ftn.fn14">14</a></sup> is the standard method used to address questions related to differences between two sample means. Several factors influence
                  the <span class="emphasis"><em>power</em></span> of the <span class="emphasis"><em>t-</em></span>test to detect significant differences. These include the size of the sample and the amount of variation present within the
                  sample. If these sound familiar, they should. They were both factors that influence the size of the SEM, discussed in the
                  preceding section. This is not a coincidence, as the heart of a <span class="emphasis"><em>t-</em></span>test resides in estimating the standard error of the difference between two means (SEDM). Greater variance in the sample data
                  increases the size of the SEDM, whereas higher sample sizes reduce it. Thus, lower variance and larger samples make it easier
                  to detect differences. If the size of the SEDM is small relative to the absolute difference in means, then the finding will
                  likely hold up as being <span class="emphasis"><em>statistically significant</em></span>.
               </p>
               <p>In fact, it is not necessary to deal directly with the SEDM to be perfectly proficient at interpreting results from a <span class="emphasis"><em>t</em></span>-test. We will therefore focus primarily on aspects of the <span class="emphasis"><em>t</em></span>-test that are most relevant to experimentalists. These include choices of carrying out tests that are either one- or two-tailed
                  and are either paired or unpaired, assumptions of equal variance or not, and issues related to sample sizes and normality.
                  We would also note, in passing, that alternatives to the <span class="emphasis"><em>t</em></span>-test do exist. These tests, which include the computationally intensive bootstrap (see <a href="#sec6-7" title="6.7.&nbsp;Fear not the bootstrap">Section 6.7</a>), typically require somewhat fewer assumptions than the <span class="emphasis"><em>t</em></span>-test and will generally yield similar or superior results. For reasonably large sample sizes, a <span class="emphasis"><em>t</em></span>-test will provide virtually the same answer and is currently more straightforward to carry out using available software and
                  websites. It is also the method most familiar to reviewers, who may be skeptical of approaches that are less commonly used.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-2"></a>2.2.&nbsp;Understanding the t-test: a brief foray into some statistical theory
                        </h3>
                     </div>
                  </div>
               </div>
               <p>To aid in understanding the logic behind the <span class="emphasis"><em>t</em></span>-test, as well as the basic requirements for the <span class="emphasis"><em>t</em></span>-test to be valid, we need to introduce a few more statistical concepts. We will do this through an example. Imagine that
                  we are interested in knowing whether or not the expression of gene <span class="emphasis"><em>a</em></span> is altered in comma-stage embryos when gene <span class="emphasis"><em>b</em></span> has been inactivated by a mutation. To look for an effect, we take total fluorescence intensity measurements<sup><a name="fn15" href="#ftn.fn15">15</a></sup> of an integrated <span class="emphasis"><em>a</em></span>::GFP reporter in comma-stage embryos in both wild-type (Control, <a href="#figure5">Figure 5A</a>) and <span class="emphasis"><em>b</em></span> mutant (Test, <a href="#figure5">Figure 5B</a>) strains. For each condition, we analyze 55 embryos. Expression of gene <span class="emphasis"><em>a</em></span> appears to be greater in the control setting; the difference between the two sample means is 11.3 billion fluorescence units
                  (henceforth simply referred to as &#8220;11.3 units&#8221;).
               </p>
               <div class="mediaobject" align="center"><a name="figure5"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig5.jpg"><img src="statisticalanalysis_fig5_s.jpg" border="2" align="middle" alt=" figure 5"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 5. Summary of GFP-reporter expression data for a control and a test group.&nbsp;</b></p>
                  </div>
               </div>
               <p>Along with the familiar mean and SD, <a href="#figure5">Figure 5</a> shows some additional information about the two data sets. Recall that in <a href="#sec1-2" title="1.2.&nbsp;Quantifying variation in population or sample data">Section 1.2</a>, we described what a data set looks like that is normally distributed (<a href="#figure1">Figure 1</a>). What we didn't mention is that distribution of the data<sup><a name="fn16" href="#ftn.fn16">16</a></sup> can have a strong impact, at least indirectly, on whether or not a given statistical test will be valid. Such is the case
                  for the <span class="emphasis"><em>t</em></span>-test. Looking at <a href="#figure5">Figure 5</a>, we can see that the datasets are in fact a bit lopsided, having somewhat longer tails on the right. In technical terms,
                  these distributions would be categorized as <span class="emphasis"><em>skewed</em></span> right. Furthermore, because our sample size was sufficiently large (n=55), we can conclude that the populations from whence
                  the data came are also skewed right. Although not critical to our present discussion, several parameters are typically used
                  to quantify the shape of the data including the extent to which the data deviate from normality (e.g., <span class="emphasis"><em>skewness<sup><a name="fn17" href="#ftn.fn17">17</a></sup></em></span>, <span class="emphasis"><em>kurtosis</em></span><sup><a name="fn18" href="#ftn.fn18">18</a></sup> and <span class="emphasis"><em>A-squared</em></span><sup><a name="fn19" href="#ftn.fn19">19</a></sup>). In any case, an obvious question now becomes, how can you know whether your data are distributed normally (or at least
                  normally enough), to run a <span class="emphasis"><em>t</em></span>-test?
               </p>
               <p>Before addressing this question, we must first grapple with a bit of statistical theory. The Gaussian curve shown in <a href="#figure6">Figure 6A</a> represents a theoretical <span class="emphasis"><em>distribution of differences between sample means</em></span> for our experiment. Put another way, this is the distribution of differences that we would expect to obtain if we were to
                  repeat our experiment an infinite number of times. Remember that for any given population, when we randomly &#8220;choose&#8221; a sample,
                  each repetition will generate a slightly different sample mean. Thus, if we carried out such sampling repetitions with our
                  two populations ad infinitum, the bell-shaped distribution of differences between the two means would be generated (<a href="#figure6">Figure 6A</a>). Note that this theoretical distribution of differences is based on our actual sample means and SDs, as well as on the assumption
                  that our original data sets were derived from populations that are normal, which is something we already know isn't true.
                  So what to do?
               </p>
               <div class="mediaobject" align="center"><a name="figure6"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig6.jpg"><img src="statisticalanalysis_fig6_s.jpg" border="2" align="middle" alt=" figure 6"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 6. Theoretical and simulated sampling distribution of differences between two means.&nbsp;</b>The distributions are from the gene expression example. The mean and SE (SEDM) of the theoretical (A) and simulated (B) distributions
                        are both approximately 11.3 and 1.4 units, respectively. The black vertical line in each panel is centered on the mean of
                        the differences. The blue vertical lines indicate SEs (SEDMs) on each side.
                     </p>
                  </div>
               </div>
               <p>As it happens, this lack of normality in the distribution of the populations from which we derive our samples does not often
                  pose a problem. The reason is that the distribution of sample means, as well as the distribution of differences between two
                  independent sample means (along with many<sup><a name="fn20" href="#ftn.fn20">20</a></sup> other conventionally used statistics), is often normal enough for the statistics to still be valid. The reason is the <span class="emphasis"><em>The Central Limit Theorem</em></span>, a &#8220;statistical law of gravity&#8221;, that states (in its simplest form<sup><a name="fn21" href="#ftn.fn21">21</a></sup>) that the distribution of a sample mean will be approximately normal providing the sample size is sufficiently large. How
                  large is large enough? That depends on the distribution of the data values in the population from which the sample came. The
                  more non-normal it is (usually, that means the more skewed), the larger the sample size requirement. Assessing this is a matter
                  of judgment<sup><a name="fn22" href="#ftn.fn22">22</a></sup>. <a href="#figure7">Figure 7</a> was derived using a computational sampling approach to illustrate the effect of sample size on the distribution of the sample
                  mean. In this case, the sample was derived from a population that is sharply skewed right, a common feature of many biological
                  systems where negative values are not encountered (<a href="#figure7">Figure 7A</a>). As can be seen, with a sample size of only 15 (<a href="#figure7">Figure 7B</a>), the distribution of the mean is still skewed right, although much less so than the original population. By the time we
                  have sample sizes of 30 or 60 (<a href="#figure7">Figure 7C, D</a>), however, the distribution of the mean is indeed very close to being symmetrical (i.e., normal).
               </p>
               <div class="mediaobject" align="center"><a name="figure7"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig7.jpg"><img src="statisticalanalysis_fig7_s.jpg" border="2" align="middle" alt=" figure 7"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 7. Illustration of Central Limit Theorem for a skewed population of values.&nbsp;</b>Panel A shows the population (highly skewed right and truncated at zero); Panels B, C, and D show distributions of the mean
                        for sample sizes of 15, 30, and 60, respectively, as obtained through a computational sampling approach. As indicated by the
                        <span class="emphasis"><em>x</em></span> axes, the sample means are approximately 3. The <span class="emphasis"><em>y</em></span> axes indicate the number of computational samples obtained for a given mean value. As would be expected, larger-sized samples
                        give distributions that are closer to normal and have a narrower range of values.
                     </p>
                  </div>
               </div>
               <p>The Central Limit Theorem having come to our rescue, we can now set aside the caveat that the populations shown in <a href="#figure5">Figure 5</a> are non-normal and proceed with our analysis. From <a href="#figure6">Figure 6</a> we can see that the center of the theoretical distribution (black line) is 11.29, which is the actual difference we observed
                  in our experiment. Furthermore, we can see that on either side of this center point, there is a decreasing likelihood that
                  substantially higher or lower values will be observed. The vertical blue lines show the positions of one and two SDs from
                  the apex of the curve, which in this case could also be referred to as SEDMs. As with other SDs, roughly 95% of the area under
                  the curve is contained within two SDs. This means that in 95 out of 100 experiments, we would expect to obtain differences
                  of means that were between &#8220;8.5&#8221; and &#8220;14.0&#8221; fluorescence units. In fact, this statement amounts to a 95% CI for the difference
                  between the means, which is a useful measure and amenable to straightforward interpretation. Moreover, because the 95% CI
                  of the difference in means does not include zero, this implies that the <span class="emphasis"><em>P</em></span>-value for the difference must be less than 0.05 (i.e., that the null hypothesis of no difference in means is not true). Conversely,
                  had the 95% CI included zero, then we would already know that the <span class="emphasis"><em>P</em></span>-value will not support conclusions of a difference based on the conventional cutoff (assuming application of the two-tailed
                  <span class="emphasis"><em>t</em></span>-test; see below).
               </p>
               <p>The key is to understand that the <span class="emphasis"><em>t</em></span>-test is based on the theoretical distribution shown in <a href="#figure6">Figure 6A</a>, as are many other statistical parameters including 95% CIs of the mean. Thus, for the <span class="emphasis"><em>t</em></span>-test to be valid, the shape of the actual differences in sample means must come reasonably close to approximating a normal
                  curve. But how can we know what this distribution would look like without repeating our experiment hundreds or thousands of
                  times? To address this question, we have generated a complementary distribution shown in <a href="#figure6">Figure 6B</a>. In contrast to <a href="#figure6">Figure 6A</a>, <a href="#figure6">Figure 6B</a> was generated using a computational re-sampling method known as bootstrapping (discussed in <a href="#sec6-7" title="6.7.&nbsp;Fear not the bootstrap">Section 6.7</a>). It shows a histogram of the differences in means obtained by carrying out 1,000 <span class="emphasis"><em>in silico</em></span> repeats of our experiment. Importantly, because this histogram was generated using our actual sample data, it automatically
                  takes skewing effects into account. Notice that the data from this histogram closely approximate a normal curve and that the
                  values obtained for the mean and SDs are virtually identical to those obtained using the theoretical distribution in <a href="#figure6">Figure 6A</a>. What this tells us is that even though the sample data were indeed somewhat skewed, a <span class="emphasis"><em>t</em></span>-test will still give a legitimate result. Moreover, from this exercise we can see that with a sufficient sample size, the
                  <span class="emphasis"><em>t</em></span>-test is quite robust to some degree of non-normality in the underlying population distributions. Issues related to normality
                  are also discussed further below.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-3"></a>2.3.&nbsp;One- versus two-sample tests
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Although <span class="emphasis"><em>t</em></span>-tests always evaluate differences between two means, in some cases only one of the two mean values may be derived from an
                  experimental sample. For example, we may wish to compare the number of vulval cell fates induced in wild-type hermaphrodites
                  versus mutant <span class="emphasis"><em>m</em></span>. Because it is broadly accepted that wild type induces (on average) three progenitor vulval cells, we could theoretically
                  dispense with re-measuring this established value and instead measure it only in the mutant <span class="emphasis"><em>m</em></span> background (<a href="#bib20">Sulston and Horvitz, 1977</a>). In such cases, we would be obliged to run a one-sample <span class="emphasis"><em>t</em></span>-test to determine if the mean value of mutant <span class="emphasis"><em>m</em></span> is different from that of wild type.
               </p>
               <p>There is, however, a problem in using the one-sample approach, which is not statistical but experimental. Namely, there is
                  always the possibility that something about the growth conditions, experimental execution, or alignment of the planets, could
                  result in a value for wild type that is different from that of the established norm. If so, these effects would likely conspire
                  to produce a value for mutant <span class="emphasis"><em>m</em></span> that is different from the traditional wild-type value, even if no real difference exists. This could then lead to a false
                  conclusion of a difference between wild type and mutant <span class="emphasis"><em>m</em></span>. In other words, the statistical test, though valid, would be carried out using flawed data. For this reason, one doesn't
                  often see one-sample <span class="emphasis"><em>t</em></span>-tests in the worm literature. Rather, researchers tend to carry out parallel experiments on both populations to avoid being
                  misled. Typically, this is only a minor inconvenience and provides much greater assurance that any conclusions will be legitimate.
                  Along these lines, historical controls, including those carried out by the same lab but at different times, should typically
                  be avoided.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-4"></a>2.4.&nbsp;One versus two tails
                        </h3>
                     </div>
                  </div>
               </div>
               <p>One aspect of the <span class="emphasis"><em>t</em></span>-test that tends to agitate users is the obligation to choose either the one or two-tailed versions of the test. That the
                  term &#8220;tails&#8221; is not particularly informative only exacerbates the matter. The key difference between the one- and two-tailed
                  versions comes down to the formal statistical question being posed. Namely, the difference lies in the wording of the research
                  question. To illustrate this point, we will start by applying a two-tailed <span class="emphasis"><em>t</em></span>-test to our example of embryonic GFP expression. In this situation, our typical goal as scientists would be to detect a difference
                  between the two means. This aspiration can be more formally stated in the form of a <span class="emphasis"><em>research</em></span> or <span class="emphasis"><em>alternative hypothesis</em></span>. Namely, that the average expression levels of <span class="emphasis"><em>a</em></span>::GFP in wild type and in mutant <span class="emphasis"><em>b</em></span> are different. The <span class="emphasis"><em>null hypothesis</em></span> must convey the opposite sentiment. For the two-tailed <span class="emphasis"><em>t</em></span>-test, the null hypothesis is simply that the expression of <span class="emphasis"><em>a</em></span>::GFP in wild type and mutant <span class="emphasis"><em>b</em></span> backgrounds is the same. Alternatively, one could state that the difference in expression levels between wild type and mutant
                  <span class="emphasis"><em>b</em></span> is zero.
               </p>
               <p>It turns out that our example, while real and useful for illustrating the idea that the sampling distribution of the mean
                  can be approximately normal (and indeed should be if a <span class="emphasis"><em>t</em></span>-test is to be carried out), even if the distribution of the data are not, is not so useful for illustrating <span class="emphasis"><em>P</em></span>-value concepts. Hence, we will continue this discussion with a contrived variation: suppose the SEDM was 5.0, reflecting
                  a very large amount of variation in the gene expression data. This would lead to the distribution shown in <a href="#figure8">Figure 8A</a>, which is analogous to the one from <a href="#figure6">Figure 6A</a>. You can see how the increase in the SEDM affects the values that are contained in the resulting 95% CI. The mean is still
                  11.3, but now there is some probability (albeit a small one) of obtaining a difference of zero, our null hypothesis. <a href="#figure8">Figure 8B</a> shows the same curve and SEDMs. This time, however, we have shifted the values of the <span class="emphasis"><em>x</em></span> axis to consider the condition under which the null hypothesis is true. Thus the postulated difference in this scenario is
                  zero (at the peak of the curve).
               </p>
               <div class="mediaobject" align="center"><a name="figure8"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig8.jpg"><img src="statisticalanalysis_fig8_s.jpg" border="2" align="middle" alt=" figure 8"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 8. Graphical representation of a two-tailed <span class="emphasis"><em>t</em></span>-test.&nbsp;</b>(A) The same theoretical sampling distribution shown in Figure 6A in which the SEDM has been changed to 5.0 units (instead
                        of 1.4). The numerical values on the <span class="emphasis"><em>x</em></span>-axis represent differences from the mean in original units; numbers on the green background are values corresponding to the
                        black and blue vertical lines. The black vertical line indicates a mean difference of 11.3 units, the blue vertical lines
                        show SEs (SEDMs). (B) The results shown in panel A are considered for the case where the null hypothesis is indeed true (i.e.,
                        the difference of the means is zero). The units on the <span class="emphasis"><em>x</em></span>-axis represent differences from the mean in SEs (SEDMs). As for panel A, the numbers on the green background correspond to
                        the original differences. The rejection cutoffs are indicated with red lines using ( = 0.05 (i.e., the red lines partition
                        5% of the total space under the curve on each tail. The blue vertical line indicates the actual difference observed. The dashed
                        blue line indicates the negative value of the observed actual difference. In this case, the two-tailed <span class="emphasis"><em>P</em></span>-value for the difference in means will be equal to the proportion of the volume under the curve that is isolated by the two
                        blue lines in each tail.
                     </p>
                  </div>
               </div>
               <p>Now recall that The <span class="emphasis"><em>P</em></span>-value answers the following question: If the null hypothesis is true, what is the probability that chance sampling could
                  have resulted in a difference in sample means at least as extreme as the one obtained? In our experiment, the difference in
                  sample means was 11.3, where <span class="emphasis"><em>a</em></span>::GFP showed lower expression in the mutant <span class="emphasis"><em>b</em></span> background. However, to derive the <span class="emphasis"><em>P</em></span>-value for the two-tailed <span class="emphasis"><em>t</em></span>-test, we would need to include the following two possibilities, represented here in equation forms:
               </p>
               <div class="mediaobject" align="center"><a name="unfig9"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig9.jpg" align="middle" width="600"></td>
                     </tr>
                  </table>
               </div>
               <p>Most notably, with a two-tailed <span class="emphasis"><em>t</em></span>-test we impose <span class="emphasis"><em>no bias</em></span> as to the direction of the difference when carrying out the statistical analysis. Looking at <a href="#figure8">Figure 8B</a>, we can begin to see how the <span class="emphasis"><em>P</em></span>-value is calculated. Depicted is a <span class="emphasis"><em>normal curve</em></span>, with the observed difference represented by the vertical blue line located at 11.3 units (&#8764;2.3 SEs). In addition, a dashed
                  vertical blue line at &#8722;11.3 is also included. Red lines are located at about 2 SEs to either side of the apex. Based on our
                  understanding of the normal curve, we know that about 95% of the total area under the curve resides between the two red lines,
                  leaving the remaining 5% to be split between the two areas outside of the red lines in the tail regions. Furthermore, the
                  proportion of the area under the curve that is to the outside of each individual blue line is 1.3%, for a total of 2.6%. This
                  directly corresponds to the calculated two-tailed <span class="emphasis"><em>P</em></span>-value of 0.026. Thus, the probability of having observed an effect this large by mere chance is only 2.6%, and we can conclude
                  that the observed difference of 11.3 is statistically significant.
               </p>
               <p>Once you understand the idea behind the two-tailed <span class="emphasis"><em>t</em></span>-test, the one-tailed type is fairly straightforward. For the one-tailed <span class="emphasis"><em>t</em></span>-test, however, there will always be two distinct versions, each with a different research hypothesis and corresponding null
                  hypothesis. For example, if there is sufficient <span class="emphasis"><em>a priori<sup><a name="fn23" href="#ftn.fn23">23</a></sup></em></span> reason to believe that GFP<sup>wt</sup> will be greater than GFP<sup>mut <span class="emphasis"><em>b</em></span></sup>, then the research hypothesis could be written as
               </p>
               <div class="mediaobject" align="center"><a name="unfig10"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig10.jpg" align="middle" width="200"></td>
                     </tr>
                  </table>
               </div>
               <p>This means that the null hypothesis would be written as</p>
               <div class="mediaobject" align="center"><a name="unfig11"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig11.jpg" align="middle" width="200"></td>
                     </tr>
                  </table>
               </div>
               <p>Most importantly, the <span class="emphasis"><em>P</em></span>-value for this test will answer the question: If the null hypothesis is true, what is the probability that the following
                  result could have occurred by chance sampling?
               </p>
               <div class="mediaobject" align="center"><a name="unfig12"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig12.jpg" align="middle" width="280"></td>
                     </tr>
                  </table>
               </div>
               <p>Looking at <a href="#figure8">Figure 8B</a>, we can see that the answer is just the proportion of the area under the curve that lies to the <span class="emphasis"><em>right</em></span> of positive 11.3 (solid vertical blue line). Because the graph is perfectly symmetrical, the <span class="emphasis"><em>P</em></span>-value for this right-tailed test will be exactly half the value that we determined for the two-tailed test, or 0.013. Thus
                  in cases where the direction of the difference coincides with a directional research hypothesis, the <span class="emphasis"><em>P</em></span>-value of the one-tailed test will always be half that of the two-tailed test. This is a useful piece of information. Anytime
                  you see a <span class="emphasis"><em>P</em></span>-value from a one-tailed <span class="emphasis"><em>t</em></span>-test and want to know what the two-tailed value would be, simply multiply by two.
               </p>
               <p>Alternatively, had there been sufficient reason to posit <span class="emphasis"><em>a priori</em></span> that GFP<sup>mut <span class="emphasis"><em>b</em></span></sup> will be greater than GFP<sup>wt</sup>, and then the research hypothesis could be written as
               </p>
               <div class="mediaobject" align="center"><a name="unfig13"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig13.jpg" align="middle" width="200"></td>
                     </tr>
                  </table>
               </div>
               <p>Of course, our experimental result that GFP<sup>wt</sup> was greater than GFP<sup>mut <span class="emphasis"><em>b</em></span></sup> clearly fails to support this research hypothesis. In such cases, there would be no reason to proceed further with a <span class="emphasis"><em>t</em></span>-test, as the <span class="emphasis"><em>P</em></span>-value in such situations is guaranteed to be &gt;0.5. Nevertheless, for the sake of completeness, we can write out the null
                  hypothesis as
               </p>
               <div class="mediaobject" align="center"><a name="unfig14"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig14.jpg" align="middle" width="200"></td>
                     </tr>
                  </table>
               </div>
               <p>And the <span class="emphasis"><em>P</em></span>-value will answer the question: If the null hypothesis is true, what is the probability that the following result could have
                  occurred by chance sampling?
               </p>
               <div class="mediaobject" align="center"><a name="unfig15"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig15.jpg" align="middle" width="280"></td>
                     </tr>
                  </table>
               </div>
               <p>Or, written slightly differently to keep things consistent with <a href="#figure8">Figure 8B</a>,
               </p>
               <div class="mediaobject" align="center"><a name="unfig16"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig16.jpg" align="middle" width="280"></td>
                     </tr>
                  </table>
               </div>
               <p>This one-tailed test yields a <span class="emphasis"><em>P</em></span>-value of 0.987, meaning that the observed lower mean of <span class="emphasis"><em>a</em></span>::GFP in mut <span class="emphasis"><em>b</em></span> embryos is entirely consistent with a null hypothesis of GFP<sup>mut <span class="emphasis"><em>b</em></span></sup> &#8804; GFP<sup>wt</sup>.
               </p>
               <p>Interestingly, there is considerable debate, even among statisticians, regarding the appropriate use of one- versus two-tailed
                  <span class="emphasis"><em>t</em></span>-tests. Some argue that because in reality no two population means are ever identical, that all tests should be one tailed,
                  as one mean must in fact be larger (or smaller) than the other (<a href="#bib13">Jones and Tukey, 2000</a>). Put another way, the null hypothesis of a two-tailed test is always a false premise. Others encourage standard use of the
                  two-tailed test largely on the basis of its being more conservative. Namely, the <span class="emphasis"><em>P</em></span>-value will always be higher, and therefore fewer false-positive results will be reported. In addition, two-tailed tests impose
                  no preconceived bias as to the direction of the change, which in some cases could be arbitrary or based on a misconception.
                  A universally held rule is that one should never make the choice of a one-tailed <span class="emphasis"><em>t</em></span>-test <span class="emphasis"><em>post hoc</em></span><sup><a name="fn24" href="#ftn.fn24">24</a></sup> after determining which direction is suggested by your data<span class="emphasis"><em>.</em></span> In other words, if you are hoping to see a difference and your two-tailed <span class="emphasis"><em>P</em></span>-value is 0.06, don't then decide that you really intended to do a one-tailed test to reduce the <span class="emphasis"><em>P</em></span>-value to 0.03. Alternatively, if you were hoping for no significant difference, choosing the one-tailed test that happens
                  to give you the highest <span class="emphasis"><em>P</em></span>-value is an equally unacceptable practice.
               </p>
               <p>Generally speaking, one-tailed tests are often reserved for situations where a clear directional outcome is anticipated or
                  where changes in only one direction are relevant to the goals of the study. Examples of the latter are perhaps more often
                  encountered in industry settings, such as testing a drug for the alleviation of symptoms. In this case, there is no reason
                  to be interested in proving that a drug worsens symptoms, only that it improves them. In such situations, a one-tailed test
                  may be suitable. Another example would be tracing the population of an endangered species over time, where the anticipated
                  direction is clear and where the cost of being too conservative in the interpretation of data could lead to extinction. Notably,
                  for the field of <span class="emphasis"><em>C. elegans</em></span> experimental biology, these circumstances rarely, if ever, arise. In part for this reason, two-tailed tests are more common
                  and further serve to dispel any suggestion that one has manipulated the test to obtain a desired outcome.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-5"></a>2.5.&nbsp;Equal or non-equal variances
                        </h3>
                     </div>
                  </div>
               </div>
               <p>It is common to read in textbooks that one of the underlying assumptions of the <span class="emphasis"><em>t</em></span>-test is that both samples should be derived from populations of equal variance. Obviously this will often not be the case.
                  Furthermore, when using the <span class="emphasis"><em>t</em></span>-test, we are typically not asking whether or not the samples were derived from identical populations, as we already know
                  they are not. Rather, we want to know if the two independent populations from which they were derived have different means.
                  In fact, the original version of the <span class="emphasis"><em>t</em></span>-test, which does not formally take into account unequal sample variances, is nevertheless quite robust for small or even
                  moderate differences in variance. Nevertheless, it is now standard to use a modified version of the <span class="emphasis"><em>t</em></span>-test that directly adjusts for unequal variances. In most statistical programs, this may simply require checking or unchecking
                  a box. The end result is that for samples that do have similar variances, effectively no differences in <span class="emphasis"><em>P</em></span>-values will be observed between the two methods. For samples that do differ considerably in their variances, <span class="emphasis"><em>P</em></span>-values will be higher using the version that takes unequal variances into account. This method therefore provides a slightly
                  more conservative and accurate estimate for <span class="emphasis"><em>P</em></span>-values and can generally be recommended.
               </p>
               <p>Also, just to reinforce a point raised earlier, greater variance in the sample data will lead to higher <span class="emphasis"><em>P</em></span>-values because of the effect of sample variance on the SEDM. This will make it more difficult to detect differences between
                  sample means using the <span class="emphasis"><em>t</em></span>-test. Even without any technical explanation, this makes intuitive sense given that greater scatter in the data will create
                  a level of background noise that could obscure potential differences. This is particularly true if the differences in means
                  are small relative to the amount of scatter. This can be compensated for to some extent by increasing the sample size. This,
                  however, may not be practical in some cases, and there can be downsides associated with accumulating data solely for the purpose
                  of obtaining low <span class="emphasis"><em>P</em></span>-values (see <a href="#sec6-3" title="6.3.&nbsp;Can a sample size be too large?">Section 6.3</a>).
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-6"></a>2.6.&nbsp;Are the data normal enough?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Technically speaking, we know that for <span class="emphasis"><em>t</em></span>-tests to be valid, the distribution of the differences in means must be close to normal (discussed above). For this to be
                  the case, the populations from which the samples are derived must also be sufficiently close to normal. Of course we seldom
                  know the true nature of populations and can only infer this from our sample data. Thus in practical terms the question often
                  boils down to whether or not the sample data suggest that the underlying population is normal or <span class="emphasis"><em>normal enough</em></span>. The good news is that in cases where the sample size is not too small, the distribution of the sample will reasonably reflect
                  the population from which it was derived (as mentioned above). The bad news is that with small sample sizes (say below 20),
                  we may not be able to tell much about the population distribution. This creates a considerable conundrum when dealing with
                  small samples from unknown populations. For example, for certain types of populations, such as a theoretical collection of
                  bands on a western blot, we may have no way of knowing if the underlying population is normal or skewed and probably can't
                  collect sufficient data to make an informed judgment. In these situations, you would admittedly use a <span class="emphasis"><em>t</em></span>-test at your own risk<sup><a name="fn25" href="#ftn.fn25">25</a></sup>.
               </p>
               <p>Textbooks will tell you that using highly skewed data for <span class="emphasis"><em>t</em></span>-tests can lead to unreliable <span class="emphasis"><em>P</em></span>-values. Furthermore, the reliability of certain other statistics, such as CIs, can also be affected by the distribution of
                  data. In the case of the <span class="emphasis"><em>t</em></span>-test, we know that the ultimate issue isn't whether the data or populations are skewed but whether the theoretical population
                  of differences between the two means is skewed. In the examples shown earlier (<a href="#figure6">Figures 6</a> and <a href="#figure8">8</a>), the shapes of the distributions were normal, and thus the <span class="emphasis"><em>t</em></span>-tests were valid, even though our original data were skewed (<a href="#figure5">Figure 5</a>). A basic rule of thumb is that if the data are normal or only slightly skewed, then the test statistic will be normal and
                  the <span class="emphasis"><em>t</em></span>-test results will be valid, even for small sample sizes. Conversely, if one or both samples (or populations) are strongly
                  skewed, this can result in a skewed test statistic and thus invalid statistical conclusions.
               </p>
               <p>Interestingly, although increasing the sample size will not change the underlying distribution of the population, it can often
                  go a long way toward correcting for skewness in the test statistic<sup><a name="fn26" href="#ftn.fn26">26</a></sup>. Thus, the <span class="emphasis"><em>t</em></span>-test often becomes valid, even with fairly skewed data, if the sample size is large enough. In fact, using data from <a href="#figure5">Figure 5</a>, we did a simulation study and determined that the sampling distribution for the difference in means is indeed approximately
                  normal with a sample size of 30 (data not shown). In that case, the histogram of that sampling distribution looked very much
                  like that in <a href="#figure6">Figure 6B</a>, with the exception that the SD<sup><a name="fn27" href="#ftn.fn27">27</a></sup> of the distribution was &#8764;1.9 rather than 1.4. In contrast, carrying out a simulation with a sample size of only 15 did not
                  yield a normal distribution of the test statistic and thus the <span class="emphasis"><em>t</em></span>-test would not have been valid.
               </p>
               <p>Unfortunately, there is no simple rule for choosing a minimum sample size to protect against skewed data, although some textbooks
                  recommend 30. Even a sample size of 30, however, may not be sufficient to correct for skewness or kurtosis in the test statistic
                  if the sample data (i.e., populations) are severely non-normal to begin with<sup><a name="fn28" href="#ftn.fn28">28</a></sup>. The bottom line is that there are unfortunately no hard and fast rules to rely on. Thus, if you have reason to suspect that
                  your samples or the populations from which there are derived are strongly skewed, consider consulting your nearest statistician
                  for advice on how to proceed. In the end, given a sufficient sample size, you may be cleared for the <span class="emphasis"><em>t</em></span>-test. Alternatively, several classes of <span class="emphasis"><em>nonparametric tests</em></span> can potentially be used (<a href="#sec6-5" title="6.5.&nbsp;Nonparametric tests">Section 6.5</a>). Although these tests tend to be less powerful than the <span class="emphasis"><em>t</em></span>-test at detecting differences, the statistical conclusions drawn from these approaches will be much more valid. Furthermore,
                  the computationally intensive method bootstrapping retains the power of the <span class="emphasis"><em>t</em></span>-test but doesn't require a normal distribution of the test statistic to yield valid results.
               </p>
               <p>In some cases, it may also be reasonable to assume that the population distributions are normal enough. Normality, or something
                  quite close to it, is typically found in situations where many small factors serve to contribute to the ultimate distribution
                  of the population. Because such effects are frequently encountered in biological systems, many natural distributions may be
                  normal enough with respect to the <span class="emphasis"><em>t</em></span>-test. Another way around this conundrum is to simply ask a different question, one that doesn't require the <span class="emphasis"><em>t</em></span>-test approach. In fact, the western blot example is one where many of us would intuitively look toward analyzing the ratios
                  of band intensities within individual blots (discussed in <a href="#sec6-5" title="6.5.&nbsp;Nonparametric tests">Section 6.5</a>).
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-7"></a>2.7.&nbsp;Is there a minimum acceptable sample size?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>You may be surprised to learn that nothing can stop you from running a <span class="emphasis"><em>t</em></span>-test with sample sizes of two. Of course, you may find it difficult to convince anyone of the validity of your conclusion,
                  but run it you may! Another problem is that very low sample sizes will render any test <span class="emphasis"><em>much less</em></span> <span class="emphasis"><em>powerful</em></span>. What this means in practical terms is that to detect a statistically significant difference with small sample sizes, the
                  difference between the two means must be quite large. In cases where the inherent difference is not large enough to compensate
                  for a low sample size, the <span class="emphasis"><em>P</em></span>-value will likely be above the critical threshold. In this event, you might state that there is insufficient evidence to
                  indicate a difference between the populations, although there could be a difference that the experiment failed to detect.
                  Alternatively, it may be tempting to continue gathering samples to push the <span class="emphasis"><em>P</em></span>-value below the traditionally acceptable threshold of 0.05. As to whether this is a scientifically appropriate course of
                  action is a matter of some debate, although in some circumstances it may be acceptable. However, this general tactic does
                  have some grave pitfalls, which are addressed in later sections (e.g., <a href="#sec6-3" title="6.3.&nbsp;Can a sample size be too large?">Section 6.3</a>).
               </p>
               <p>One good thing about working with <span class="emphasis"><em>C. elegans</em></span>, however, is that for many kinds of experiments, we can obtain fairly large sample sizes without much trouble or expense.
                  The same cannot be said for many other biological or experimental systems. This advantage should theoretically allow us to
                  determine if our data are normal enough or to simply not care about normality since our sample sizes are high. In any event,
                  we should always strive to take advantage of this aspect of our system and not short-change our experiments. Of course, no
                  matter what your experimental system might be, issues such as convenience and expense should not be principal driving forces
                  behind the determination of sample size. Rather, these decisions should be based on logical, pragmatic, and statistically
                  informed arguments (see <a href="#sec6-2" title="6.2.&nbsp;Statistical power">Section 6.2</a> on power analysis).
               </p>
               <p>Nevertheless, there are certain kinds of common experiments, such as qRT-PCR, where a sample size of three is quite typical.
                  Of course, by three we do not mean three worms. For each sample in a qRT-PCR experiment, many thousands of worms may have
                  been used to generate a single mRNA extract. Here, three refers to the number of <span class="emphasis"><em>biological replicates</em></span>. In such cases, it is generally understood that worms for the three extracts may have been grown in parallel but were processed
                  for mRNA isolation and cDNA synthesis separately. Better yet, the templates for each biological replicate may have been grown
                  and processed at different times. In addition, qRT-PCR experiments typically require <span class="emphasis"><em>technical replicates</em></span>. Here, three or more equal-sized aliquots of cDNA from the same biological replicate are used as the template in individual
                  PCR reactions. Of course, the data from technical replicates will nearly always show less variation than data from true biological
                  replicates. <span class="underline">Importantly, technical replicates should never be confused with biological replicates.</span> In the case of qRT-PCR, the former are only informative as to the variation introduced by the pipetting or amplification
                  process. As such, technical replicates should be averaged, and this value treated as a single data point.
               </p>
               <p>In this case, suppose for the sake of discussion that each replicate contains extracts from 5,000 worms. If all 15,000 worms
                  can be considered to be from some a single population (at least with respect to the mRNA of interest), then each observed
                  value is akin to a mean from a sample of 5,000. In that case, one could likely argue that the three values do come from a
                  normal population (the <span class="emphasis"><em>Central Limit Theorem</em></span> having done its magic on each), and so a <span class="emphasis"><em>t</em></span>-test using the mean of those three values would be acceptable from a statistical standpoint. It might possibly still suffer
                  from a lack of power, but the test itself would be valid. Similarly, western blot quantitation values, which average proteins
                  from many thousands of worms, could also be argued to fall under this umbrella.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-8"></a>2.8.&nbsp;Paired versus unpaired tests
                        </h3>
                     </div>
                  </div>
               </div>
               <p>The paired <span class="emphasis"><em>t</em></span>-test is a powerful way to detect differences in two sample means, provided that your experiment has been designed to take
                  advantage of this approach. In our example of embryonic GFP expression, the two samples were <span class="emphasis"><em>independent</em></span> in that the expression within any individual embryo was not linked to the expression in any other embryo. For situations
                  involving independent samples, the paired <span class="emphasis"><em>t</em></span>-test is not applicable; we carried out an unpaired <span class="emphasis"><em>t</em></span>-test instead. For the paired method to be valid, data points must be linked in a meaningful way. If you remember from our
                  first example, worms that have a mutation in <span class="emphasis"><em>b</em></span> show lower expression of the <span class="emphasis"><em>a</em></span>::GFP reporter. In this example of a paired <span class="emphasis"><em>t</em></span>-test, consider a strain that carries a construct encoding a hairpin dsRNA corresponding to gene <span class="emphasis"><em>b</em></span>. Using a specific promoter and the appropriate genetic background, the dsRNA will be expressed only in the rightmost cell
                  of one particular neuronal pair, where it is expected to inhibit the expression of gene <span class="emphasis"><em>b</em></span> via the RNAi response. In contrast, the neuron on the left should be unaffected. In addition, this strain carries the same
                  <span class="emphasis"><em>a</em></span>::GFP reporter described above, and it is known that this reporter is expressed in both the left and right neurons at identical
                  levels in wild type. The experimental hypothesis is therefore that, analogous to what was observed in embryos, fluorescence
                  of the <span class="emphasis"><em>a</em></span>::GFP reporter will be weaker in the right neuron, where gene <span class="emphasis"><em>b</em></span> has been inhibited.
               </p>
               <p>In this scenario, the data are meaningfully paired in that we are measuring GFP levels in two distinct cells, but within a
                  single worm. We then collect fluorescence data from 14 wild-type worms and 14 <span class="emphasis"><em>b(RNAi)</em></span> worms. A visual display of the data suggests that expression of <span class="emphasis"><em>a</em></span>::GFP is perhaps slightly decreased in the right cell where gene <span class="emphasis"><em>b</em></span> has been inhibited, but the difference between the control and experimental dataset is not very impressive (<a href="#figure9">Figure 9A, B</a>). Furthermore, whereas the means of GFP expression in the left neurons in wild-type and <span class="emphasis"><em>b(RNAi)</em></span> worms are nearly identical, the mean of GFP expression in the right neurons in wild type is a bit higher than that in the
                  right neurons of <span class="emphasis"><em>b(RNAi)</em></span> worms. For our <span class="emphasis"><em>t</em></span>-test analysis, one option would be to ignore the natural pairing in the data and treat left and right cells of individual
                  animals as independent. In doing so, however, we would hinder our ability to detect real differences. The reason is as follows.
                  We already know that GFP expression in some worms will happen to be weaker or stronger (resulting in a dimmer or brighter
                  signal) than in other worms. This variability, along with a relatively small mean difference in expression, may preclude our
                  ability to support differences statistically. In fact, a two-tailed <span class="emphasis"><em>t</em></span>-test using the (hypothetical) data for right cells from wild-type and <span class="emphasis"><em>b(RNAi)</em></span> strains (<a href="#figure9">Figure 9B</a>) turns out to give a <span class="emphasis"><em>P</em></span> &gt; 0.05.
               </p>
               <div class="mediaobject" align="center"><a name="figure9"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig9.jpg"><img src="statisticalanalysis_fig9_s.jpg" border="2" align="middle" alt=" figure 9"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 9. Representation of paired data.&nbsp;</b></p>
                  </div>
               </div>
               <p><a href="#figure9">Figure 9C, D</a>, in contrast, shows a slightly different arrangement of the same GFP data. Here the wild-type and <span class="emphasis"><em>b(RNAi)</em></span> strains have been separated, and we are specifically comparing expression in the left and right neurons for each genotype.
                  In addition, lines have been drawn between left and right data points from the same animal. Two things are quite striking.
                  One is that worms that are bright in one cell tend to be bright in the other. Second, looking at <span class="emphasis"><em>b(RNAi)</em></span> worms, we can see that within individuals, there is a strong tendency to have reduced expression in the right neuron as compared
                  with its left counterpart (<a href="#figure9">Figure 9D</a>). However, because of the inherent variability between worms, this difference was largely obscured when we failed to make
                  use of the paired nature of the experiment. This wasn't a problem in the embryonic analysis, because the difference between
                  wild type and <span class="emphasis"><em>b</em></span> mutants was large enough relative to the variability between embryos. In the case of neurons (and the use of RNAi), the difference
                  was, however, much smaller and thus fell below the level necessary for statistical validation. Using a paired two-tailed <span class="emphasis"><em>t</em></span>-test for this dataset gives a <span class="emphasis"><em>P</em></span> &lt; 0.01.
               </p>
               <p>The rationale behind using the paired <span class="emphasis"><em>t</em></span>-test is that it takes meaningfully linked data into account when calculating the <span class="emphasis"><em>P</em></span>-value. The paired <span class="emphasis"><em>t</em></span>-test works by first calculating the difference between each individual pair. Then a mean and variance are calculated for
                  all the differences among the pairs. Finally, a one-sample <span class="emphasis"><em>t</em></span>-test is carried out where the null hypothesis is that the mean of the differences is equal to zero. Furthermore, the paired
                  <span class="emphasis"><em>t</em></span>-test can be one- or two-tailed, and arguments for either are similar to those for two independent means. Of course, standard
                  programs will do all of this for you, so the inner workings are effectively invisible. Given the enhanced power of the paired
                  <span class="emphasis"><em>t</em></span>-test to detect differences, it is often worth considering how the statistical analysis will be carried out at the stage when
                  you are developing your experimental design. Then, if it's feasible, you can design the experiment to take advantage of the
                  paired <span class="emphasis"><em>t</em></span>-test method.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec2-9"></a>2.9.&nbsp;The critical value approach
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Some textbooks, particularly older ones, present a method known as the <span class="emphasis"><em>critical value</em></span> approach in conjunction with the <span class="emphasis"><em>t</em></span>-test. This method, which traditionally involves looking up <span class="emphasis"><em>t</em></span>-values in lengthy appendices, was developed long before computer software was available to calculate precise <span class="emphasis"><em>P</em></span>-values. Part of the reason this method may persist, at least in some textbooks, is that it provides authors with a vehicle
                  to explain the basis of hypothesis testing along with certain technical aspects of the <span class="emphasis"><em>t</em></span>-test. As a modern method for analyzing data, however, it has long since gone the way of the dinosaur. Feel no obligation
                  to learn this.
               </p>
            </div>
            <div class="footnotes"><br><hr width="100" align="left">
               <div class="footnote">
                  <p><sup><a name="ftn.fn13" href="#fn13">13</a></sup>The term &#8220;statistically significant&#8221;, when applied to the results of a statistical test for a difference between two means,
                     implies only that it is plausible that the observed difference (i.e., the difference that arises from the data) likely represents
                     a difference that is real. It does not imply that the difference is &#8220;biologically significant&#8221; (i.e., important). A better
                     phrase would be &#8220;statistically plausible&#8221; or perhaps &#8220;statistically supported&#8221;. Unfortunately, &#8220;statistically significant&#8221;
                     (in use often shortened to just &#8220;significant&#8221;) is so heavily entrenched that it is unlikely we can unseat it. It's worth a
                     try, though. Join us, won't you?
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn14" href="#fn14">14</a></sup>When William Gossett introduced the test, it was in the context of his work for Guinness Brewery. To prevent the dissemination
                     of trade secrets and/or to hide the fact that they employed statisticians, the company at that time had prohibited the publication
                     of any articles by their employees. Gossett was allowed an exception, but the higher-ups insisted that he use a pseudonym.
                     He chose the unlikely moniker &#8220;Student&#8221;.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn15" href="#fn15">15</a></sup>These are measured by the number of pixels showing fluorescence in a viewing area of a specified size. We will use &#8220;billions
                     of pixels&#8221; as our unit of measurement.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn16" href="#fn16">16</a></sup>More accurately, it is the distribution of the underlying populations that we are really concerned with, although this can
                     usually only be inferred from the sample data.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn17" href="#fn17">17</a></sup>For data sets with distributions that are perfectly symmetric, the skewness will be zero. In this case the mean and median
                     of the data set are identical. For left-skewed distributions, the mean is less than the median and the skewness will be a
                     negative number. For right-skewed distributions, the mean is more than the median and the skewness will be a positive number.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn18" href="#fn18">18</a></sup>Kurtosis describes the shape or &#8220;peakedness&#8221; of the data set. In the case of a normal distribution, this number is zero. Distributions
                     with relatively sharp peaks and long tails will have a positive kurtosis value whereas distributions with relatively flat
                     peaks and short tails will have a negative kurtosis value.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn19" href="#fn19">19</a></sup>A-squared (A2) refers to a numerical value produced by the Anderson-Darling test for normality. The test ultimately generates
                     an approximate P-value where the null hypothesis is that the data are derived from a population that is normal. In the case
                     of the data in <a href="#figure5">Figure 5</a>, the conclusion is that there is &lt; 0.5% chance that the sample data were derived from a normal population. The conclusion
                     of non-normality can also be reached informally by a visual inspection of the histograms. The Anderson-Darling test does not
                     indicate whether test statistics generated by the sample data will be sufficiently normal.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn20" href="#fn20">20</a></sup>The list is long, but it includes coefficients in regression models and estimated binomial proportions (and differences in
                     proportions from two independent samples). For an illustration of this phenomenon for proportions, see <a href="#figure12">Figure 12</a> and discussion thereof.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn21" href="#fn21">21</a></sup>There are actually many Central Limit Theorems, each with the same conclusion: normality prevails for the distribution of
                     the statistic under consideration. Why many? This is so mainly because details of the proof of the theorem depend on the particular
                     statistical context.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn22" href="#fn22">22</a></sup>And, as we all know, good judgment comes from experience, and experience comes from bad judgment.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn23" href="#fn23">23</a></sup>Meaning reasons based on prior experience.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn24" href="#fn24">24</a></sup>Meaning &#8220;after the fact&#8221;.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn25" href="#fn25">25</a></sup>Also see discussion on sample sizes (<a href="#sec2-7" title="2.7.&nbsp;Is there a minimum acceptable sample size?">Section 2.7</a>) and <a href="#sec5" title="5.&nbsp;Relative differences, ratios, and correlations">Section 5</a> for a more complete discussion of issues related to western blots.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn26" href="#fn26">26</a></sup>This is due to a statistical &#8220;law of gravity&#8221; called the Central Limit Theorem: as the sample size gets larger, the distribution
                     of the sample mean (i.e., the distribution you would get if you repeated the study ad infinitum) becomes more and more like
                     a normal distribution.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn27" href="#fn27">27</a></sup>Estimated from the data; again, this is also called the SEDM.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn28" href="#fn28">28</a></sup>In contrast, you can, with data from sample sizes that are not too small, ask whether they (the data and, hence, the population
                     from whence they came) are normal enough. Judging this requires experience, but, in essence, the larger the sample size, the
                     less normal the distribution can be without causing much concern.
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec3"></a>3.&nbsp;Comparisons of more than two means
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-1"></a>3.1.&nbsp;Introduction
                        </h3>
                     </div>
                  </div>
               </div>
               <p>The two-sample <span class="emphasis"><em>t</em></span>-test works well in situations where we need to determine if differences exist between two populations for which we have sample
                  means. But what happens when our analyses involve comparisons between three or more separate populations? Here things can
                  get a bit tricky. Such scenarios tend to arise in one of several ways. Most common to our field is that we have obtained a
                  collection of sample means that we want to compare with a single standard. For example, we might measure the body lengths
                  of young adult-stage worms grown on RNAi-feeding plates, each targeting one of 100 different collagen genes. In this case,
                  we would want to compare mean lengths from animals grown on each plate with a control RNAi that is known to have no effect
                  on body length. On the surface, the statistical analysis might seem simple: just carry out 100 two-sample <span class="emphasis"><em>t</em></span>-tests where the average length from each collagen RNAi plate is compared with the same control. The problem with this approach
                  is the unavoidable presence of <span class="emphasis"><em>false-positive findings</em></span> (also known as <span class="emphasis"><em>Type I errors</em></span>). The more <span class="emphasis"><em>t</em></span>-tests you run, the greater the chance of obtaining a statistically significant result through chance sampling. Even if all
                  the populations were identical in their lengths, 100 <span class="emphasis"><em>t</em></span>-tests would result on average in five RNAi clones showing differences supported by <span class="emphasis"><em>P-</em></span>values of &lt;0.05, including one clone with a <span class="emphasis"><em>P</em></span>-value of &lt;0.01. This type of <span class="emphasis"><em>multiple comparisons problem</em></span> is common in many of our studies and is a particularly prevalent issue in high-throughput experiments such as microarrays,
                  which typically involve many thousands of comparisons.
               </p>
               <p>Taking a slightly different angle, we can calculate the probability of incurring <span class="emphasis"><em>at least one</em></span> <span class="emphasis"><em>false significance</em></span> in situations of multiple comparisons. For example, with just two <span class="emphasis"><em>t</em></span>-tests and a significance threshold of 0.05, there would be an &#8764;10% chance<sup><a name="fn29" href="#ftn.fn29">29</a></sup> that we would obtain at least one <span class="emphasis"><em>P</em></span>-value that was &lt;0.05 just by chance [1 &#8211; (0.95)<sup>2</sup> = 0.0975]. With just fourteen comparisons, that probability leaps to &gt;50% (1 &#8211; (0.95)<sup>14</sup> = 0.512). With 100 comparisons, there is a 99% chance of obtaining at least one statistically significant result by chance.
                  Using probability calculators available on the web (also see <a href="#sec4-10" title="4.10.&nbsp;Tests for differences between more than one binomial proportion">Section 4.10</a>), we can determine that for 100 tests there is a 56.4% chance of obtaining five or more false positives and a 2.8% chance
                  of obtaining ten or more. Thinking about it this way, we might be justifiably concerned that our studies may be riddled with
                  incorrect conclusions! Furthermore, reducing the chosen significance threshold to 0.01 will only help so much. In this case,
                  with 50 comparisons, there is still an &#8764;40% probability that at least one comparison will sneak below the cutoff by chance.
                  Moreover, by reducing our threshold, we run the risk of discarding results that are both statistically and biologically significant.
               </p>
               <p>A related but distinct situation occurs when we have a collection of sample means, but rather than comparing each of them
                  to a single standard, we want to compare all of them to each other. As is with the case of multiple comparisons to a single
                  control, the problem lies in the sheer number of tests required. With only five different sample means, we would need to carry
                  out 10 individual <span class="emphasis"><em>t</em></span>-tests to analyze all possible pair-wise comparisons [5(5 &#8722; 1)/2 = 10]. With 100 sample means, that number skyrockets to 4,950.
                  (100(100 &#8722; 1)/2 = 4,950). Based on a significance threshold of 0.05, this would lead to about 248 statistically significant
                  results occurring by mere chance! Obviously, both common sense, as well as the use of specialized statistical methods, will
                  come into play when dealing with these kinds of scenarios. In the sections below, we discuss some of the underlying concepts
                  and describe several practical approaches for handling the analysis of multiple means.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-2"></a>3.2.&nbsp;Safety through repetition
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Before delving into some of the common approaches used to cope with multiple comparisons, it is worth considering an experimental
                  scenario that would likely not require specialized statistical methods. Specifically, we may consider the case of a large-scale
                  &#8220;functional genomics screen&#8221;. In <span class="emphasis"><em>C. elegans</em></span>, these would typically be carried out using RNAi-feeding libraries (<a href="#bib14">Kamath et al., 2003</a>) or chemical genetics (<a href="#bib8">Carroll et al., 2003</a>) and may involve many thousands of comparisons. For example, if 36 RNAi clones are ultimately identified that lead to resistance
                  to a particular bacterial pathogen from a set of 17,000 clones tested, how does one analyze this result? No worries: the methodology
                  is not composed of 17,000 statistical tests (each with some chance of failing). That's because the final reported tally, 36
                  clones, was presumably not the result of a single round of screening. In the first round (the primary screen), a larger number
                  (say, 200 clones) might initially be identified as possibly resistant (with some &#8220;false significances&#8221; therein). A second
                  or third round of screening would effectively eliminate virtually all of the false positives, reducing the number of clones
                  that show a consistent biological affect to 36. In other words, secondary and tertiary screening would reduce to near zero
                  the chance that any of the clones on the final list are in error because the chance of getting the same false positives repeatedly
                  would be very slim. This idea of &#8220;safety through independent experimental repeats&#8221; is also addressed in <a href="#sec4-10" title="4.10.&nbsp;Tests for differences between more than one binomial proportion">Section 4.10</a> in the context of proportion data. Perhaps more than anything else, carrying out independent repeats is often best way to
                  solidify results and avoid the presence of false positives within a dataset.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-3"></a>3.3.&nbsp;The family-wise error rate
                        </h3>
                     </div>
                  </div>
               </div>
               <p>To help grapple with the problems inherent to multiple comparisons, statisticians came up with something called the <span class="emphasis"><em>family-wise error rate</em></span>. This is also sometimes referred to as the <span class="emphasis"><em>family-wide error rate</em></span>, which may provide a better description of the underlying intent. The basic idea is that rather than just considering each
                  comparison in isolation, a statistical cutoff is applied that takes into account the entire collection of comparisons. Recall
                  that in the case of individual comparisons (sometimes called the <span class="emphasis"><em>per-comparison</em></span> or <span class="emphasis"><em>comparison-wise error rate</em></span>), a <span class="emphasis"><em>P</em></span>-value of &lt;0.05 tells us that for that particular comparison, there is less than a 5% chance of having obtained a difference
                  at least as large as the one observed by chance. Put another way, in the absence of any real difference between two populations,
                  there is a 95% chance that we will not render a false conclusion of statistical significance. In the family-wise error rate
                  approach, the criterion used to judge the statistical significance of any individual comparison is made more stringent as
                  a way to compensate for the total number of comparisons being made. This is generally done by lowering the <span class="emphasis"><em>P</em></span>-value cutoff (&#945; level) for individual <span class="emphasis"><em>t</em></span>-tests. When all is said and done, a <span class="emphasis"><em>P</em></span>-value of &lt;0.05 will mean that there is less than a 5% chance that the entire collection of declared positive findings contains
                  any false positives.
               </p>
               <p>We can use our example of the collagen experiment to further illustrate the meaning of the family-wise error rate. Suppose
                  we test 100 genes and apply a family-wise error rate cutoff of 0.05. Perhaps this leaves us with a list of 12 genes that lead
                  to changes in body size that are deemed <span class="emphasis"><em>statistically significant</em></span>. This means that there is only a 5% chance that one or more of the 12 genes identified is a false positive. This also means
                  that if none of the 100 genes really controlled body size, then 95% of the time our experiment would lead to no positive findings.
                  Without this kind of adjustment, and using an &#945; level of 0.05 for individual tests, the presence of one or more false positives
                  in a data set based on 100 comparisons could be expected to happen &gt;99% of the time. Several techniques for applying the family-wise
                  error rate are described below.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-4"></a>3.4.&nbsp;Bonferroni-type corrections
                        </h3>
                     </div>
                  </div>
               </div>
               <p>The <span class="emphasis"><em>Bonferroni method</em></span>, along with several related techniques, is conceptually straightforward and provides conservative family-wise error rates.
                  To use the Bonferroni method, one simply divides the chosen family-wise error rate (e.g., 0.05) by the number of comparisons
                  to obtain a Bonferroni-adjusted <span class="emphasis"><em>P</em></span>-value cutoff. Going back to our example of the collagen genes, if the desired family-wise error rate is 0.05 and the number
                  of comparisons is 100, the adjusted per-comparison significance threshold would be reduced to 0.05/100 = 0.0005. Thus, individual
                  <span class="emphasis"><em>t</em></span>-tests yielding <span class="emphasis"><em>P</em></span>-values as low as 0.0006 would be declared insignificant. This may sound rather severe. In fact, a real problem with the Bonferroni
                  method is that for large numbers of comparisons, the significance threshold may be so low that one may fail to detect a substantial
                  proportion of true positives within a data set. For this reason, the Bonferroni method is widely considered to be too conservative
                  in situations with large numbers of comparisons.
               </p>
               <p>Another variation on the Bonferroni method is to apply significance thresholds for each comparison in a non-uniform manner.
                  For example, with a family-wise error rate of 0.05 and 10 comparisons, a uniform cutoff would require any given <span class="emphasis"><em>t</em></span>-test to have an associated <span class="emphasis"><em>P</em></span>-value of &lt;0.005 to be declared significant. Another way to think about this is that the sum of the 10 individual cutoffs
                  must add up to 0.05. Interestingly, the integrity of the family-wise error rate is not compromised if one were to apply a
                  0.04 significance threshold for one comparison, and a 0.00111 (0.01/9) significance threshold for the remaining nine. This
                  is because 0.04 + 9(0.00111) &#8776; 0.05. The rub, however, is that the decision to apply non-uniform significance cutoffs cannot
                  be made <span class="emphasis"><em>post hoc</em></span> based on how the numbers shake out! For this method to be properly implemented, researchers must first prioritize comparisons
                  based on the perceived importance of specific tests, such as if a negative health or environmental consequence could result
                  from failing to detect a particular difference. For example, it may be more important to detect a correlation between industrial
                  emissions and childhood cancer rates than to effects on the rate of tomato ripening. This may all sound rather arbitrary,
                  but it is nonetheless statistically valid.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-5"></a>3.5.&nbsp;False discovery rates
                        </h3>
                     </div>
                  </div>
               </div>
               <p>As discussed above, the Bonferroni method runs into trouble in situations where many comparisons are being made because a
                  substantial proportion of true positives are likely to be discarded for failing to score below the adjusted significance threshold.
                  Stated another way, the <span class="emphasis"><em>power</em></span> of the experiment to detect real differences may become unacceptably low. <a href="#bib5">Benjamini and Hochberg (1995)</a> are credited with introducing the idea of the <span class="emphasis"><em>false discovery rate (FDR)</em></span>, which has become an indispensable approach for handling the statistical analysis of experiments composed of large numbers
                  of comparisons. Importantly, the FDR method has greater power than does the Bonferroni method. In the vernacular of the FDR
                  method, a statistically significant finding is termed a &#8220;discovery&#8221;. Ultimately, the FDR approach allows the investigator
                  to set an acceptable level of <span class="emphasis"><em>false discoveries</em></span> (usually 5%), which means that any declared significant finding has a 5% chance of being a false positive. This differs fundamentally
                  from the idea behind the family-wise model, where an error rate of 5% means that there is a 5% chance that any of the declared
                  significant findings are false. The latter method starts from the position that no differences exist. The FDR method does
                  not suppose this.
               </p>
               <p>The FDR method is carried out by first making many pairwise comparisons and then ordering them according to their associated
                  <span class="emphasis"><em>P</em></span>-values, with lowest to highest displayed in a top to bottom manner. In the examples shown in <a href="#table3" title="Table 3. Illustration of FDR method, based on artificial P-values from 10 comparisons.">Table 3</a>, this was done with only 10 comparisons (for three different data sets), but this method is more commonly applied to studies
                  involving hundreds or thousands of comparisons. What makes the FDR method conceptually unique is that each of the test-derived
                  <span class="emphasis"><em>P</em></span>-values is measured against a different significance threshold. In the example with 10 individual tests, the one giving the
                  lowest <span class="emphasis"><em>P</em></span>-value is measured against<sup><a name="fn30" href="#ftn.fn30">30</a></sup> 0.005 (0.05/10). Conversely, the highest <span class="emphasis"><em>P</em></span>-value is measured against 0.05. With ten comparisons, the other significance thresholds simply play out in ordered increments
                  of 0.005 (<a href="#table3" title="Table 3. Illustration of FDR method, based on artificial P-values from 10 comparisons.">Table 3</a>). For example, the five significance thresholds starting from the top of the list would be 0.005, 0.010, 0.015, 0.020, and
                  0.025. The formula is <span class="emphasis"><em>k</em></span>(&#945;/<span class="emphasis"><em>C</em></span>), where <span class="emphasis"><em>C</em></span> is the number of comparisons and <span class="emphasis"><em>k</em></span> is the rank order (by sorted <span class="emphasis"><em>P</em></span>-values) of the comparison. If 100 comparisons were being made, the highest threshold would still be 0.05, but the lowest
                  five in order would be 0.0005, 0.0010, 0.0015, 0.0020, and 0.0025. Having paired off each experimentally derived <span class="emphasis"><em>P</em></span>-value with a different significance threshold, one checks to see if the <span class="emphasis"><em>P</em></span>-value is less than the prescribed threshold. If so, then the difference is declared to be statistically significant (a discovery),
                  at which point one moves on to the next comparison, involving the second-lowest <span class="emphasis"><em>P</em></span>-value. This process continues until a <span class="emphasis"><em>P</em></span>-value is found that is higher than the corresponding threshold. At that point, this and all remaining results are deemed
                  not significant.
               </p>
               <div class="table"><a name="table3"></a><p class="title">Table 3. Illustration of FDR method, based on artificial <span class="emphasis"><em>P</em></span>-values from 10 comparisons.
                  </p>
                  <table summary="Table 3. Illustration of FDR method, based on artificial P-values from 10 comparisons." border="1">
                     <colgroup>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                     </colgroup>
                     <thead>
                        <tr valign="bottom">
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>Comparison</strong></span></th>
                           <th rowspan="2" align="center" valign="bottom"><span class="bold"><strong>FDR Critical Value</strong></span></th>
                           <th colspan="3" align="center" valign="bottom"><span class="bold"><strong><span class="emphasis"><em>P</em></span>-values</strong></span></th>
                        </tr>
                        <tr valign="bottom">
                           <th align="center" valign="bottom"><span class="bold"><strong>Data Set #1</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Data Set #2</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Data Set #3</strong></span></th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>1</strong></span></td>
                           <td align="left" valign="top">0.005</td>
                           <td align="left" valign="top">0.001*</td>
                           <td align="left" valign="top">0.004*</td>
                           <td align="left" valign="top">0.006</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>2</strong></span></td>
                           <td align="left" valign="top">0.010</td>
                           <td align="left" valign="top">0.003*</td>
                           <td align="left" valign="top">0.008*</td>
                           <td align="left" valign="top">0.008</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>3</strong></span></td>
                           <td align="left" valign="top">0.015</td>
                           <td align="left" valign="top">0.012*</td>
                           <td align="left" valign="top">0.014*</td>
                           <td align="left" valign="top">0.011</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>4</strong></span></td>
                           <td align="left" valign="top">0.020</td>
                           <td align="left" valign="top">0.015*</td>
                           <td align="left" valign="top">0.048</td>
                           <td align="left" valign="top">0.019</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>5</strong></span></td>
                           <td align="left" valign="top">0.025</td>
                           <td align="left" valign="top">0.019*</td>
                           <td align="left" valign="top">0.210</td>
                           <td align="left" valign="top">0.020</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>6</strong></span></td>
                           <td align="left" valign="top">0.030</td>
                           <td align="left" valign="top">0.022*</td>
                           <td align="left" valign="top">0.346</td>
                           <td align="left" valign="top">0.025</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>7</strong></span></td>
                           <td align="left" valign="top">0.035</td>
                           <td align="left" valign="top">0.034*</td>
                           <td align="left" valign="top">0.719</td>
                           <td align="left" valign="top">0.111</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>8</strong></span></td>
                           <td align="left" valign="top">0.040</td>
                           <td align="left" valign="top">0.056</td>
                           <td align="left" valign="top">0.754</td>
                           <td align="left" valign="top">0.577</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>9</strong></span></td>
                           <td align="left" valign="top">0.045</td>
                           <td align="left" valign="top">0.127</td>
                           <td align="left" valign="top">0.810</td>
                           <td align="left" valign="top">0.636</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top"><span class="bold"><strong>10</strong></span></td>
                           <td align="left" valign="top">0.050</td>
                           <td align="left" valign="top">0.633</td>
                           <td align="left" valign="top">0.985</td>
                           <td align="left" valign="top">0.731</td>
                        </tr>
                        <tr valign="top">
                           <td colspan="5" align="left" valign="top">
                              <p>The highlighted values indicate the first <span class="emphasis"><em>P</em></span>-value that is larger than the significance threshold (i.e., the FDR critical value)].
                              </p>
                              
                              <p>*Comparisons that were declared significant by the method.</p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               <p>Examples of how this can play out are shown in <a href="#table3" title="Table 3. Illustration of FDR method, based on artificial P-values from 10 comparisons.">Table 3</a>. Note that even though some of the comparisons below the first failed test may themselves be less than their corresponding
                  significance thresholds (Data Set #3), these tests are nevertheless declared not significant. This may seem vexing, but without
                  this property the test would not work. This is akin to a &#8220;one strike and you're out&#8221; rule. Put another way, that test, along
                  with all those below it on the list, are declared <span class="emphasis"><em>persona non grata</em></span> and asked to leave the premises!
               </p>
               <p>Although the FDR approach is not hugely intuitive, and indeed the logic is not easily tractable, it is worth considering several
                  scenarios to see how the FDR method might play out. For example, with 100 independent tests of two populations that are identical,
                  chance sampling would be expected to result on average with a single <span class="emphasis"><em>t</em></span>-test having an associated <span class="emphasis"><em>P</em></span>-value of 0.01<sup><a name="fn31" href="#ftn.fn31">31</a></sup>. However, given that the corresponding significance threshold would be 0.0005, this test would not pass muster and the remaining
                  tests would also be thrown out. Even if by chance a <span class="emphasis"><em>P</em></span>-value of &lt;0.0005 was obtained, the next likely lowest <span class="emphasis"><em>P</em></span>-value on the list, 0.02, would be measured against 0.001, underscoring that the FDR method will be effective at weeding out
                  imposters. Next, consider the converse situation: 100 <span class="emphasis"><em>t</em></span>-tests carried out on two populations that are indeed different. Furthermore, based on the magnitude of the difference and
                  the chosen sample size, we would expect to obtain an average <span class="emphasis"><em>P</em></span>-value of 0.01 for all the tests. Of course, chance sampling will lead to some experimental differences that result in <span class="emphasis"><em>P</em></span>-values that are higher or lower than 0.01, including on average one that is 0.0001 (0.01/100). Because this is less than
                  the cutoff of 0.0005, this would be classified as a discovery, as will many, though not all, of the tests on this particular
                  list. Thus, the FDR approach will also render its share of false-negative conclusions (often referred to as Type II errors).
                  But compared with the Bonferroni method, where the significance threshold always corresponds to the lowest FDR cutoff, the
                  proportion of these errors will be much smaller.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-6"></a>3.6.&nbsp;Analysis of variance
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Entire books are devoted to the statistical method known as <span class="emphasis"><em>analysis of variance<sup><a name="fn32" href="#ftn.fn32">32</a></sup> (ANOVA)</em></span>. This section will contain only three paragraphs. This is in part because of the view of some statisticians that ANOVA techniques
                  are somewhat dated or at least redundant with other methods such as <span class="emphasis"><em>multiple regression</em></span> (see <a href="#sec5-5" title="5.5.&nbsp;Modeling and regression">Section 5.5</a>). In addition, a casual perusal of the worm literature will uncover relatively scant use of this method. Traditionally, an
                  ANOVA answers the following question: are any of the mean values within a dataset likely to be derived from populations<sup><a name="fn33" href="#ftn.fn33">33</a></sup> that are truly different? Correspondingly, the null hypothesis for an ANOVA is that all of the samples are derived from populations,
                  whose means are identical and that any difference in their means are due to chance sampling. Thus, an ANOVA will implicitly
                  compare all possible pairwise combinations of samples to each other in its search for differences. Notably, in the case of
                  a positive finding, an ANOVA will not directly indicate which of the populations are different from each other. An ANOVA tells
                  us only that at least one sample is likely to be derived from a population that is different from at least one other population.
               </p>
               <p>Because such information may be less than totally satisfying, an ANOVA is often used in a two-tiered fashion with other tests;
                  these latter tests are sometimes referred to as <span class="emphasis"><em>post hoc tests</em></span>. In cases where an ANOVA suggests the presence of different populations, <span class="emphasis"><em>t</em></span>-tests or other procedures (described below) can be used to identify differences between specific populations. Moreover, so
                  long as the <span class="emphasis"><em>P</em></span>-value associated with the ANOVA is below the chosen significance threshold, the two means that differ by the greatest amount
                  are assured of being supported by further tests. The correlate, however, is not true. Namely, it is possible to &#8220;cherry pick&#8221;
                  two means from a data set (e.g., those that differ by the greatest amount) and obtain a P value that is &lt;0.05 based on a <span class="emphasis"><em>t</em></span>-test even if the <span class="emphasis"><em>P</em></span>-value of the ANOVA (which simultaneously takes into account all of the means) is &gt;0.05. Thus, ANOVA will provide a more conservative
                  interpretation than <span class="emphasis"><em>t</em></span>-tests using chosen pairs of means. Of course, focusing on certain comparisons may be perfectly valid in some instances (see
                  discussion of planned comparisons below). In fact, it is generally only in situations where there is insufficient <span class="emphasis"><em>structure</em></span> among treatment groups to inspire particular comparisons where ANOVA is most applicable. In such cases, an insignificant
                  ANOVA finding might indeed be grounds for proceeding no further.
               </p>
               <p>In cases of a positive ANOVA finding, a commonly used <span class="emphasis"><em>post hoc</em></span> method is <span class="emphasis"><em>Tukey's test</em></span>, which goes by a number of different names including <span class="emphasis"><em>Tukey's honest significant difference test</em></span> and the <span class="emphasis"><em>Tukey-Kramer</em></span> <span class="emphasis"><em>test</em></span>. The output of this test is a list of 95% CIs of the <span class="emphasis"><em>differences between means</em></span> for all possible pairs of populations. Real differences between populations are indicated when the 95% CI for a given comparison
                  does not include zero. Moreover, because of the family-wise nature of this analysis, the entire set of comparisons has only
                  a 5% chance of containing any false positives. As is the case for other methods for multiple comparisons, the chance of obtaining
                  <span class="emphasis"><em>false negatives</em></span> increases with the number of populations being tested, and, with <span class="emphasis"><em>post hoc</em></span> ANOVA methods, this increase is typically exponential. For Tukey's test, the effect of increasing the number of populations
                  is manifest as a widening of 95% CIs, such that a higher proportion will encompass zero. Tukey's test does have more power
                  than the Bonferroni method but does not generate precise <span class="emphasis"><em>P</em></span>-values for specific comparisons. To get some idea of significance levels, however, one can run Tukey's test using several
                  different family-wise significance thresholds (0.05, 0.01, etc.) to see which comparisons are significant at different thresholds.
                  In addition to Tukey's test, many other methods have been developed for <span class="emphasis"><em>post hoc</em></span> ANOVA including Dunnett's test, Holm's test, and Scheffe's test. Thus if your analyses take you heavily into the realm of
                  the ANOVA, it may be necessary to educate yourself about the differences between these approaches.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-7"></a>3.7.&nbsp;Summary of multiple comparisons methods
                        </h3>
                     </div>
                  </div>
               </div>
               <p><a href="#figure10">Figure 10</a> provides a visual summary of the multiple comparisons methods discussed above. As can be seen, the likelihood of falsely
                  declaring a result to be statistically significant is highest when conducting multiple <span class="emphasis"><em>t</em></span>-tests without corrections and lowest using Bonferroni-type methods. Conversely, incorrectly concluding no significant difference
                  even when one exists is most likely to occur using the Bonferroni method. Thus the Bonferroni method is the most conservative
                  of the approaches discussed, with FDR occupying the middle ground. Additionally, there is no rule as to whether the uniform
                  or non-uniform Bonferroni method will be more conservative as this will always be situation dependent. Though discussed above,
                  ANOVA has been omitted from <a href="#figure10">Figure 10</a> since this method does not apply to individual comparisons. Nevertheless, it can be posited that ANOVA is more conservative
                  than uncorrected multiple <span class="emphasis"><em>t</em></span>-tests and less conservative than Bonferroni methods. Finally, we can note that the <span class="emphasis"><em>statistical power</em></span> of an analysis is lowest when using approaches that are more conservative (discussed further in <a href="#sec6-2" title="6.2.&nbsp;Statistical power">Section 6.2</a>).
               </p>
               <div class="mediaobject" align="center"><a name="figure10"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig10.jpg"><img src="statisticalanalysis_fig10_s.jpg" border="2" align="middle" alt=" figure 10"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 10. Strength versus weakness comparison of statistical methods used for analyzing multiple means.&nbsp;</b></p>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-8"></a>3.8.&nbsp;When are multiple comparison adjustments not required?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>There is no law that states that all possible comparisons must be made. It is perfectly permissible to choose a small subset
                  of the comparisons for analysis, provided that this decision is made prior to generating the data and not afterwards based
                  on how the results have played out! In addition, with certain datasets, only certain comparisons may make biological sense
                  or be of interest. Thus one can often focus on a subset of relevant comparisons. As always, common sense and a clear understanding
                  of the biology is essential. These situations are sometimes referred to as <span class="emphasis"><em>planned comparisons,</em></span> thus emphasizing the requisite premeditation. An example might be testing for the effect on longevity of a particular gene
                  that you have reason to believe controls this process. In addition, you may include some negative controls as well as some
                  &#8220;long-shot&#8221; candidates that you deduced from reading the literature. The fact that you included all of these conditions in
                  the same experimental run, however, would not necessarily obligate you to compensate for multiple comparisons when analyzing
                  your data.
               </p>
               <p>In addition, when the results of multiple tests are internally consistent, multiple comparison adjustments are often not needed.
                  For example, if you are testing the ability of gene X loss of function to suppress a gain-of-function mutation in gene Y,
                  you may want to test multiple mutant alleles of gene X as well as RNAi targeting several different regions of X. In such cases,
                  you may observe varying degrees of genetic suppression under all the tested conditions. Here you need not adjust for the number
                  of tests carried out, as all the data are supporting the same conclusion. In the same vein, it could be argued that suppression
                  of a mutant phenotype by multiple genes within a single pathway or complex could be exempt from issues of multiple comparisons.
                  Finally, as discussed above, carrying out multiple independent tests may be sufficient to avoid having to apply statistical
                  corrections for multiple comparisons.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec3-9"></a>3.9.&nbsp;A philosophical argument for making no adjustments for multiple comparisons
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Imagine that you have written up a manuscript that contains fifteen figures (fourteen of which are supplemental). Embedded
                  in those figures are 23 independent <span class="emphasis"><em>t</em></span>-tests, none of which would appear to be obvious candidates for multiple comparison adjustments. However, you begin to worry.
                  Since the chosen significance threshold for your tests was 0.05, there is nearly a 70% chance [1 &#8211; (0.95)<sup>23</sup> = 0.693] that at least one of your conclusions is wrong<sup><a name="fn34" href="#ftn.fn34">34</a></sup>. Thinking about this more, you realize that over the course of your career you hope to publish at least 50 papers, each of
                  which could contain an average of 20 statistical tests. This would mean that over the course of your career you are 99.9999999999999999999947%
                  likely to publish at least one error and will undoubtedly publish many (at least those based on statistical tests). To avoid
                  this humiliation, you decide to be proactive and impose a <span class="emphasis"><em>career-wise</em></span> Bonferroni correction to your data analysis. From now on, for results with corresponding statistical tests to be considered
                  valid, they must have a <span class="emphasis"><em>P</em></span>-value of &lt;0.00005 (0.05/1000). Going through your current manuscript, you realize that only four of the 23 tests will meet
                  your new criteria. With great sadness in your heart, you move your manuscript into the trash folder on your desktop.
               </p>
               <p>Although the above narrative may be ridiculous (indeed, it is meant to be so), the underlying issues are very real. Conclusions
                  based on single <span class="emphasis"><em>t</em></span>-tests, which are not supported by additional complementary data, may well be incorrect. Thus, where does one draw the line?
                  One answer is that no line should be drawn, even in situations where multiple comparison adjustments would seem to be warranted.
                  Results can be presented with corresponding <span class="emphasis"><em>P</em></span>-values, and readers can be allowed to make their own judgments regarding their validity. For larger data sets, such as those
                  from microarray studies, an estimation of either the number or proportion of likely false positives can be provided to give
                  readers a feeling for the scope of the problem. Even without this, readers could in theory look at the number of comparisons
                  made, the chosen significance threshold, and the number of positive hits to come up with a general idea about the proportion
                  of false positives. Although many reviewers and readers may not be satisfied with this kind of approach, know that there are
                  professional statisticians who support this strategy. Perhaps most importantly, understand that whatever approaches are used,
                  data sets, particularly large ones, will undoubtedly contain errors, including both false positives and false negatives. Wherever
                  possible, seek to confirm your own results using multiple independent methods so that you are less likely to be fooled by
                  chance occurrence.
               </p>
            </div>
            <div class="footnotes"><br><hr width="100" align="left">
               <div class="footnote">
                  <p><sup><a name="ftn.fn29" href="#fn29">29</a></sup>This discussion assumes that the null hypothesis (of no difference) is true in all cases.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn30" href="#fn30">30</a></sup>Notice that this is the Bonferroni critical value against which all P-values would be compared.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn31" href="#fn31">31</a></sup>If the null hypothesis is true, P-values are random values, uniformly distributed between 0 and 1.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn32" href="#fn32">32</a></sup>The name is a bit unfortunate in that all of statistics is devoted to analyzing variance and ascribing it to random sources
                     or certain modeled effects.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn33" href="#fn33">33</a></sup>These are referred to in the official ANOVA vernacular as treatment groups.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn34" href="#fn34">34</a></sup>This is true supposing that none are in fact real.
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec4"></a>4.&nbsp;Probabilities and Proportions
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-1"></a>4.1.&nbsp;Introduction
                        </h3>
                     </div>
                  </div>
               </div>
               <p><a href="#sec2" title="2.&nbsp;Comparing two means">Sections 2</a> and <a href="#sec3" title="3.&nbsp;Comparisons of more than two means">3</a> dealt exclusively with issues related to means. For many experiments conducted in our field, however, mean values are not
                  the end goal. For example, we may seek to determine the <span class="emphasis"><em>frequency</em></span> of a particular defect in a mutant background, which we typically report as either a <span class="emphasis"><em>proportion</em></span> (e.g., 0.7) or a <span class="emphasis"><em>percentage</em></span> (e.g., 70%). Moreover, we may want to calculate CIs for our sample percentages or may use a formal statistical test to determine
                  if there is likely to be a real difference between the frequencies observed for two or more samples. In other cases, our analyses
                  may be best served by determining <span class="emphasis"><em>ratios</em></span> or <span class="emphasis"><em>fold changes</em></span>, which may require specific statistical tools. Finally, it is often useful, particularly when carrying out genetic experiments,
                  to be able to calculate the <span class="emphasis"><em>probabilities</em></span> of various outcomes. This section will cover major points that are relevant to our field when dealing with these common situations.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-2"></a>4.2.&nbsp;Calculating simple probabilities
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Most readers are likely proficient at calculating the probability of two <span class="emphasis"><em>independent</em></span> events occurring through application of the <span class="emphasis"><em>multiplication rule</em></span>. Namely, If event A occurs 20% of the time and event B occurs 40% of the time, then the probability of event A and B both
                  occurring is 0.2 &times; 0.4 = 0.08 or 8%. More practically, we may wish to estimate the frequency of EcoRI restriction endonuclease
                  sites in the genome. Because the EcoRI binding motif is GAATTC and each nucleotide has a roughly one-in-four chance of occurring
                  at each position, then the chance that any six-nucleotide stretch in the genome will constitute a site for EcoRI is (0.25)<sup>6</sup> = 0.000244140625 or 1 in 4,096. Of course, if all nucleotides are not equally represented or if certain sequences are more
                  or less prevalent within particular genomes, then this will change the true frequency of the site. In fact, GAATTC is over-represented
                  in phage genomes but under-represented in human viral sequences (<a href="#bib7">Burge et al., 1992</a>). Thus, even when calculating straightforward probabilities, one should be careful not to make false assumptions regarding
                  the independence of events.
               </p>
               <p>In carrying out genetic studies, we will often want to determine the likelihood of obtaining a desired genotype. For example,
                  if we are propagating an unbalanced recessive lethal mutation (<span class="emphasis"><em>let</em></span>), we will need to pick phenotypically wild-type animals at each generation and then assess for the presence of the lethal
                  mutation in the first-generation progeny. Basic Mendelian genetics (as applied to <span class="emphasis"><em>C. elegans</em></span> hermaphrodites) states that the progeny of a <span class="emphasis"><em>let/+</em></span> parent will be one-fourth <span class="emphasis"><em>let/let</em></span>, one-half <span class="emphasis"><em>let/+</em></span>, and one-fourth <span class="emphasis"><em>+/+</em></span>. Thus, among the non-lethal progeny of a <span class="emphasis"><em>let/+</em></span> parent, two-thirds will be <span class="emphasis"><em>let/+</em></span> and one-third will be <span class="emphasis"><em>+/+</em></span>. A practical question is how many wild-type animals should we single-clone at each generation to ensure that we pick at least
                  one <span class="emphasis"><em>let/+</em></span> animal? In this case, using the <span class="emphasis"><em>complement</em></span> of the multiplication rule, also referred to as the probability of &#8220;<span class="emphasis"><em>at least one</em></span>&#8221;, will be most germane. We start by asking what is the probability of an individual not being <span class="emphasis"><em>let/+</em></span>, which in this case is one-third or 0.333? Therefore the probability of picking five animals, none of which are of genotype
                  <span class="emphasis"><em>let/+</em></span> is (0.333)<sup>5</sup> or 0.41%, and therefore the probability of picking at least one <span class="emphasis"><em>let/+</em></span> would be 1 &#8722; 0.041 = 99.59%. Thus, picking five wild-type animals will nearly guarantee that at least one of the F1 progeny
                  is of our desired genotype. Furthermore, there is a (0.667)<sup>5</sup> &#8776; 13.20% chance that all five animals will be <span class="emphasis"><em>let/+</em></span>.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-3"></a>4.3.&nbsp;Calculating more-complex probabilities
                        </h3>
                     </div>
                  </div>
               </div>
               <p>To calculate probabilities for more-complex problems, it is often necessary to account for the total number of <span class="emphasis"><em>combinations</em></span> or <span class="emphasis"><em>permutations</em></span> that are possible in a given situation. In this vernacular, the three different arrangements of the letters ABC, ACB, BAC,
                  are considered to be distinct permutations, but only one combination. Thus, for permutations the order matters, whereas for
                  combinations it does not. Depending on the situation, either combinations or permutations may be most relevant. Because of
                  the polarity inherent to DNA polymers, GAT and TAG are truly different sequences and thus permutations would be germane. So
                  far as a standard mass spectroscopy is concerned, however, the peptides DAVDKEN and KENDAVD are identical, and thus combinations
                  might be more relevant in this case.
               </p>
               <p>To illustrate the process of calculating combinations and permutations, we'll first use an example involving peptides. If
                  each of the twenty standard amino acids (aa) is used only once in the construction of a 20-aa peptide, how many distinct sequences
                  can be assembled? We start by noting that the order of the amino acids will matter, and thus we are concerned with permutations.
                  In addition, given the set up where each amino acid can be used only once, we are <span class="emphasis"><em>sampling without replacement</em></span>. The solution can be calculated using the following generic formula: # of permutations = <span class="emphasis"><em>n</em></span>!. Here <span class="emphasis"><em>n</em></span> is the total number of items and &#8220;!&#8221; is the mathematical <span class="emphasis"><em>factorial</em></span> symbol, such that <span class="emphasis"><em>n</em></span>! = <span class="emphasis"><em>n</em></span> &times; (<span class="emphasis"><em>n</em></span> &#8722; 1) &times; (<span class="emphasis"><em>n</em></span> &#8722; 2) &#8230; &times; 1. For example, 5! = 5 &times; 4 &times; 3 &times; 2 &times; 1 = 120. Also by convention, 1! and 0! are both equal to one. To solve this
                  problem we therefore multiply 20 &times; 19 &times; 18 &#8230; 3 &times; 2 &times; 1 or 20! &#8776; 2.4e<sup>18</sup>, an impressively large number. Note that because we were sampling without replacement, the incremental decrease with each
                  multiplier was necessary to reflect the reduced choice of available amino acids at each step. Had we been sampling <span class="emphasis"><em>with</em></span> replacement, where each amino acid can be used any number of times, the equation would simply be 20<sup>20</sup> &#8776; 1.1e<sup>26</sup>, an even more impressive number!
               </p>
               <p>Going back to the previous genetic example, one might wish to determine the probability of picking five progeny from a parent
                  that is <span class="emphasis"><em>let/+</em></span> where three are of genotype <span class="emphasis"><em>let/+</em></span> and two are <span class="emphasis"><em>+/+</em></span>. One thought would be to use the multiplication rule where we multiply 0.667 &times; 0.667 &times; 0.667 &times; 0.333 &times; 0.333, or more compactly,
                  (0.667)<sup>3</sup>(0.333)<sup>2</sup> = 0.0329 or 3.29%. If this seems a bit lower than you might expect, your instincts are correct. The above calculation describes
                  only the probability of obtaining any one particular sequence that produces three <span class="emphasis"><em>let/+</em></span> (L) and two <span class="emphasis"><em>+/+</em></span> (W) worms. For this reason, it underestimates the true frequency of interest, since there are multiple ways of getting the
                  same combination. For example, one possible order would be WWLLL, but equally probable are WLWLL, WLLWL, WLLLW, LLLWW, LLWLW,
                  LLWWL, LWLLW, LWWLL, and LWLWL, giving a total of ten possible permutations. Of course, unlike peptides or strands of DNA,
                  all of the possible orders are equivalent with respect to the relevant outcome, obtaining three <span class="emphasis"><em>let/+</em></span> and two <span class="emphasis"><em>+/+</em></span> worms. Thus, we must take permutations into account in order to determine the frequency of the generic combination. Because
                  deriving permutations by hand (as we did above) becomes cumbersome (if not impossible) very quickly, one can use the following
                  equation where <span class="emphasis"><em>n</em></span> is the total number of items with <span class="emphasis"><em>n<sub>1</sub></em></span> that are alike and <span class="emphasis"><em>n<sub>2</sub></em></span> that are alike, etc., up through <span class="emphasis"><em>n<sub>k</sub></em></span>.
               </p>
               <div class="mediaobject" align="center"><a name="unfig17"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig17.jpg" align="middle" width="200"></td>
                     </tr>
                  </table>
               </div>
               <p>Thus plugging in the numbers for our example, we would have 5!/3! 2! = 120/(6 &times; 2) = 10. Knowing the number of possible permutations
                  we can then multiply this by the probability of getting any single arrangement of three <span class="emphasis"><em>let/+</em></span> and two <span class="emphasis"><em>+/+</em></span> worms calculated above, such that 0.0329 &times; 10 = 0.329 or 32.9%, a number that makes much more sense. This illustrates a more
                  general rule regarding the probability (<span class="emphasis"><em>Pr</em></span>) of obtaining specific combinations:
               </p>
               <p><span class="emphasis"><em>Pr</em></span> combination = (# of permutations) &times; (probability of obtaining any single permutation)
               </p>
               <p>Note, however, that we may often be interested in a slightly different question than the one just posed. For example, what
                  is the probability that we will obtain at least three <span class="emphasis"><em>let/+</em></span> animals with five picks from a <span class="emphasis"><em>let/+</em></span> parent? In this case, we would have to sum the probabilities for three out of five [(5!/3! 2!) (0.0329) = .329], four out
                  of five [(5!/4! 1!) (0.0329) = 0.165], five out of five [(0.667)<sup>5</sup> = 0.132] <span class="emphasis"><em>let/+</em></span> animals, giving us 0.329 + 0.165 + 0.132 = 0.626 or 62.6%.
               </p>
               <p>The ability to calculate permutations can also be used to determine the number of different nucleotide sequences in a 20-mer
                  where each of the four nucleotides (G, A, T, C) is used five times. Namely, 20!/(5!)<sup>4</sup> &#8776; 1.2e<sup>10</sup>. Finally, we can calculate the number of different peptides containing five amino acids where each of the twenty amino acids
                  is chosen once without replacement. In this case, we can use a generic formula where <span class="emphasis"><em>n</em></span> is the total number of items from which we select <span class="emphasis"><em>r</em></span> items without replacement.
               </p>
               <div class="mediaobject" align="center"><a name="unfig18"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig18.jpg" align="middle" width="110"></td>
                     </tr>
                  </table>
               </div>
               <p>This would give us 20!/(20 &#8722; 5)! = 20!/(15)! = 20 &times; 19 &times; 18 &times; 17 &times; 16 = 1,860,480. The same scenario carried out with replacement
                  would simply be (20)<sup>5</sup> = 3,200,000. Thus, <span class="underline">using just a handful of formulas, we are empowered with the ability to make a wide range of predictions for the probabilities
                     that we may encounter</span>. This is important because probabilities are not always intuitive as illustrated by the classic &#8220;birthday problem&#8221;, which
                  demonstrates that within a group of only 23 people, there is a &gt;50% probability that at least two will share the same birthday<sup><a name="fn35" href="#ftn.fn35">35</a></sup>.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-4"></a>4.4.&nbsp;The Poisson distribution
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Certain types of probabilistic events can be modeled using a distribution developed by the French mathematician Sim&eacute;on Denis
                  Poisson. Specifically, the <span class="emphasis"><em>Poisson distribution</em></span> can be used to predict the probability that a given number of <span class="emphasis"><em>events</em></span> will occur over a stipulated <span class="emphasis"><em>interval</em></span> of time, distance, space, or other related measure, when said events occur independently of one another. For example, given
                  a known forward mutation rate caused by a chemical mutagen, what is the chance that three individuals from a collection of
                  1,000 F1s (derived from mutagenized <a href="http://www.wormbase.org/db/get?name=P0;class=Cell" target="_blank">P0</a> parents) will contain a mutation in gene X? Also, what is the chance that any F1 worm would contain two or more independent
                  mutations within gene X?
               </p>
               <p>The generic formula used to calculate such probabilities is shown below, where &micro; is the mean number of expected events, <span class="emphasis"><em>x</em></span> is the number of times that the event occurs over a specified interval, and <span class="emphasis"><em>e</em></span> is the natural log.
               </p>
               <div class="mediaobject" align="center"><a name="unfig19"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig19.jpg" align="middle" width="210"></td>
                     </tr>
                  </table>
               </div>
               <p>For this formula to predict probabilities accurately, it is required that the events be independent of each other and occur
                  at a constant average rate over the given interval. If these criteria are violated, then the Poisson distribution will not
                  provide a valid model. For example, imagine that we want to calculate the likelihood that a mutant worm that is prone to seizures
                  will have two seizures (i.e., events or <span class="emphasis"><em>x</em></span>) within a 5-minute interval. For this calculation, we rely on previous data showing that, on average, mutant worms have 6.2
                  seizures per hour. Thus, the average (&#956;) for a 5-minute interval would be 6.2/12 = 0.517. Plugging these numbers into the
                  above formula we obtain <span class="emphasis"><em>P(x)</em></span> = 0.080 or 8%. Note that if we were to follow 20 different worms for 5 minutes and observed six of them to have two seizures,
                  this would suggest that the Poisson distribution is not a good model for our particular event<sup><a name="fn36" href="#ftn.fn36">36</a></sup>. Rather, the data would suggest that multiple consecutive seizures occur at a frequency that is higher than predicted by
                  the Poisson distribution, and thus the seizure events are not independent. In contrast, had only one or two of the 20 worms
                  exhibited two seizures within the time interval, this would be consistent with a Poisson model.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-5"></a>4.5.&nbsp;Intuitive methods for calculating probabilities
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Another useful strategy for calculating probabilities, as well as other parameters of interest that are governed by chance,
                  is sometimes referred to as the <span class="emphasis"><em>intuitive approach</em></span>. This includes the practice of plugging hypothetical numbers into scenarios to maximize the clarity of the calculations and
                  conclusions. Our example here will involve efforts to maximize the efficiency of an F2-clonal genetic screen to identify recessive
                  maternal-effect lethal or sterile mutations (<a href="#figure11">Figure 11</a>). For this experiment, we will specify that 100 <a href="http://www.wormbase.org/db/get?name=P0;class=Cell" target="_blank">P0</a> adults are to be cloned singly onto plates following mutagenesis. Then ten F1 progeny from each <a href="http://www.wormbase.org/db/get?name=P0;class=Cell" target="_blank">P0</a> will be single-cloned, some small fraction of which will be heterozygous for a desired class of mutation (<span class="emphasis"><em>m/+</em></span>). To identify mutants of interest, however, F2s of genotype <span class="emphasis"><em>m/m</em></span> must be single-cloned, and their F3 progeny must be inspected for the presence of the phenotype. The question is: what is
                  the optimal number of F2s to single-clone from each F1 plate?
               </p>
               <div class="mediaobject" align="center"><a name="figure11"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig11.jpg"><img src="statisticalanalysis_fig11_s.jpg" border="2" align="middle" alt=" figure 11"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 11. Schematic diagram of F2-clonal genetic screen for recessive mutations in <span class="emphasis"><em>C. elegans</em></span>.&nbsp;</b></p>
                  </div>
               </div>
               <p>Mendelian genetics states that the chance of picking an <span class="emphasis"><em>m/m</em></span> F2 from an <span class="emphasis"><em>m/+</em></span> F1 parent is one in four or 25%, so picking more will of course increase the likelihood of obtaining the desired genotype.
                  But will the returns prove diminishing and, if so, what is the most efficient practice? <a href="#table4" title="Table 4. Intuitive approach to determine the maximum efficiency of an F2-clonal genetic screen.">Table 4</a> plugs in real numbers to determine the frequency of obtaining <span class="emphasis"><em>m/m</em></span> animals based on the number of cloned F2s. The first column shows the number of F2 animals picked per F1, which ranges from
                  one to six. In the second column, the likelihood of picking at least one <span class="emphasis"><em>m/m</em></span> animal is determined using the inverse multiplication rule. As expected, the likelihood increases with larger numbers of
                  F2s, but diminishing returns are evident as the number of F2s increases. Columns 3&#8211;5 tabulate the number of worm plates required,
                  the implication being that more plates are both more work and more expense. Columns six and eight calculate the expected number
                  of <span class="emphasis"><em>m/m</em></span> F2s that would be isolated given frequencies of (<span class="emphasis"><em>m/+</em></span>) heterozygotes of 0.01 (10 in 1,000 F1s) or 0.001 (1 in 1,000 F1s), respectively. Here, a higher frequency would infer that
                  the desired mutations of interest are more common. Finally, columns seven and nine show the predicted efficiencies of the
                  screening strategies by dividing the number of isolated <span class="emphasis"><em>m/m</em></span> F2s by the total number of F1 and F2 plates required (e.g., 2.50/2,000 = 1.25e<sup>&#8211;3</sup>).
               </p>
               <div class="table"><a name="table4"></a><p class="title">Table 4. Intuitive approach to determine the maximum efficiency of an F2-clonal genetic screen.</p>
                  <table summary="Table 4. Intuitive approach to determine the maximum efficiency of an F2-clonal genetic screen." border="1">
                     <colgroup>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                        <col>
                     </colgroup>
                     <thead>
                        <tr valign="bottom">
                           <th align="center" valign="bottom"><span class="bold"><strong># of F2s/F1</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Likelihood of cloning at least one <span class="emphasis"><em>m/m</em></span></strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong># F1 plates</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong># F2 plates</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Total # plates</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Expected # <span class="emphasis"><em>m/m</em></span> isolated <span class="emphasis"><em>f</em></span> = 0.01</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Efficiency (#<span class="emphasis"><em>m/m</em></span> per total) <span class="emphasis"><em>f</em></span> = 0.01</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Expected # <span class="emphasis"><em>m/m</em></span> isolated <span class="emphasis"><em>f</em></span> = 0.001</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Efficiency (#<span class="emphasis"><em>m/m</em></span> per total) <span class="emphasis"><em>f</em></span>= 0.001</strong></span></th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr valign="top">
                           <td align="left" valign="top">1</td>
                           <td align="left" valign="top">25.0%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">2000</td>
                           <td align="left" valign="top">2.50</td>
                           <td align="left" valign="top">1.25e<sup>-3</sup></td>
                           <td align="left" valign="top">0.250</td>
                           <td align="left" valign="top">1.25e<sup>-4</sup></td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">2</td>
                           <td align="left" valign="top">43.8%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">2000</td>
                           <td align="left" valign="top">3000</td>
                           <td align="left" valign="top">4.38</td>
                           <td align="left" valign="top">1.46e<sup>-3</sup></td>
                           <td align="left" valign="top">0.438</td>
                           <td align="left" valign="top">1.46e<sup>-4</sup></td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">3</td>
                           <td align="left" valign="top">57.8%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">3000</td>
                           <td align="left" valign="top">4000</td>
                           <td align="left" valign="top">5.78</td>
                           <td align="left" valign="top">1.45e<sup>-3</sup></td>
                           <td align="left" valign="top">0.578</td>
                           <td align="left" valign="top">1.45e<sup>-4</sup></td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">4</td>
                           <td align="left" valign="top">68.4%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">4000</td>
                           <td align="left" valign="top">5000</td>
                           <td align="left" valign="top">6.84</td>
                           <td align="left" valign="top">1.37e<sup>-3</sup></td>
                           <td align="left" valign="top">0.684</td>
                           <td align="left" valign="top">1.37e<sup>-4</sup></td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">5</td>
                           <td align="left" valign="top">76.3%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">5000</td>
                           <td align="left" valign="top">6000</td>
                           <td align="left" valign="top">7.63</td>
                           <td align="left" valign="top">1.27e<sup>-3</sup></td>
                           <td align="left" valign="top">0.763</td>
                           <td align="left" valign="top">1.27e<sup>-4</sup></td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">6</td>
                           <td align="left" valign="top">82.2%</td>
                           <td align="left" valign="top">1000</td>
                           <td align="left" valign="top">6000</td>
                           <td align="left" valign="top">7000</td>
                           <td align="left" valign="top">8.22</td>
                           <td align="left" valign="top">1.17e<sup>-3</sup></td>
                           <td align="left" valign="top">0.822</td>
                           <td align="left" valign="top">1.17e<sup>-4</sup></td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               <p>From this we can see that either two or three F2s is the most efficient use of plates and possibly time, although other factors
                  could potentially factor into the decision of how many F2s to pick. We can also see that the <span class="emphasis"><em>relative efficiencies</em></span> are independent of the frequency of the mutation of interest. Importantly, this potentially useful insight was accomplished
                  using basic intuition and a very rudimentary knowledge of probabilities. Of course, the outlined intuitive approach failed
                  to address whether the optimal number of cloned F2s is 2.4 or 2.5<sup><a name="fn37" href="#ftn.fn37">37</a></sup>, but as we haven't yet developed successful methods to pick or propagate fractions of <span class="emphasis"><em>C. elegans</em></span>, such details are irrelevant!
               </p>
               <p>We note that an online tool has been created by Shai Shaham (<a href="#bib18">Shaham, 2007</a>) that allows users to optimize the efficiency of genetic screens in <span class="emphasis"><em>C. elegans</em></span><sup><a name="fn38" href="#ftn.fn38">38</a></sup>. To use the tool, users enter several parameters that describe the specific genetic approach (e.g., F1 versus F2 clonal).
                  The website's algorithm then provides a recommended F2-to-F1 screening ratio. Entering parameters that match the example used
                  above, the website suggests picking two F2s for each F1, which corresponds to the number we calculated using our intuitive
                  approach. In addition, the website provides a useful tool for calculating the screen size necessary to achieve a desired level
                  of genetic saturation. For example, one can determine the number of cloned F1s required to ensure that all possible genetic
                  loci will be identified at least once during the course of screening with a 95% confidence level.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-6"></a>4.6.&nbsp;Conditional probability: calculating probabilities when events are not independent
                        </h3>
                     </div>
                  </div>
               </div>
               <p>In many situations, the likelihood of two events occurring is not independent. This does not mean that the two events need
                  be totally interdependent or mutually exclusive, just that one event occurring may increase or decrease the likelihood of
                  the other. Put another way, having prior knowledge of one outcome may change the effective probability of a second outcome.
                  Knowing that someone has a well-worn &#8220;1997 <span class="emphasis"><em>C. elegans</em></span> International Meeting&#8221; t-shirt in his drawer does not guarantee that he is an aging nerd, but it certainly does increase
                  the probability! The area of statistics that handles such situations is known as <span class="emphasis"><em>Bayesian analysis</em></span> or inference, after an early pioneer in this area, Thomas Bayes. More generally, <span class="emphasis"><em>conditional</em></span> <span class="emphasis"><em>probability</em></span> refers to the probability of an event occurring based on the condition that another event has occurred. Although conditional
                  probabilities are extremely important in certain types of biomedical and epidemiological research, such as predicting disease
                  states given a set of known factors<sup><a name="fn39" href="#ftn.fn39">39</a></sup>, this issue doesn't arise too often for most <span class="emphasis"><em>C. elegans</em></span> researchers. Bayesian models and networks have, however, been used in the worm field for applications that include phylogenetic
                  gene tree construction (<a href="#bib12">Hoogewijs et al., 2008</a>; <a href="#bib1">Agarwal and States, 1996</a>), modeling developmental processes (<a href="#bib21">Sun and Hong, 2007</a>), and predicting genetic interactions (<a href="#bib23">Zhong and Sternberg, 2006</a>). Bayesian statistics is also used quite extensively in behavioral neuroscience (<a href="#bib15">Knill and Pouget, 2004</a>; <a href="#bib22">Vilares and Kording, 2011</a> ), which is growing area in the <span class="emphasis"><em>C. elegans</em></span> field. We refer interested readers to textbooks or the web for additional information (see <a href="#sec10" title="9.&nbsp;Appendix A: Microsoft Excel tools">Appendix A</a>).
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-7"></a>4.7.&nbsp;Binomial proportions
                        </h3>
                     </div>
                  </div>
               </div>
               <p>It is common in our field to generate data that take the form of <span class="emphasis"><em>binomial proportions</em></span>. Examples would include the percentage of mutant worms that arrest as embryos or that display ectopic expression of a GFP
                  reporter. As the name implies, binomial proportions arise from data that fall into two categories such as heads or tails,
                  on or off, and normal or abnormal. More generically, the two outcomes are often referred to as a <span class="emphasis"><em>success or failure</em></span>. To properly qualify, data forming a binomial distribution must be acquired by <span class="emphasis"><em>random sampling</em></span>, and each outcome must be <span class="emphasis"><em>independent</em></span> of all other outcomes. Coin flips are a classic example where the result of any given flip has no influence on the outcome
                  of any other flip. Also, when using a statistical method known as the <span class="emphasis"><em>normal approximation</em></span> (discussed below), the binomial dataset should contain a minimum of ten outcomes in each category (although some texts may
                  recommend a more relaxed minimum of five). This is generally an issue only when relatively rare events are being measured.
                  For example, flipping a coin 50 times would certainly result in at least ten heads or ten tails, whereas a phenotype with
                  very low penetrance might be detected only in three worms from a sample of 100. In this latter case, a larger sample size
                  would be necessary for the approximation method to be valid. Lastly, sample sizes should not be &gt;10% of the entire population.
                  As we often deal with theoretical populations that are effectively infinite in size, however, this stipulation is generally
                  irrelevant.
               </p>
               <p>An aside on the role of normality in binomial proportions is also pertinent here. It might seem counterintuitive, but the
                  distribution of sample proportions arising from data that are binary does have, with sufficient sample size, an approximately
                  normal distribution. This concept is illustrated in <a href="#figure12">Figure 12</a>, which computationally simulates data drawn from a population with an underlying &#8220;success&#8221; rate of 0.25. As can be seen,
                  the distribution becomes more normal with increasing sample size. How large a sample is required, you ask? The short answer
                  is that the closer the underlying rate is to 50%, the smaller the required sample size is; with a more extreme rate (i.e.,
                  closer to 0% or 100%), a larger size is required. The requirements are reasonably met by the aforementioned <span class="emphasis"><em>minimum of ten</em></span> rule.
               </p>
               <div class="mediaobject" align="center"><a name="figure12"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig12.jpg"><img src="statisticalanalysis_fig12_s.jpg" border="2" align="middle" alt=" figure 12"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 12. Illustration of the Central Limit Theorem for binomial proportions.&nbsp;</b>Panels A&#8211;D show results from a computational sampling experiment where the proportion of successes in the population is 0.25.
                        The <span class="emphasis"><em>x</em></span> axes indicate the proportions obtained from samples sizes of 10, 20, 40, and 80. The <span class="emphasis"><em>y</em></span> axes indicate the number of computational samples obtained for a given proportion. As expected, larger-sized samples give
                        distributions that are closer to normal in shape and have a narrower range of values.
                     </p>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-8"></a>4.8.&nbsp;Calculating confidence intervals for binomial proportions
                        </h3>
                     </div>
                  </div>
               </div>
               <p>To address the accuracy of proportions obtained through random sampling, we will typically want to provide an accompanying
                  CI. For example, political polls will often report the percentage in favor of a candidate along with a 95% CI, which may encompass
                  several percentage points to either side of the midpoint estimate<sup><a name="fn40" href="#ftn.fn40">40</a></sup>. As previously discussed in the context of means, determining CIs for sample proportions is important because in most cases
                  we can never know the true proportion of the population under study. Although different confidence levels can be used, binomial
                  data are often accompanied by 95% CIs. As for means, lower CIs (e.g., 90%) are associated with narrower intervals, whereas
                  higher CIs (e.g., 99%) are associated with wider intervals. Once again, the meaning of a 95% CI is the same as that discussed
                  earlier in the context of means. If one were to repeat the experiment 100 times and calculate 95% CIs for each repeat, on
                  average 95 of the calculated CIs would contain the true population proportion. Thus, there is a 95% chance that the CI calculated
                  for any given experiment contains the true population proportion.
               </p>
               <p>Perhaps surprisingly, there is no perfect consensus among statisticians as to which of several methods is best for calculating
                  CIs for binomial proportions<sup><a name="fn41" href="#ftn.fn41">41</a></sup>. Thus, different textbooks or websites may describe several different approaches. That said, for most purposes we can recommend
                  a test that goes by several names including the adjusted Wald, the modified Wald, and the Agresti-Coull (A-C) method (<a href="#bib2">Agresti and Coull, 1998</a>; <a href="#bib3">Agresti and Caffo, 2000</a>). Reasons for recommending this test are: (1) it is widely accepted, (2) it is easy to implement (if the chosen confidence
                  level is 95%), and (3) it gives more-accurate CIs than other straightforward methods commonly in use. Furthermore, even though
                  this approach is based on the normal approximation method, the <span class="emphasis"><em>minimum of ten</em></span> rule can be relaxed.
               </p>
               <p>To implement the A-C method for a 95% CI (the most common choice), add two to both the number of successes and failures. Hence
                  this is sometimes referred to as the <span class="emphasis"><em>plus-four</em></span> or <span class="emphasis"><em>&#8220;+4&#8221; method</em></span>. It then uses the doctored numbers, together with the normal approximation method, to determine the CI for the population
                  proportion. Admittedly this sounds weird, if not outright suspicious, but empirical studies have shown that this method gives
                  consistently better 95% CIs than would be created using the simpler method. For example, if in real life you assayed 83 animals
                  and observed larval arrest in 22, you would change the total number of trials to 87 and the number of arrested larvae to 24.
                  In addition, depending on the software or websites used, you may need to choose the normal approximation method and not something
                  called the <span class="emphasis"><em>exact method</em></span> for this to work as intended.
               </p>
               <p>Importantly, the proportion and sample size that you report should be the actual proportion and sample size from what you
                  observed; the doctored (i.e., plus-four) numbers are used exclusively to generate the 95% CI. Thus, in the case of the above
                  example, you would report the proportion as 22/83 = 0.265 or 26.5%. The 95% CI would, however, be calculated using the numbers
                  24 and 87 to give a 95% CI of 18.2%&#8211;37.0%. Note that the +4 version of the A-C method is specific for 95% CIs and not for
                  other intervals<sup><a name="fn42" href="#ftn.fn42">42</a></sup>. Finally, it is worth noting that CIs for proportions that are close to either 0% or 100% will get a bit funky. This is because
                  the CI cannot include numbers &lt;0% or &gt;100%. Thus, CIs for proportions close to 0% or 100% will often be quite lopsided around
                  the midpoint and may not be particularly accurate. Nevertheless, unless the obtained percentage is 0 or 100, we do not recommend
                  doing anything about this as measures used to compensate for this phenomenon have their own inherent set of issues. In other
                  words, if the percentage ranges from 1% to 99%, use the A-C method for calculation of the CI. In cases where the percentage
                  is 0% or 100%, instead use the exact method.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-9"></a>4.9.&nbsp;Tests for differences between two binomial proportions
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Very often we will want to compare two proportions for differences. For example, we may observe 85% larval arrest in mutants
                  grown on control RNAi plates and 67% arrest in mutants on RNAi-feeding plates targeting gene <span class="emphasis"><em>X</em></span>. Is this difference significant from a statistical standpoint? To answer this, two distinct tests are commonly used. These
                  are generically known as the <span class="emphasis"><em>normal approximation</em></span> and <span class="emphasis"><em>exact</em></span> methods. In fact, many website calculators or software programs will provide the <span class="emphasis"><em>P</em></span>-value calculated by each method as a matter of course, although in some cases you may need to select one method. The approximation
                  method (based on the so-called normal distribution) has been in general use much longer, and the theory behind this method
                  is often outlined in some detail in statistical texts. The major reason for the historical popularity of the approximation
                  method is that prior to the advent of powerful desktop computers, calculations using the exact method simply weren't feasible.
                  Its continued use is partly due to convention, but also because the approximation and exact methods typically give very similar
                  results. Unlike the normal approximation method, however, the exact method is valid in all situations, such as when the number
                  of successes is less than five or ten, and can thus be recommended over the approximation method.
               </p>
               <p>Regardless of the method used, the <span class="emphasis"><em>P</em></span>-value derived from a test for differences between proportions will answer the following question: What is the probability
                  that the two experimental samples were derived from the same population? Put another way, the null hypothesis would state
                  that both samples are derived from a single population and that any differences between the sample proportions are due to
                  chance sampling. Much like statistical tests for differences between means, proportions tests can be one- or two-tailed, depending
                  on the nature of the question. For the purpose of most experiments in basic research, however, two-tailed tests are more conservative
                  and tend to be the norm. In addition, analogous to tests with means, one can compare an experimentally derived proportion
                  against a historically accepted standard, although this is rarely done in our field and comes with the possible caveats discussed
                  in <a href="#sec2-3" title="2.3.&nbsp;One- versus two-sample tests">Section 2.3</a>. Finally, some software programs will report a 95% CI for the difference between two proportions. In cases where no statistically
                  significant difference is present, the 95% CI for the difference will always include zero.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-10"></a>4.10.&nbsp;Tests for differences between more than one binomial proportion
                        </h3>
                     </div>
                  </div>
               </div>
               <p>A question that may arise when comparing more than two binomial proportions is whether or not multiple comparisons should
                  be factored into the statistical analysis. The issues here are very similar to those discussed in the context of comparing
                  multiple means (<a href="#sec3" title="3.&nbsp;Comparisons of more than two means">Section 3</a>). In the case of proportions, rather than carrying out an ANOVA, a <span class="emphasis"><em>Chi-square</em></span> test (discussed below) could be used to determine if any of the proportions are significantly different from each other.
                  Like an ANOVA, however, this may be a somewhat less-than-satisfying test in that a positive finding would not indicate which
                  particular proportions are significantly different. In addition, FDR and Bonferroni-type corrections could also be applied
                  at the level of <span class="emphasis"><em>P</em></span>-value cutoffs, although these may prove to be too conservative and could reduce the ability to detect real differences (i.e.,
                  the <span class="emphasis"><em>power</em></span> of the experiment).
               </p>
               <p>In general, we can recommend that for findings confirmed by several independent repeats, corrections for multiple comparisons
                  may not be necessary. We illustrate our rationale with the following example. Suppose you were to carry out a genome-wide
                  RNAi screen to identify suppressors of larval arrest in the mutant <span class="emphasis"><em>Y</em></span> background. A preliminary screen might identify &#8764;1,000 such clones ranging from very strong to very marginal suppressors.
                  With retesting of these 1,000 clones, most of the false positives from the first round will fail to suppress in the second
                  round and will be thrown out. A third round of retesting will then likely eliminate all but a few false positives, leaving
                  mostly valid ones on the list. This effect can be quantified by imagining that we carry out an exact binomial test on each
                  of &#8764;20,000 clones in the RNAi library, together with an appropriate negative control, and chose an &#945; level (i.e., the statistical
                  cutoff) of 0.05. By chance alone, 5% or 1,000 out of 20,000 would fall below the <span class="emphasis"><em>P</em></span>-value threshold. In addition, let's imagine that 100 real positives would also be identified giving us 1,100 positives in
                  total. Admittedly, at this point, the large majority of identified clones would be characterized as false positives. In the
                  second round of tests, however, the large majority of true positives would again be expected to exhibit statistically significant
                  suppression, whereas only 50 of the 1,000 false positives will do so. Following the third round of testing, all but two or
                  three of the false positives will have been eliminated. These, together with the &#8764;100 true positives, most of which will have
                  passed all three tests, will leave a list of genes that is strongly enriched for true positives. Thus, by carrying out several
                  experimental repeats, additional correction methods are not needed.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-11"></a>4.11.&nbsp;Probability calculations for binomial proportions
                        </h3>
                     </div>
                  </div>
               </div>
               <p>At times one may be interested in calculating the probability of obtaining a particular proportion or one that is more extreme,
                  given an expected frequency. For example, what are the chances of tossing a coin 100 times and getting heads 55 times? This
                  can be calculated using a small variation on the formulae already presented above.
               </p>
               <div class="mediaobject" align="center"><a name="unfig20"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="90">
                     <tr>
                        <td align="center"><img src="statisticalanalysis_unfig20.jpg" align="middle" width="450"></td>
                     </tr>
                  </table>
               </div>
               <p>Here <span class="emphasis"><em>n</em></span> is the number of trials, Y is the number of positive outcomes or successes, and <span class="emphasis"><em>p</em></span> is the probability of a success occurring in each trial. Thus we can determine that the chance of getting exactly 55 heads
                  is quite small, only 4.85%. Nevertheless, given an expected proportion of 0.5, we intuitively understand that 55 out of 100
                  heads is not an unusual result. In fact, we are probably most interested in knowing the probability of getting a result at
                  least as or more extreme than 55 (whether that be 55 heads or 55 tails). Thus our probability calculations must also include
                  the results where we get 56, 57, 58&#8230;100 heads as well as 45, 44, 43 &#8230;0 heads. Adding up these probabilities then tells us
                  that we have a 36.8% chance of obtaining at least 55 heads or tails in 100 tosses, which is certainly not unusual. Rather
                  than having to calculate each probability and adding them up, however, a number of websites will do this for you. Nevertheless,
                  be alert and understand the principles behind what you are doing, as some websites may only give you the probability of &#8805;55%,
                  whereas what you really may need is the summed probability of both &#8805;55% and &#8804;45%.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-12"></a>4.12.&nbsp;Probability calculations when sample sizes are large relative to the population size
                        </h3>
                     </div>
                  </div>
               </div>
               <p>One of the assumptions for using the binomial distribution is that our population size must be very large relative to our
                  sample size<sup><a name="fn43" href="#ftn.fn43">43</a></sup>. Thus, the act of sampling itself should not appreciably alter the course of future outcomes (i.e., the probability is fixed
                  and does not change each trial). For example, if we had a (very large) jar with a million marbles, half of them black and
                  half of them white, removing one black marble would not grossly alter the probability of the next marble being black or white.
                  We can therefore treat these types of situations as though the populations were infinite or as though we were <span class="emphasis"><em>sampling with replacement</em></span>. In contrast, with only ten marbles (five white and five black), picking a black marble first would reduce the probability
                  of the next marble being black from 50% (5/10) to 44.4% (4/9), while increasing the probability of the next marble being white
                  to 55.6% (5/9)<sup><a name="fn44" href="#ftn.fn44">44</a></sup>. For situations like this, in which the act of sampling noticeably affects the remaining population, the binomial is shelved
                  in favor of something called the <span class="emphasis"><em>hyper-geometric distribution</em></span>. For example, in the case of the ten marbles, the probability of picking out five marbles, all of which are black, is 0.0040
                  using the hypergeometric distribution. In contrast, the binomial applied to this situation gives an erroneously high value
                  of 0.031.
               </p>
               <p>For our field, we often see hyper-geometric calculations applied to computational or genomics types of analyses. For example,
                  imagine that we have carried out an RNA-seq experiment and have identified 1,000 genes that are mis-expressed in a particular
                  mutant background. A gene ontology (GO) search reveals that 13 of these genes encode proteins with a RING domain. Given that
                  there are 152 annotated RING domain&#8211;containing proteins in <span class="emphasis"><em>C. elegans</em></span>, what is the probability that at least 13 would arise by chance in a dataset of this size? The rationale for applying a hyper-geometric
                  distribution to this problem would be as follows. If one were to randomly pick one of &#8764;20,000 worm proteins out of a hat,
                  the probability of it containing a RING domain would be 152/20,000 (0.00760). This leaves 151 remaining RING proteins that
                  could be picked in future turns. The chance that the next protein would contain a RING domain is then 151/19,999 (0.00755).
                  By the time we have come to the thirteenth RING protein in our sample of 1,000 differentially expressed genes, our chances
                  might be something like 140/19,001 (0.00737). Plugging the required numbers into both binomial and hyper-geometric calculators<sup><a name="fn45" href="#ftn.fn45">45</a></sup> (available on the web), we get probabilities of 0.0458 and 0.0415, respectively. Admittedly, in this case the hyper-geometric
                  method gives us only a slightly smaller probability than the binomial. Here the difference isn't dramatic because the population
                  size (20,000) is not particularly small.
               </p>
               <p>We would also underscore the importance of being conservative in our interpretations of GO enrichment studies. In the above
                  example with RING finger proteins, had the representation in our dataset of 1,000 genes been 12 instead of 13, the probability
                  would have been greater than 0.05 (0.0791 and 0.0844 by hyper-geometric and binomial methods, respectively). Furthermore,
                  we need to consider that there are currently several thousand distinct GO terms that are currently used to classify <span class="emphasis"><em>C. elegans</em></span> genes. Thus, the random chance of observing over-representation within any one particular GO class will be much less than
                  observing over-representation within some&#8212;but no particular&#8212;GO class. Going back to marbles, if we had an urn with 100 marbles
                  of ten different colors (10 marbles each), the chance that a random handful of 5 marbles would contain at least 3 of one particular
                  color is only 0.00664. However, the chance of getting at least three out of five the same color is 0.0664. Thus, for GO over-representation
                  to be meaningful, we should look for very low <span class="emphasis"><em>P</em></span>-values (e.g., &#8804;1e<sup>&#8211;5</sup>).
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec4-13"></a>4.13.&nbsp;Tests for differences between multinomial proportions
                        </h3>
                     </div>
                  </div>
               </div>
               <p><span class="emphasis"><em>Multinomial</em></span> proportions or distributions refer to data sets where outcomes are divided into three or more discrete categories. A common
                  textbook example involves the analysis of genetic crosses where either genotypic or phenotypic results are compared to what
                  would be expected based on Mendel's laws. The standard prescribed statistical procedure in these situations is the <span class="emphasis"><em>Chi-square</em></span> <span class="emphasis"><em>goodness-of-fit</em></span> test, an approximation method that is analogous to the normal approximation test for binomials. The basic requirements for
                  multinomial tests are similar to those described for binomial tests. Namely, the data must be acquired through random sampling
                  and the outcome of any given trial must be independent of the outcome of other trials. In addition, a minimum of five outcomes
                  is required for each category for the Chi-square goodness-of-fit test to be valid. To run the Chi-square goodness-of-fit test,
                  one can use standard software programs or websites. These will require that you enter the number of expected or control outcomes
                  for each category along with the number of experimental outcomes in each category. This procedure tests the null hypothesis
                  that the experimental data were derived from the same population as the control or theoretical population and that any differences
                  in the proportion of data within individual categories are due to chance sampling.
               </p>
               <p>As is the case with binomials, exact tests can also be carried out for multinomial proportions. Such tests tend to be more
                  accurate than approximation methods, particularly if the requirement of at least five outcomes in each category cannot be
                  met. Because of the more-complicated calculations involved, however, exact multinomial tests are less commonly used than the
                  exact binomial test, and web versions are currently difficult to come by. The good news is that, like the binomial tests,
                  the approximate and exact methods for multinomials will largely yield identical results. Also make sure that you don't confuse
                  the Chi-square goodness-of-fit test with the <span class="emphasis"><em>Chi-square test of independence</em></span>, which also has an exact version termed the <span class="emphasis"><em>Fisher's exact</em></span> test.
               </p>
               <p>Although many of us will probably not require the Chi-square goodness-of-fit test to sort out if our proportion of yellow
                  wrinkled peas is what we might have expected, understand that this test can be used for any kind of sample data where more
                  than two categories are represented. Imagine that we are studying a gene in which mutations lead to pleiotropy, meaning that
                  a spectrum of distinct phenotypes is observed. Here, the proportion of animals displaying each phenotype could be compared
                  across different alleles of the same gene to determine if specific mutations affect certain developmental processes more than
                  others. In other instances, numerical data, such as the number of mRNA molecules in an embryo, may also benefit from imposing
                  some broader categorization. For example, embryos might be divided into those containing 0&#8211;10, 11&#8211;50, 51&#8211;200, and &gt;200 transcripts.
                  These outcomes could then be compared across different mutant backgrounds or at different developmental time points to identify
                  broad categorical differences in expression. In all of the above cases, a Chi-square goodness-of-fit test would be appropriate
                  to determine if any differences in the observed proportions are statistically significant.
               </p>
            </div>
            <div class="footnotes"><br><hr width="100" align="left">
               <div class="footnote">
                  <p><sup><a name="ftn.fn35" href="#fn35">35</a></sup>Proofs showing this abound on the internet.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn36" href="#fn36">36</a></sup>Although this may seem intuitive, we can calculate this using some of the formulae discussed above. Namely, this boils down
                     to a combinations problem described by the formula: Pr(combination) = (# permutations) (probability of any single permutation).
                     For six events in 20, the number of permutations = 20!/6! 14! = 38,760. The probability of any single permutation = (0.08)6
                     (0.92)14 = 8.16e-8. Multiplying these together we obtain a value of 0.00316. Thus, there is about a 0.3% chance of observing
                     six events if the events are indeed random and independent of each other. Of course, what we really want to know is the chance
                     of observing at least six events, so we also need to include (by simple addition) the probabilities of observing 7, 8, &#8230;20
                     events. For seven events, the probability is only 0.000549, and this number continues to decrease precipitously with increasing
                     numbers of events. Thus, the chance of observing at least six events is still &lt;0.4%, and thus we would suspect that the Poisson
                     distribution does not accurately model our event.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn37" href="#fn37">37</a></sup>Given that <a href="#table4" title="Table 4. Intuitive approach to determine the maximum efficiency of an F2-clonal genetic screen.">Table 4</a> indicates that the optimal number of F2s is between 2 and 3.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn38" href="#fn38">38</a></sup><a href="http://shahamlab.rockefeller.edu/cgi-bin/Genetic_screens/screenfrontpage.cgi" target="_top">http://shahamlab.rockefeller.edu/cgi-bin/Genetic_screens/screenfrontpage.cgi</a></p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn39" href="#fn39">39</a></sup>Calculating the probability that a medical patient has a particular (often rare) disease given a positive diagnostic test
                     result is a classic example used to illustrate the utility of Baye's Theorem. Two complimentary examples on the web can be
                     found at: <a href="http://vassarstats.net/bayes.html" target="_top">http://vassarstats.net/bayes.html</a> and <a href="http://www.tc3.edu/instruct/sbrown/stat/falsepos.htm" target="_top">http://www.tc3.edu/instruct/sbrown/stat/falsepos.htm</a>.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn40" href="#fn40">40</a></sup>This will often be stated in terms of a margin of error rather than the scientific formalism of a confidence interval.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn41" href="#fn41">41</a></sup>The reasons for this are complex and due in large part to the demonstrated odd behavior of proportions (See <a href="#bib2">Agresti and Coull 1998</a>, <a href="#bib3">Agresti and Caffo 2000</a>; and <a href="#bib6">Brown et al., 2001</a>).
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn42" href="#fn42">42</a></sup>A more precise description of the A-C method is to add the square of the appropriate z-value to the denominator and half of
                     the square of the z-value to the numerator. Conveniently, for the 95% CI, the z-value is 1.96 and thus we add 1.962 = 3.84
                     (rounded to 4) to the denominator and 3.84/2 = 1.92 (rounded to 2) to the numerator. For a 99% A-C CI, we would add 6.6 (2.5752)
                     to the denominator and 3.3 (6.6/2) to the numerator. Note that many programs will not accept anything other than integers
                     (whole numbers) for the number of successes and failures and so rounding is necessary.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn43" href="#fn43">43</a></sup>Other assumptions for the binomial include random sampling, independence of trials, and a total of two possible outcomes.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn44" href="#fn44">44</a></sup>Card counters in Las Vegas use this premise to predict the probability of future outcomes to inform their betting strategies,
                     which makes them unpopular with casino owners.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn45" href="#fn45">45</a></sup>Note that the numbers you will need to enter for each method are slightly different. The binomial calculators will require
                     you to enter the probability of a success (0.00760), the number of trials (1,000), and the number of successes (13). The hyper-geometric
                     calculator will require you to enter the population size (20,000), the number of successes in the population (152), the sample
                     size (1,000), and the number of success in the sample (13). Also note that because of the computational intensity of the hyper-geometric
                     approach, many websites will not accommodate a population size of &gt;1,000. One website that will handle larger populations
                     (<a href="http://keisan.casio.com/has10/SpecExec.cgi?id=system/2006/1180573202" target="_top">http://keisan.casio.com/has10/SpecExec.cgi?id=system/2006/1180573202</a>) may use an approximation method.
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec5"></a>5.&nbsp;Relative differences, ratios, and correlations
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec5-1"></a>5.1.&nbsp;Comparing relative versus incremental differences
                        </h3>
                     </div>
                  </div>
               </div>
               <p>It is common in biology for relative changes to be more germane than incremental ones. There are two principal reasons for
                  this. One is that certain biological phenomena can only be properly described and understood through relative changes. For
                  example, if we were to count the number of bacterial cells in a specified volume of liquid culture every hour, we might derive
                  the following numbers: 1,000, 2,000, 4,000, 8,000, 16,000. The pattern is clear; the cells are doubling every hour. Conversely,
                  it would be ridiculous to take the mean of the observed changes in cell number and to state that, on average, the cells increase
                  by 3,750 each hour with a 95% CI of &#8722;1,174.35 to 8,674.35! The second reason is due to experimental design. There are many
                  instances where variability between experiments or specimens makes it difficult, if not impossible, to pool mean values from
                  independent repeats in a productive way. Rather, the ratio of experimental and control values within individual experiments
                  or specimens should be our focus. One example of this involves quantifying bands on a western blot and is addressed below.
                  Most traditional statistical approaches, however, are oriented toward the analysis of incremental changes (i.e., where change
                  is measured by subtraction). Thus, it may not always be clear how to analyze data when the important effects are relative.
               </p>
               <p>As an example of a situation in which ratios are likely to be most useful, we consider the analysis of a western blot (<a href="#figure13">Figure 13</a>) (also see <a href="#bib10">Gassmann et al., 2009</a>). This schematic blot shows the outcome of an experiment designed to test the hypothesis that loss of gene <span class="emphasis"><em>y</em></span> activity leads to changes in the expression of protein X in <span class="emphasis"><em>C. elegans</em></span>. In one scenario, the three blots (A&#8211;C) could represent independent biological repeats with lanes 1&#8211;3 serving as technical
                  (e.g., loading) repeats. In another scenario, the three blots could serve as technical repeats with lanes 1&#8211;3 representing
                  independent biological repeats. Regardless, either scenario will give essentially the same result. Quantification of each
                  band, based on pixel intensity, is indicated in blue<sup><a name="fn46" href="#ftn.fn46">46</a></sup>. Based on <a href="#figure13">Figure 13</a>, it seems clear that loss of gene <span class="emphasis"><em>y</em></span> leads to an increase in the amount of protein X. So does the statistical analysis agree?
               </p>
               <div class="mediaobject" align="center"><a name="figure13"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig13.jpg"><img src="statisticalanalysis_fig13_s.jpg" border="2" align="middle" alt=" figure 13"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 13. Representative western blot analysis.&nbsp;</b></p>
                  </div>
               </div>
               <p><a href="#figure14">Figure 14</a> shows the results of carrying out the statistical analysis in several different ways. This includes, for illustrative purposes,
                  seven distinct two-tailed <span class="emphasis"><em>t</em></span>-tests (1&#8211;7). In <span class="emphasis"><em>t</em></span>-tests 1&#8211;3, data from wild-type and <span class="emphasis"><em>mut y</em></span> bands were pooled within individual blots to obtain an average. Interestingly, only one of the three, blot A, showed a statistically
                  significant difference (<span class="emphasis"><em>P</em></span> &#8804; 0.05) between wild type and <span class="emphasis"><em>mut y</em></span>, despite all three blots appearing to give the same general result. In this case, the failure of blots B and C to show a
                  significant difference is due to slightly more variability between samples of the same kind (i.e., wild type or <span class="emphasis"><em>mut</em></span> <span class="emphasis"><em>y</em></span>) and because with an <span class="emphasis"><em>n</em></span> = 3, the power of the <span class="emphasis"><em>t</em></span>-test to detect small or even moderate differences is weak. The situation is even worse when we combine subsets of bands from
                  different blots, such as pooled lanes 1, 2, and 3 (<span class="emphasis"><em>t</em></span>-tests 4&#8211;6). Pooling all of the wild-type and <span class="emphasis"><em>mut y</em></span> data (<span class="emphasis"><em>t</em></span>-test 7) does, however, lead to a significant difference (<span class="emphasis"><em>P</em></span> = 0.0065).
               </p>
               <div class="mediaobject" align="center"><a name="figure14"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig14.jpg"><img src="statisticalanalysis_fig14_s.jpg" border="2" align="middle" alt=" figure 14"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 14. Statistical analysis of western blot data from Figure 13.&nbsp;</b>A summary of test options is shown.
                     </p>
                  </div>
               </div>
               <p>So was the <span class="emphasis"><em>t</em></span>-test the right way to go? Admittedly, it probably wasn't very satisfying that only one of the first three <span class="emphasis"><em>t</em></span>-tests indicated a significant difference, despite the raw data looking similar for all three. In addition, we need to consider
                  whether or not pooling data from different blots was even kosher. On the one hand, the intensity of the protein X band in
                  any given lane is influenced by the concentration of protein X in the lysate, which is something that we care about. On the
                  other hand, the observed band intensity is also a byproduct of the volume of lysate loaded, the efficiency of protein transfer
                  to the membrane, the activity of the radiolabel or enzymes used to facilitate visualization, and the length of the exposure
                  time, none of which are relevant to our central question! In fact, in the case of western blots, comparing intensities across
                  different blots is really an apples and oranges proposition, and thus pooling such data violates basic principles of logic<sup><a name="fn47" href="#ftn.fn47">47</a></sup>. Thus, even though pooling all the data (<span class="emphasis"><em>t</em></span>-test 7) gave us a sufficiently low <span class="emphasis"><em>P</em></span>-value as to satisfy us that there is a statistically significant difference in the numbers we entered, the premise for combining
                  such data was flawed scientifically.
               </p>
               <p>The last test shown in <a href="#figure14">Figure 14</a> is the output from confidence interval calculations for two ratios. This test was carried out using an Excel tool that is
                  included in this chapter<sup><a name="fn48" href="#ftn.fn48">48</a></sup>. To use this tool, we must enter for each paired experiment the mean (termed &#8220;estimate&#8221;) and the SE (&#8220;SE of est&#8221;) and must
                  also choose a confidence level (<a href="#figure15">Figure 15</a>). Looking at the results of the statistical analysis of ratios (<a href="#figure14">Figure 14</a>), we generally observe much crisper results than were provided by the <span class="emphasis"><em>t</em></span>-tests. For example, in the three cases where comparisons were made only within individual blots, all three showed significant
                  differences corresponding to <span class="emphasis"><em>P</em></span> &lt; 0.05 and two (blots A and B) were significant to <span class="emphasis"><em>P</em></span> &lt; 0.01<sup><a name="fn49" href="#ftn.fn49">49</a></sup>. In contrast, as would be expected, combining lane data between different blots to obtain ratios did not yield significant
                  results, even though the ratios were of a similar magnitude to the blot-specific data. Furthermore, although combining all
                  values to obtain means for the ratios did give <span class="emphasis"><em>P</em></span> &lt; 0.05, it was not significant at the &#945; level of 0.01. In any case, we can conclude that this statistical method for acquiring
                  confidence intervals for ratios is a clean and powerful way to handle this kind of analysis.
               </p>
               <div class="mediaobject" align="center"><a name="figure15"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig15.jpg"><img src="statisticalanalysis_fig15_s.jpg" border="2" align="middle" alt=" figure 15"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 15. Confidence interval calculator for a ratio.&nbsp;</b></p>
                  </div>
               </div>
               <p>It is also worth pointing out that there is another way in which the <span class="emphasis"><em>t</em></span>-test could be used for this analysis. Namely, we could take the ratios from the first three blots (3.33, 3.41, and 2.48),
                  which average to 3.07, and carry out a one-sample two-tailed <span class="emphasis"><em>t</em></span>-test. Because the null hypothesis is that there is no difference in the expression of protein X between wild-type and <span class="emphasis"><em>mut y</em></span> backgrounds, we would use an expected ratio of 1 for the test. Thus, the <span class="emphasis"><em>P</em></span>-value will tell us the probability of obtaining a ratio of 3.07 if the expected ratio is really one. Using the above data
                  points, we do in fact obtain <span class="emphasis"><em>P</em></span> = 0.02, which would pass our significance cutoff. In fact, this is a perfectly reasonable use of the <span class="emphasis"><em>t</em></span>-test, even though the test is now being carried out on ratios rather than the unprocessed data. Note, however, that changing
                  the numbers only slightly to 3.33, 4.51, and 2.48, we would get a mean of 3.44 but with a corresponding <span class="emphasis"><em>P</em></span>-value of 0.054. This again points out the problem with <span class="emphasis"><em>t</em></span>-tests when one has very small sample sizes and moderate variation within samples.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec5-2"></a>5.2.&nbsp;Ratio of means versus mean of ratios
                        </h3>
                     </div>
                  </div>
               </div>
               <p>There is also an important point to be made with respect to ratios that concerns the mean value that you would report. Based
                  on the above paragraph, the mean of the three ratios is 3.07. However, looking at <span class="emphasis"><em>t</em></span>-test 7, which used the pooled data, we can see that the ratio calculated from the total means would be 477/167 = 2.86. This
                  points out a rather confounding property of ratio arithmetic. Namely, that the mean of the ratios (MoR; in this case 3.07)
                  is usually not equal to the ratio of the means (RoM; 2.86). Which of the two you choose to use to report will depend on the
                  question you are trying to answer.
               </p>
               <p>To use a non-scientific (but intuitive) example, we can consider changes in housing prices over time. In a given town, the
                  current appraised value of each house is compared to its value 20 years prior. Some houses may have doubled in value, whereas
                  others may have quadrupled (<a href="#table5" title="Table 5. A tinker-toy illustration for increases in house prices in TinyTown (which has only two households).">Table 5</a>). Taking an average of the ratios for individual houses (i.e., the relative increase)&#8212;the MoR approach&#8212;allows us to determine
                  that the mean increase in value has been 3-fold. However, it turns out that cheaper houses (those initially costing &#8804;$100,000)
                  have actually gone up about 4-fold on average, whereas more-expensive homes (those initially valued at &#8805;$300,000) have generally
                  only doubled. Thus, the total increase in the combined value of all the homes in the neighborhood has not tripled but is perhaps
                  2.5-fold higher than it was 20 years ago (the RoM approach).
               </p>
               <div class="table"><a name="table5"></a><p class="title">Table 5. A tinker-toy illustration for increases in house prices in TinyTown (which has only two households).</p>
                  <table summary="Table 5. A tinker-toy illustration for increases in house prices in TinyTown (which has only two households)." border="1">
                     <colgroup>
                        <col>
                        <col>
                        <col>
                        <col>
                     </colgroup>
                     <thead>
                        <tr valign="bottom">
                           <th valign="bottom">&nbsp;</th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Before</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>After</strong></span></th>
                           <th align="center" valign="bottom"><span class="bold"><strong>Relative Increase</strong></span></th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr valign="top">
                           <td valign="top">&nbsp;</td>
                           <td align="left" valign="top">$100,000</td>
                           <td align="left" valign="top">$400,000</td>
                           <td align="left" valign="top">4</td>
                        </tr>
                        <tr valign="top">
                           <td valign="top">&nbsp;</td>
                           <td align="left" valign="top">$300,000</td>
                           <td align="left" valign="top">$600,000</td>
                           <td align="left" valign="top">2</td>
                        </tr>
                        <tr valign="top">
                           <td align="left" valign="top">Means</td>
                           <td align="left" valign="top">$200,000</td>
                           <td align="left" valign="top">$500,000</td>
                           <td align="left" valign="top">MoR&#8595;</td>
                        </tr>
                        <tr valign="top">
                           <td valign="top">&nbsp;</td>
                           <td align="left" valign="top">RoM&#8594;</td>
                           <td align="left" valign="top"><span class="bold"><strong>2.5</strong></span></td>
                           <td align="left" valign="top"><span class="bold"><strong>3</strong></span></td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               <p>Which statistic is more relevant? Well, if you're the mayor and if property taxes are based on the appraised value of homes,
                  your total intake will be only 2.5 times greater than it was 20 years ago. If, on the other hand, you are writing a newspaper
                  article and want to convey the extent to which average housing prices have increased over the past 20 years, 3-fold would
                  seem to be a more salient statistic. In other words, MoR tells us about the average effect on individuals, whereas RoM conveys
                  the overall effect on the population as a whole. In the case of the western blot data, 3.07 (i.e., the MoR) is clearly the
                  better indicator, especially given the stated issues with combining data from different blots. Importantly, it is critical
                  to be aware of the difference between RoM and MoR calculations and to report the statistic that is most relevant to your question
                  of interest.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec5-3"></a>5.3.&nbsp;Log scales
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Data from studies where relative or exponential changes are pervasive may also benefit from transformation to log scales.
                  For example, transforming to a log scale is the standard way to obtain a straight line from a slope that changes exponentially.
                  This can make for a more straightforward presentation and can also simplify the statistical analysis (see <a href="#sec6-4" title="6.4.&nbsp;Dealing with outliers">Section 6.4</a> on outliers). Thus, transforming 1, 10, 100, 1,000 into log<sub>10</sub> gives us 0, 1, 2, 3. Which log base you choose doesn't particularly matter, although ten and two are quite intuitive, and
                  therefore popular. The natural<sup><a name="fn50" href="#ftn.fn50">50</a></sup> log (&#8764;2.718), however, has historical precedent within certain fields and may be considered standard. In some cases, back
                  transformation (from log scale to linear) can be done after the statistical analysis to make the findings clearer to readers.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec5-4"></a>5.4.&nbsp;Correlation and modeling
                        </h3>
                     </div>
                  </div>
               </div>
               <p>For some areas of research (such as ecology, field biology, and psychology), modeling, along with its associated statistics,
                  is a predominant form of analysis. This is not the case for most research conducted in the worm field or, for that matter,
                  by most developmental geneticists or molecular biologists. Correlation, in contrast, can be an important and useful concept.
                  For this reason, we include a substantive, although brief, section on correlation, and a practically non-existent section
                  on modeling.
               </p>
               <p>Correlation describes the co-variation of two variables. For example, imagine that we have a worm strain that expresses reporters
                  for two different genes, one labeled with GFP, the other with mCherry<sup><a name="fn51" href="#ftn.fn51">51</a></sup>. To see if expression of the two genes is correlated, GFP and mCherry are measured in 50 individual worms, and the data are
                  plotted onto a graph known as a <span class="emphasis"><em>scatterplot</em></span>. Here, each worm is represented by a single dot with associated GFP and mCherry values corresponding to the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes, respectively (<a href="#figure16">Figure 16</a>). In the case of a positive correlation, the cloud of dots will trend up to the right. If there is a negative correlation,
                  the dots will trend down to the right. If there is little or no correlation, the dots will generally show no obvious pattern.
                  Moreover, the closer the dots come to forming a unified tight line, the stronger the correlation between the variables. Based
                  on <a href="#figure16">Figure 16</a>, it would appear that there is a positive correlation, even if the dots don't fall exactly on a single line. Importantly,
                  it matters not which of the two values (GFP or mCherry) is plotted on the <span class="emphasis"><em>x</em></span> or the <span class="emphasis"><em>y</em></span> axes. The results, including the statistical analysis described below, will come out exactly the same.
               </p>
               <div class="mediaobject" align="center"><a name="figure16"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig16.jpg"><img src="statisticalanalysis_fig16_s.jpg" border="2" align="middle" alt=" figure 16"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 16. Scatterplot of GFP expression versus mCherry.&nbsp;</b>The correlation coefficient is &#8764;0.68. The units on the axes are arbitrary.
                     </p>
                  </div>
               </div>
               <p>The extent of correlation between two variables can be quantified through calculation of a statistical parameter termed the
                  <span class="emphasis"><em>correlation coefficient</em></span> (a.k.a. <span class="emphasis"><em>Pearson's product moment correlation coefficient</em></span>, <span class="emphasis"><em>Pearson's r</em></span>, or just <span class="emphasis"><em>r</em></span>). The formula is a bit messy and the details are not essential for interpretation. The value of <span class="emphasis"><em>r</em></span> can range from &#8722;1 (a perfect negative correlation) to 1 (a perfect positive correlation), or can be 0<sup><a name="fn52" href="#ftn.fn52">52</a></sup> in the case of no correlation. Thus, depending on the tightness of the correlation, values will range from close to zero
                  (weak or no correlation) to 1 or &#8722;1 (perfect correlation). In our example, if one of the two genes encodes a transcriptional
                  activator of the other gene, we would expect to see a positive correlation. In contrast, if one of the two genes encodes a
                  repressor, we should observe a negative correlation. If expression of the two genes is in no way connected, <span class="emphasis"><em>r</em></span> should be close to zero, although random chance would likely result in <span class="emphasis"><em>r</em></span> having either a small positive or negative value. Even in cases where a strong correlation is observed, however, it is important
                  not to make the common mistake of equating correlation with causation<sup><a name="fn53" href="#ftn.fn53">53</a></sup>.
               </p>
               <p>Like other statistical parameters, the SD, SE, and 95% CI can be calculated for <span class="emphasis"><em>r</em></span>. In addition, a <span class="emphasis"><em>P</em></span>-value associated with a given <span class="emphasis"><em>r</em></span> can be determined, which answers the following question: What is the probability that random chance resulted in a correlation
                  coefficient as far from zero as the one observed? Like other statistical tests, larger sample sizes will better detect small
                  correlations that are statistically significant. Nevertheless, it is important to look beyond the <span class="emphasis"><em>P</em></span>-value in assessing biological significance, especially if <span class="emphasis"><em>r</em></span> is quite small. The validity of these calculations also requires many of the same assumptions described for other parametric
                  tests including the one that the data have something close to a normal distribution. Furthermore, it is essential that the
                  two parameters are measured separately and that the value for a given <span class="emphasis"><em>x</em></span> is not somehow calculated using the value of <span class="emphasis"><em>y</em></span> and vice versa.
               </p>
               <p>Examples of six different scatterplots with corresponding <span class="emphasis"><em>r</em></span> and <span class="emphasis"><em>P</em></span>-values are shown in <a href="#figure17">Figure 17</a>. In addition to these values, a black line cutting through the swarm of red dots was inserted to indicate the slope. This
                  line was determined using a calculation known as the <span class="emphasis"><em>least squares</em></span> or <span class="emphasis"><em>linear least squares method</em></span>. The basic idea of this method is to find a straight line that best represents the trend indicated by the data, such that
                  a roughly equal proportion of data points is observed above and below the line. Finally, blue dashed lines indicate the 95%
                  CI of the slope. This means that we can be 95% certain that the true slope (for the population) resides somewhere between
                  these boundaries.
               </p>
               <div class="mediaobject" align="center"><a name="figure17"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig17.jpg"><img src="statisticalanalysis_fig17_s.jpg" border="2" align="middle" alt=" figure 17"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 17. Fits and misfits of regression lines.&nbsp;</b>The units on the axes are arbitrary.
                     </p>
                  </div>
               </div>
               <p>Panels A&#8211;C of <a href="#figure17">Figure 17</a> show examples of a strong (<span class="emphasis"><em>r</em></span> = 0.86), weak (<span class="emphasis"><em>r</em></span> = &#8722;0.27), and nonexistent (<span class="emphasis"><em>r</em></span> = 0.005) correlation, respectively. The purple line in panel B demonstrates that a slope of zero can be fit within the 95%
                  CI, which is consistent with the observed <span class="emphasis"><em>P</em></span>-value of 0.092. Panel D illustrates that although small-sized samples can give the impression of a strong correlation, the
                  <span class="emphasis"><em>P</em></span>-value may be underwhelming because chance sampling could have resulted in a similar outcome. In other words, similar to SD,
                  <span class="emphasis"><em>r</em></span> is not affected by sample size<sup><a name="fn54" href="#ftn.fn54">54</a></sup>, but the <span class="emphasis"><em>P</em></span>-value most certainly will be. Conversely, a large sample size will detect significance even when the correlation coefficient
                  is relatively weak. Nevertheless, for some types of studies, a small correlation coefficient with a low <span class="emphasis"><em>P</em></span>-value might be considered scientifically important. Panels E and F point out the dangers of relying just on <span class="emphasis"><em>P</em></span>-values without looking directly at the scatterplot. In Panel E, we have both a reasonably high value for <span class="emphasis"><em>r</em></span> along with a low P-value. Looking at the plot, however, it is clear that a straight line is not a good fit for these data
                  points, which curve up to the right and eventually level out. Thus, the reported <span class="emphasis"><em>r</em></span> and <span class="emphasis"><em>P</em></span>-values, though technically correct, would misrepresent the true nature of the relationship between these variables. In the
                  case of Panel F, <span class="emphasis"><em>r</em></span> is effectively zero, but it is clear that the two variables have a very strong relationship. Such examples would require
                  additional analysis, such as modeling, which is described briefly below.
               </p>
               <p>Another very useful thing about <span class="emphasis"><em>r</em></span> is that it can be squared to give <span class="emphasis"><em>R<sup>2</sup></em></span><sup></sup> (or <span class="emphasis"><em>r<sup>2</sup></em></span>), also called the <span class="emphasis"><em>coefficient of determination</em></span>. The reason <span class="emphasis"><em>R<sup>2</sup></em></span> is useful is that it allows for a very easy interpretation of the relationship between the two variables. This is best shown
                  by example. In the case of our GFP/mCherry experiment, we obtained <span class="emphasis"><em>r</em></span> = 0.68, and squaring this gives us 0.462. Thus, we can say that 46.2% of the variability in mCherry can be explained by differences
                  in the levels of GFP. The rest, 53.8%, is due to other factors. Of course, we can also say that 46.2% of the variability in
                  GFP can be explained by differences in the levels of mCherry, as the <span class="emphasis"><em>R<sup>2</sup></em></span> itself does not imply a direction. Of course if GFP is a reporter for a transcription factor and mCherry is a reporter for
                  a structural gene, a causal relationship, along with a specific regulatory direction, is certainly suggested. In this case,
                  additional experiments would have to be carried out to clarify the underlying biology.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec5-5"></a>5.5.&nbsp;Modeling and regression
                        </h3>
                     </div>
                  </div>
               </div>
               <p>The basic idea behind <span class="emphasis"><em>modeling</em></span> and <span class="emphasis"><em>regression</em></span> methods is to come up with an equation that can make useful predictions or describe the behavior of a system. In <span class="emphasis"><em>simple linear regression</em></span>, a single <span class="emphasis"><em>predictor</em></span> or <span class="emphasis"><em>independent variable</em></span>, such as the GFP intensity of a heat-shock reporter, might be used to predict the behavior of a <span class="emphasis"><em>response</em></span> or <span class="emphasis"><em>dependent variable</em></span>, such as the life span of a worm<sup><a name="fn55" href="#ftn.fn55">55</a></sup>. The end result would be an equation<sup><a name="fn56" href="#ftn.fn56">56</a></sup> that describes a line that is often, although not always, straight<sup><a name="fn57" href="#ftn.fn57">57</a></sup>. <span class="emphasis"><em>Multiple regression</em></span> is an extension of simple linear regression, but it utilizes two or more variables in the prediction<sup><a name="fn58" href="#ftn.fn58">58</a></sup>. A classic example of multiple regression used in many statistics texts and classes concerns the weight of bears. Because
                  it's not practical to weigh bears in the field, proxy measures such as head circumference, body length, and abdominal girth
                  are acquired and fitted to an equation (by a human-aided computer or a computer-aided human), such that approximate weights
                  can be inferred without the use of a scale. Like single and multiple linear regression, <span class="emphasis"><em>nonlinear regression</em></span> also fits data (i.e., predictive variables) to a curve that can be described by an equation. In some cases, the curves generated
                  by nonlinear regression may be quite complex. Unlike linear regression, nonlinear regression cannot be described using simple
                  algebra. Nonlinear regression is an <span class="emphasis"><em>iterative</em></span> method, and the mathematics behinds its workings are relatively complex. It is used in a number of fields including pharmacology.
                  <span class="emphasis"><em>Logistic regression</em></span> uses one or more factors to predict the probability or odds of a <span class="emphasis"><em>binary</em></span> or <span class="emphasis"><em>dichotomous</em></span> outcome, such as life or death. It is often used to predict or model mortality given a set of factors; it is also used by
                  employers in decisions related to hiring or by government agencies to predict the likelihood of criminal recidivism<sup><a name="fn59" href="#ftn.fn59">59</a></sup>.
               </p>
            </div>
            <div class="footnotes"><br><hr width="100" align="left">
               <div class="footnote">
                  <p><sup><a name="ftn.fn46" href="#fn46">46</a></sup>Admittedly, standard western blots would also contain an additional probe to control for loading variability, but this has
                     been omitted for simplification purposes and would not change the analysis following adjustments for differences in loading.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn47" href="#fn47">47</a></sup>A similar, although perhaps slightly less stringent argument, can be made against averaging cycle numbers from independent
                     qRT-PCR runs. Admittedly, if cDNA template loading is well controlled, qRT-PCR cycle numbers are not as prone to the same
                     arbitrary and dramatic swings as bands on a western. However, subtle differences in the quality or amount of the template,
                     chemical reagents, enzymes, and cycler runs can conspire to produce substantial differences between experiments.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn48" href="#fn48">48</a></sup>This Excel tool was developed by KG.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn49" href="#fn49">49</a></sup>The maximum possible P-values can be inferred from the CIs. For example, if a 99% CI does not encompass the number one, the
                     ratio expected if no difference existed, then you can be sure the P-value from a two-tailed test is &lt;0.01.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn50" href="#fn50">50</a></sup>Admittedly, there is nothing particularly &#8220;natural&#8221; sounding about 2.718281828&#8230;
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn51" href="#fn51">51</a></sup>An example of this is described in Doitsidou et al., 2007
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn52" href="#fn52">52</a></sup>In the case of no correlation, the least-squares fit (which you will read about in a moment) will be a straight line with
                     a slope of zero (i.e., a horizontal line). Generally speaking, even when there is no real correlation, however, the slope
                     will always be a non-zero number because of chance sampling effects.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn53" href="#fn53">53</a></sup>For example, nations that that supplement their water with fluoride have higher cancer rates. The reason is not because fluoride
                     is mutagenic. It is because fluoride supplements are carried out by wealthier countries where health care is better and people
                     live longer. Since cancer is largely a disease of old age, increased cancer rates in this case simply reflect a wealthier
                     long-lived population. There is no meaningful cause and effect. On a separate note, it would not be terribly surprising to
                     learn that people who write chapters on statistics have an increased tendency to become psychologically unhinged (a positive
                     correlation). One possibility is that the very endeavor of a writing about statistics results in authors becoming mentally
                     imbalanced. Alternatively, volunteering to write a statistics chapter might be a symptom of some underlying psychosis. In
                     these scenarios cause and effect could be occurring, but we don't know which is the cause and which is the effect.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn54" href="#fn54">54</a></sup>In truth, SD is affected very slightly by sample size, hence SD is considered to be a &#8220;biased&#8221; estimator of variation. The
                     effect, however, is small and generally ignored by most introductory texts. The same is true for the correlation coefficient,
                     r.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn55" href="#fn55">55</a></sup>Rea et al. (2005) Nat. Genet. 37, 894-898. In this case, the investigators did not conclude causation but nevertheless suggested
                     that the reporter levels may reflect a physiological state that leads to greater longevity and robust health. Furthermore,
                     based on the worm-sorting methods used, linear regression was not an applicable outcome of their analysis.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn56" href="#fn56">56</a></sup>The standard form of simple linear regression equations takes the form y = b<sub>1</sub>x + b<sub>0</sub>, where y is the predicted value for the response variable, x is the predictor variable, b<sub>1</sub> is the slope coefficient, and b<sub>0</sub> is the y-axis intercept. Thus, because b1 and b0 are known constants, by plugging in a value for x, y can be predicted. For
                     simple linear regression where the slope is a straight line, the slope coefficient will be the same as that derived using
                     the least-squares method.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn57" href="#fn57">57</a></sup>Although seemingly nonsensical, the output of a linear regression equation can be a curved line. The confusion is the result
                     of a difference between the common non-technical and mathematical uses of the term &#8220;linear&#8221;. To generate a curve, one can
                     introduce an exponent, such as a square, to the predictor variable (e.g., x<sup>2</sup>). Thus, the equation could look like this: y = b<sub>1</sub>x<sup>2</sup> + b0.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn58" href="#fn58">58</a></sup>A multiple regression equation might look something like this: Y = b<sub>1</sub>X<sub>1</sub> + b<sub>2</sub>X<sub>2</sub> - b<sub>3</sub>X<sub>3</sub> + b<sub>0</sub>, where X<sub>1-3</sub> represent different predictor variables and b<sub>1-3</sub> represent different slope coefficients determined by the regression analysis, and b0 is the Y-axis intercept. Plugging in
                     the values for X<sub>1-3</sub>, Y could thus be predicted.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn59" href="#fn59">59</a></sup>Even without the use of logistic regression, I can predict with near 100% certainty that I will never agree to author another
                     chapter on statistics! (DF)
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec6"></a>6.&nbsp;Additional considerations and guidelines
                     </h2>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-1"></a>6.1.&nbsp;When is a sample size too small?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Several issues can arise with small sample sizes. One is that with weak <span class="emphasis"><em>statistical power</em></span>, we may simply fail to detect a finding of interest, a point that is addressed below. Even when we do detect a significant
                  effect, however, our conclusions may be undermined because one of the underlying assumptions of many statistical tests&#8212;that
                  the <span class="emphasis"><em>population</em></span> from which the data are derived is approximately <span class="emphasis"><em>normally</em></span> <span class="emphasis"><em>distributed</em></span>&#8212;is not true. This is a bit of a chicken and egg conundrum. If our sample size is sufficiently large (e.g., &gt;30), then we
                  can check to see if the underlying population is likely to have a normal (or &#8220;normal enough&#8221;) distribution. If it does, then
                  we're set. If not, we may still be safe because our sample size is sufficient to compensate for lack of normality in the population.
                  Put another way, most statistical tests are robust to all but the most skewed distributions provided that the sample size
                  is large enough. Conversely, with a small sample size, we can't tell much, or perhaps anything, about the underlying normality
                  of the population. Moreover, if the population deviates far enough from normal, then our statistical tests will not be valid.
               </p>
               <p>So, what to do? As already discussed in earlier sections, in some cases it may be reasonable to assume that the underlying
                  distribution is likely normal enough to proceed with the test. Such a decision could be reasonable if this coincides with
                  accepted standards in the field or in cases where the laboratory has previously generated similar larger data sets and shown
                  them to have approximately normal distributions. In other cases, you may decide that collecting additional data points is
                  necessary and justified. Alternatively, you may need to consider using a <span class="emphasis"><em>non-parametric</em></span> statistical approach for which normality of the data is not a prerequisite (discussed below). Typically, non-parametric tests
                  will have less power, however, so the effects may need to be stronger to achieve statistical significance. If you are uncertain
                  about what to do, consulting a nearby statistician is recommended. In general, one should be cautious about interpreting <span class="emphasis"><em>P</em></span>-values, especially ones that would be considered borderline (e.g., <span class="emphasis"><em>P</em></span> = 0.049), when normality is uncertain and sample sizes are small.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-2"></a>6.2.&nbsp;Statistical power
                        </h3>
                     </div>
                  </div>
               </div>
               <p>We can also say that a sample size is too small when real effects that would be considered biologically interesting or important
                  fail to be detected or supported by our analyses. Such cases may be referred to as <span class="emphasis"><em>false negatives</em></span> or <span class="emphasis"><em>Type II</em></span> errors. Of course, we are seldom in a position of knowing <span class="emphasis"><em>a priori</em></span> what any outcome <span class="emphasis"><em>should</em></span> be. Otherwise, why would we be doing the experiment? Thus, knowing when a Type II error has occurred is not possible. Likewise,
                  knowing when a <span class="emphasis"><em>false positive</em></span> or <span class="emphasis"><em>Type I</em></span> error has occurred is also impossible. In this latter case, however, the chosen &#945; level represents a stated degree of acceptable
                  risk that a false positive will sneak through. So what should be the allowable risk for a false negative? There is no simple
                  answer to this. In cases where significant health or environmental consequences could arise, the acceptable level of risk
                  may be very low. In contrast, if we are screening an entire genome where we are likely to uncover hundreds or thousands of
                  positive hits, missing a few genes may not be a problem.
               </p>
               <p>The branch of statistics that deals with issues related to false-negative findings is called <span class="emphasis"><em>power analysis</em></span>. Power analysis answers the following question: If there is a real effect of a certain magnitude, what is the probability
                  that a study of a given size will detect it? Put another way, if the experiment was repeated many times, what fraction would
                  lead to statistically significant outcomes? The answer to this will depend on several factors. Detecting a statistically significant
                  effect is easier when the magnitude of the effect (e.g., the difference between means) is larger and when the variation within
                  samples (e.g., SD) is smaller. In addition, false negatives are obviously more likely to occur with lower &#945; levels than with
                  higher ones (e.g., 0.001 versus 0.05).
               </p>
               <p>In the case of clinical drug trials, the relevance of statistical power is self-evident. Pharmaceutical companies want to
                  be efficient with their time and money, but not at the expense of falsely concluding that a drug is without significant benefits<sup><a name="fn60" href="#ftn.fn60">60</a></sup>. Thus, prior to any clinical trial, a power analysis is carried out to determine how many subjects must undergo the treatment
                  in order for the study to stand a good chance of detecting a benefit of some specified size. Presumably, failing to detect
                  an effect that is smaller in magnitude than the agreed-upon cutoff would be okay because a relatively ineffective drug would
                  have little commercial value. Power analysis is also critical to biological field research, where collecting samples may be
                  time consuming and costly. In other words, why spend a year chasing after data if an initial power test tells you that you'll
                  be unlikely to detect an effect that you'd consider essential for the endeavor to be worthwhile?
               </p>
               <p>Admittedly, power analysis is not something that springs to the minds of most <span class="emphasis"><em>C. elegans</em></span> researchers prior to conducting an experiment. We tend to run the experiment with what we think is a reasonable number of
                  specimens and just see what happens. There are potentially two problems with this &#8220;shoot from the hip&#8221; approach. One is that
                  we may fail to detect true positives of interest. The second is that what we may be doing is already overkill. Namely, our
                  sample sizes may be larger than necessary for detecting biologically significant effects. In particular, if we are doing genome-wide
                  screens, it is in our interest to be as efficient with our time (and money) as possible.
               </p>
               <p>Rather than discussing the formulas that go into calculating power, this section contains links to Excel tools, along with
                  an instructional manual, that can be used to calculate the power of an analysis when comparing means, proportions, and other
                  statistical parameters of interest. In addition, a number of websites and software programs can also serve this purpose. Our
                  intention is that these tools be used by investigators to make accurate projections as to the sample size that will be required
                  to observe effects of a given magnitude. For our example, we will imagine that we are testing RNAi feeding clones that correspond
                  to several hundred transcription factors to see if they affect expression of an embryonic GFP reporter at the comma stage.
                  Because quantifying expression in individual embryos is fairly time consuming, we want to calculate the minimum number of
                  embryos required per RNAi strain, such that we have at least a 90% chance of identifying RNAi clones that lead to some minimum
                  acceptable fold-change. Based on what we know about the biology of the system, we decide that expression increases or decreases
                  of at least 1.5-fold are likely to be most relevant, although slightly smaller changes could also be of interest.
               </p>
               <p>First we analyze GFP expression in 100 embryos from control RNAi plates. We find that the average number of pixels per embryo
                  is 5000, that the SD = 2000, and thus the coefficient of variation is therefore 0.4 (2000/5000). We also learn that expression
                  in comma-stage embryos is approximately normally distributed. For an effect to be of interest then, the average embryonic
                  expression would have to be &gt;7500 or &lt;3300. In addition, we will assume that the SDs of the test samples vary proportionally
                  with the mean. In other words, the SD for our test clones will be identical to our control in cases where there is no difference
                  in mean expression but will be proportionally larger or smaller when RNAi enhances or decreases expression, respectively.
                  This latter assumption is common and fairly intuitive given that larger means are typically associated with larger SDs, including
                  situations where the coefficient of variation remains constant. Finally, we choose the standard &#945; of 0.05.
               </p>
               <p>Plugging these numbers into the Excel tool, we find that with a power of 90%, 18 embryos should be sufficient to detect a
                  1.5-fold (or 50%) increase in mean GFP expression, whereas 9 embryos are sufficient to detect a 1.5-fold (or 34%) decrease
                  (using a two-tailed <span class="emphasis"><em>t</em></span>-test; <a href="#figure18">Figure 18A, B</a>). Playing with the tool, we also notice that increasing our sample size to 22 would give us 95% power to detect 1.5-fold
                  increases (<a href="#figure18">Figure 18C</a>) and &gt;99% power to detect decreases of 1.5-fold or greater. In fact, with 22 embryos, we'd have a 90% chance of detecting
                  a 1.33-fold (or 25%) decrease in expression (<a href="#figure18">Figure 18D</a>). This seems like a good deal to us and is experimentally feasible, so we decide ultimately to go with 22 embryos as our
                  target sample size. In fact, this kind of back-and-forth negotiation with respect to sample size, sensitivity, and power is
                  exactly what is supposed to take place. In any event, by carrying out the power analysis before we do our screen, we can be
                  much more confident that our studies will be efficient and fruitful.
               </p>
               <div class="mediaobject" align="center"><a name="figure18"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig18.jpg"><img src="statisticalanalysis_fig18_s.jpg" border="2" align="middle" alt=" figure 18"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 18. Examples of output from Power Calculator.&nbsp;</b>This Excel tool is available along with this chapter.
                     </p>
                  </div>
               </div>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-3"></a>6.3.&nbsp;Can a sample size be too large?
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Although it may sound paradoxical, a larger-than-necessary sample size not only may be wasteful but also could lead to results
                  that are grossly misleading. There are two reasons for this, one biological and one ethical. In the case of the former, large
                  sample sizes can lead to statistically significant outcomes even when differences are so small in magnitude as to be biologically
                  irrelevant. In this case, the detected difference is real, it's just not important. As previously stated, statistical significance
                  should never be confused with biological significance. Unfortunately, some readers may be tempted to rely on the reported
                  <span class="emphasis"><em>P</em></span>-value as a kind of proxy for discerning the bottom line with respect to &#8220;significance&#8221; in the broader sense. Don't fall into
                  this trap.
               </p>
               <p>The ethical issue surrounding large sample sizes can be illustrated by a thought experiment. Imagine that you are testing
                  for differences between two populations that are in fact identical for some trait of interest (admittedly, something you could
                  never know beforehand). An initial sample size of 10 from each group shows a slight difference in means, as could be expected,
                  but this difference isn't significant at the standard &#945; level of 0.05. Not to be discouraged, you then increase each sample
                  size by 10 and try the test again. If there is still no significant difference, you try adding another 10 and repeat the analysis.
                  Will you eventually get a <span class="emphasis"><em>P</em></span>-value &#8804; 0.05? To answer this question, first recall the meaning of &#945; = 0.05. Five percent of the time, the test will show
                  a statistically significant difference even when the two populations are identical. Put another way, if you were to carry
                  out 100 tests, each with a sample size of 10, 5 tests, on average, would give <span class="emphasis"><em>P</em></span> &#8804; 0.05 by random chance. Had you carried out 100 tests, would you only report the five that were significant? Hopefully not!
                  Well, it turns out that that is effectively what you would be doing if you were to repeatedly increase your sample size with
                  the intent of uncovering a positive finding. At some point, the two samples will show a statistically significant difference
                  by chance&#8212;it's guaranteed! In fact, over the lifetime of such a &#8220;never-ending study&#8221;, chance differences will lead to <span class="emphasis"><em>P</em></span> &#8804; 0.05 5% of the time, <span class="emphasis"><em>P</em></span> &#8804; 0.01 1% of the time, and, if one were to be particularly persistent, a <span class="emphasis"><em>P</em></span>-value of &lt;0.001 could most certainly be achieved. Obviously the magnitude of the effects in this scenario would be very small
                  (irrelevant in fact), but that won't matter to the statistical test. This kind of approach, sometimes referred to as an <span class="emphasis"><em>ad hoc</em></span>, is clearly a major no-no, but it is a surprisingly easy trap to fall into. So remember that an experiment carried out <span class="emphasis"><em>ad infinitum</em></span> will eventually give you a significant <span class="emphasis"><em>P</em></span>-value even if none exists. Power analysis, as discussed above, can be a rational way to determine the necessary sample size
                  in advance based on a specified &#945; level and a pre-determined minimal difference of interest.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-4"></a>6.4.&nbsp;Dealing with outliers
                        </h3>
                     </div>
                  </div>
               </div>
               <p>When it comes to outliers, there is no real consensus&#8212;not about how to formally define them, not about how best to detect
                  them, and, most importantly, not about what to actually do with them. There is, of course, universal agreement that any data
                  that are reported in the literature should be accurate and real. The slippery slope comes into play because discarding a data
                  point that falls outside of the expected range of values also serves the purpose of yielding a cleaner, more appealing, and
                  more statistically convincing result. In fact, getting rid of outliers, even ones that should likely be discarded, nearly
                  always does just that. Note that we do not endorse the strategy of throwing out objectionable data points simply to make your
                  results appear cleaner!
               </p>
               <p>Outliers can arise in several ways. One is through incorrect data entry. Another is through experimental error, either on
                  the part of the investigator or the apparatus being used. If there is good reason to believe that any data point, outlier
                  or not, is likely to be flawed in either of these ways, getting rid of it is an easy call. Outliers, however, may also be
                  legitimate, biologically correct data points that have occurred either through chance sampling or because the biology of the
                  system is prone to producing such values. In fact, if it's the underlying biology that's driving things, this may turn out
                  to be the most interesting outcome of the experiment.
               </p>
               <p>Outliers principally affect statistics that are based on <span class="emphasis"><em>continuous variables</em></span>, such as means and SDs, but do not generally have much of an impact on <span class="emphasis"><em>discrete</em></span> or <span class="emphasis"><em>non-continuous variables</em></span>, such as medians. For example, the median of the five values 4, 6, 7, 8, 10 is the same as the median of 4, 6, 7, 8, 235.
                  In contrast, the means and SDs of these two data sets are very different. The resistance of median values to the effects of
                  outliers provides a reasonable argument for using medians over means under certain circumstances. For example, in a subdivision
                  with 100 houses, a mean value of $400,000 may be due to a handful of multi-million dollar properties. A median value of $250,000,
                  however, might better reflect the cost of a <span class="emphasis"><em>typical</em></span> home. Some problems with medians are that they are not as widely used in the scientific literature as means and also require
                  a different set of statistical tools for their analysis. Specifically, the statistical analysis of medians is carried out
                  using nonparametric approaches, such as a <span class="emphasis"><em>sign test</em></span>, or through computational methods, such as bootstrapping (discussed below).
               </p>
               <p>Concluding that an unexpected or anomalous value is an outlier deserving of banishment is not something that should ever be
                  done informally. Some would argue that it's not something that should ever be done formally either. In any case, the danger
                  of the informal or <span class="emphasis"><em>ad hoc</em></span> approach is that our eyes, as well as our preconceptions, will identify many more outliers than really exist. In addition,
                  our hopes and biases for certain experimental outcomes can cloud our judgment as to which values should be removed. Several
                  formal tests can be used to identify &#8220;rejectable&#8221; outliers including the <span class="emphasis"><em>Grubb's outlier test</em></span> and <span class="emphasis"><em>Dixon's Q test</em></span>. These tests will answer the following question: Given a specified sample size, what is the probability that a value of this
                  magnitude would occur assuming that the population from which the sample is derived is <span class="emphasis"><em>normally distributed</em></span>? If this probability is &#8804;0.05, you then have legitimate grounds for tossing the data point. Alternatively, you can report
                  your findings with and without the outlier present and let readers make up their own minds.
               </p>
               <p>Outliers can also appear to be present in samples from populations that have <span class="emphasis"><em>lognormal distributions</em></span>. In fact, without taking this into account, you could mistakenly reject a legitimate data point based on a statistical test
                  that assumes normality. Lognormal distributions turn out to be quite common in biology. Whereas normal distributions result
                  when many factors contribute <span class="emphasis"><em>additively</em></span> to some measured outcome, lognormal distributions occur when factors act in a <span class="emphasis"><em>multiplicative</em></span> fashion. Thus, sample data that appear to contain an outlier or otherwise look to be non-Gaussian, should be tested first
                  by transforming the raw data into their corresponding log values and plotting them on a histogram. If the distribution of
                  the transformed values looks normal, then the original distribution was probably lognormal. To handle things statistically,
                  you can use the log-transformed data to carry out tests that assume normality. Back-transformations to the linear scale can
                  also be carried out if necessary to report certain values. In the case of means, this will produce a parameter referred to
                  as the <span class="emphasis"><em>geometric mean</em></span>, which (like the median) will tend to give values that are more typical of the data set. For example, we can transform the
                  numbers 6, 12, 19, 42, and 951 to their log<sub>10</sub> values (0.78, 1.1, 1.3, 1.6, 3.0), calculate their mean (1.55), and then back-transform (10<sup>x</sup>) to get a geometric mean of 35.3, which differs substantially from the standard or <span class="emphasis"><em>arithmetic mean</em></span> of 206.
               </p>
               <p>Another approach, which may be particularly well suited to handling situations where inconsistent or spurious data tend to
                  arise at either end of the spectrum, is to use <span class="emphasis"><em>trimmed</em></span> or <span class="emphasis"><em>truncated</em></span> data. For example, a 10% <span class="emphasis"><em>trimmed</em></span> <span class="emphasis"><em>mean</em></span> would remove 10% of the highest values AND 10% of the lowest values. Thus, for the series 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
                  both 1 and 10 would be removed before calculating the mean. This method can provide more information than the median<sup><a name="fn61" href="#ftn.fn61">61</a></sup> but is conveniently resistant to the effects of outliers. An example of an appropriate use for a trimmed mean might involve
                  quantifying protein expression in <span class="emphasis"><em>C. elegans</em></span> embryos using immunostaining. Even if done carefully, this method can result in a small fraction of embryos that have no
                  staining as well as some that appear to have high levels of non-specific staining. Thus, the trimmed mean would provide a
                  reasonable measure of the <span class="emphasis"><em>central tendency</em></span> of the dataset.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-5"></a>6.5.&nbsp;Nonparametric tests
                        </h3>
                     </div>
                  </div>
               </div>
               <p>Most of the tests described in the above sections rely on the assumption that the data are sampled from populations with relatively
                  normal distributions<sup><a name="fn62" href="#ftn.fn62">62</a></sup>. Such tests, which include the <span class="emphasis"><em>t</em></span>-test, are referred to as <span class="emphasis"><em>parametric tests</em></span>. <span class="emphasis"><em>Nonparametric tests</em></span> should be used if the population distributions are known to deviate or are suspected to deviate far from normality. In addition,
                  nonparametric tests may be appropriate if a population deviates only moderately from a normal distribution but the sample
                  size is insufficient to compensate for this at the level of the <span class="emphasis"><em>distribution of the statistic</em></span>. Although it would be nice to be able to provide some cut-and-dry criteria for making a call regarding when to use parametric
                  versus nonparametric tests, the minimum sample size required for conducting a parametric test will depend on the degree to
                  which the underlying population deviates from normality. The larger the deviation, the larger the sample size must be for
                  parametric tests to be valid. Although some texts recommend a sample size of 30 as being safe, this may be insufficient if
                  the population distribution is very far from normal. Nonparametric tests are also required when statistics are to be carried
                  out on <span class="emphasis"><em>discontinuous</em></span> parameters such as medians.
               </p>
               <p>The basis for many nonparametric tests involves discarding the actual numbers in the dataset and replacing them with numerical
                  rankings from lowest to highest. Thus, the dataset 7, 12, 54, 103 would be replaced with 1, 2, 3, and 4, respectively. This
                  may sound odd, but the general method, referred to as a <span class="emphasis"><em>sign test</em></span>, is well grounded. In the case of the Mann-Whitney test, which is used to compare two unpaired groups, data from both groups
                  are combined and ranked numerically (1, 2, 3, &#8230; <span class="emphasis"><em>n</em></span>). Then the rank numbers are sorted back into their respective starting groups, and a <span class="emphasis"><em>mean rank</em></span> is tallied for each group<sup><a name="fn63" href="#ftn.fn63">63</a></sup>. If both groups were sampled from populations with identical means (the null hypothesis), then there should be relatively
                  little difference in their mean ranks, although chance sampling will lead to some differences. Put another way, high- and
                  low-ranking values should be more or less evenly distributed between the two groups. Thus for the Mann-Whitney test, the <span class="emphasis"><em>P</em></span>-value will answer the following question: Based on the mean ranks of the two groups, what is the probability that they are
                  derived from populations with identical means? As for parametric tests, a <span class="emphasis"><em>P</em></span>-value &#8804; 0.05 is traditionally accepted as statistically significant.
               </p>
               <p>The obvious strength of nonparametric tests is that they allow an investigator to conduct a statistical analysis that would
                  otherwise not be possible (or at least valid) using traditional parametric tests. A downside is that nonparametric tests are
                  somewhat less powerful than parametric tests and so should be applied only when necessary<sup><a name="fn64" href="#ftn.fn64">64</a></sup>. The reduced power is due to the fact that quantitative information is discarded when ranks are assigned. In addition, nonparametric
                  tests will often lack the ability to provide CIs, which can be quite useful. Finally, nonparametric tests can underestimate
                  the effects of outliers, which may be valid data points and biologically significant. Below is a brief list of some common
                  nonparametric tests and their applications. Note that although these are so-called nonparametric tests, most do require certain
                  assumptions about the data and collection methods for their results to be valid. Be sure to read up on these methods or consult
                  a statistician before you apply them.
               </p>
               <p>The <span class="emphasis"><em>Mann-Whitney test</em></span> (a.k.a. Wilcoxon rank-sum test, Mann-Whitney <span class="emphasis"><em>U</em></span>-test, and sign test) is used in the place of a two-sample <span class="emphasis"><em>t</em></span>-test to compare two groups. Variations on this theme are also used to test for differences in medians.
               </p>
               <p>The <span class="emphasis"><em>Wilcoxon matched pairs signed-rank test</em></span> (a.k.a. Wilcoxon's test and Wilcoxon signed-rank test) is used in place of the paired <span class="emphasis"><em>t</em></span>-test for matched data.
               </p>
               <p>The <span class="emphasis"><em>one-sample sign tool</em></span>, which is similar to a one-sample <span class="emphasis"><em>t</em></span>-test, is used for medians and can also provide CIs. No major assumptions are required.
               </p>
               <p>The <span class="emphasis"><em>one-sample Wilcoxon signed-rank test</em></span> has the same applications as the one-sample sign tool but is more powerful. It does, however, assume that the distribution
                  of values is symmetric around the median.
               </p>
               <p>The <span class="emphasis"><em>Kruskal-Wallis test</em></span> is used in place of a one-way ANOVA.
               </p>
               <p>The <span class="emphasis"><em>rank correlation test</em></span> (a.k.a. Spearman's rank correlation coefficient and Spearman's rho) is used in place of linear correlation.
               </p>
               <p>The <span class="emphasis"><em>logrank test</em></span> (a.k.a. Mantel Cox method) is used to compare survival curves.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-6"></a>6.6.&nbsp;A brief word about survival
                        </h3>
                     </div>
                  </div>
               </div>
               <p>In the fields of aging and stress, it is common to compare two or more strains for differences in innate laboratory lifespan
                  or in survival following an experimental insult. Relative to studies on human subjects, survival analysis in <span class="emphasis"><em>C. elegans</em></span> and other simple laboratory organisms is greatly simplified by the nature of the system. The experiments generally begin
                  on the same day for all the subjects within a given sample and the study is typically concluded after all subjects have completed
                  their lifespans. In addition, individual <span class="emphasis"><em>C. elegans</em></span> rarely need to be <span class="emphasis"><em>censored</em></span>. That is they don't routinely move way without leaving a forwarding address (although they have been known to crawl off plates)
                  or stop taking their prescribed medicine.
               </p>
               <p>One relatively intuitive approach in these situations would be to calculate the mean duration of survival for each sample
                  along with the individual SDs and SEs. These averages could then be compared using a <span class="emphasis"><em>t</em></span>-test or related method. Though seemingly straightforward, this approach has several caveats and is not recommended (<a href="#bib19">Suciu et al., 2004</a>). One issue is that survival data are often quite skewed, and therefore tests that rely on assumptions of normality (e.g.,
                  the <span class="emphasis"><em>t</em></span>-test) may not be valid. A second is that the <span class="emphasis"><em>t-test</em></span> will have less power than some well-accepted alternative tests to detect small, but biologically meaningful, differences
                  between populations. Another possible approach would be to focus on differences in survival at just one or perhaps several
                  selective time points. However, this approach has the disadvantage of superficially appearing to be arbitrary, and it also
                  fails to make use of all the available data.
               </p>
               <p>The preferred alternative to the approaches discussed above is to make use of statistical tests that compare viability over
                  the duration of the entire experiment, wherein each time point represents a cumulative opportunity to detect differences.
                  For the <span class="emphasis"><em>C. elegans</em></span> field, this can be accomplished using a standard version of the <span class="emphasis"><em>logrank test</em></span>. The null hypothesis of the logrank test is that there is no difference in survival (or some other trait of interest) between
                  the two populations under study. Thus, a trend of consistent differences over time would lead to rejection of the null hypothesis
                  and a statistically significant finding. The logrank test is mathematically accomplished in part by combining the survival
                  data from both populations and using this value to calculate the expected number of <span class="emphasis"><em>events</em></span> (such as deaths) that would be predicted to occur in each population at each time point.
               </p>
               <p>For example, from starting samples of 20 worms each, if five deaths occurred in strain <span class="emphasis"><em>A</em></span> and one death occurred in strain <span class="emphasis"><em>B</em></span> on day 1, then the total number of deaths on day 1 would be six. In this case, the predicted number of deaths for each strain
                  on day 1 is simply the average of the two strains, namely three deaths. Thus, the difference between the observed and expected
                  (<span class="emphasis"><em>E<sub>obs</sub></em></span> &#8211; <span class="emphasis"><em>E<sub>exp</sub></em></span>) deaths for strain <span class="emphasis"><em>A</em></span> is 2 (5 &#8722; 3 = 2) and is &#8722;2 for strain <span class="emphasis"><em>B</em></span> (1 &#8722; 3 = &#8722;2). These calculations get slightly more cumbersome at subsequent time points because adjustments must be made
                  to compensate for differences in sample sizes at the start of each day (only the viable animals remain to be sampled), but
                  the principle remains the same<sup><a name="fn65" href="#ftn.fn65">65</a></sup>. In this example, if the frequency of death in strain <span class="emphasis"><em>A</em></span> was authentically higher, then summing the differences from each day (until the strain <span class="emphasis"><em>A</em></span> sample had perished entirely) should give a positive number. If this number were sufficiently large<sup><a name="fn66" href="#ftn.fn66">66</a></sup> then a low <span class="emphasis"><em>P</em></span>-value would be detected. If daily differences were due only to chance, however, it would be expected that a roughly equal
                  number of positive and negative differences would be obtained over the course of the experiment, and thus the summed values
                  would be close to zero, leading to a high <span class="emphasis"><em>P</em></span>-value (e.g., &gt;0.05). It is important to note that this test as described above is always between two populations, and thus
                  the two populations tested should be explicitly stated for each <span class="emphasis"><em>P</em></span>-value. Furthermore, issues of multiple comparisons should be taken into account when multiple samples are being compared.
               </p>
            </div>
            <div class="sect2" lang="en">
               <div class="titlepage">
                  <div>
                     <div>
                        <h3 class="title"><a name="sec6-7"></a>6.7.&nbsp;Fear not the bootstrap
                        </h3>
                     </div>
                  </div>
               </div>
               <p>I (DF) have it on good word (from KG) that if statistics were being invented today, <span class="emphasis"><em>bootstrapping</em></span>, as well as related <span class="emphasis"><em>re-sampling methods</em></span>, would be the standard go-to approach. The reason that the statistics field took so long to come up with bootstrapping is
                  forgivable. Until very recently, the computing power required to run resampling methods was unfathomable. Had statistics been
                  invented in the last 10 years, however, the central statistical principles that underlie most of our favorite tests would
                  probably be relegated to the side stream of mathematical curiosities as interesting but obscure numerical properties of interest
                  only to theoreticians. Because the genesis and much of the development of statistics took place in the absence of powerful
                  computers we are still, however, entrenched in what could be viewed as an outdated way of doing things. This is not to suggest
                  that the old ways don't work quite well. In fact, they usually give answers that are nearly identical to those provided by
                  the newer computationally intensive methods. The newer methods, however, require fewer assumptions and are therefore more
                  broadly applicable and have fewer associated caveats. In particular, re-sampling methods do not require that the data be sufficiently
                  normal. As such, bootstrapping and related approaches are in fact nonparametric methods<sup><a name="fn67" href="#ftn.fn67">67</a></sup>. Re-sampling methods can also be applied to statistical parameters that are not handled well by traditional methods, such
                  as the median. Differences between the re-sampling methods and the nonparametric approaches outlined above are that re-sampling
                  methods are not limited to the use of ranked-sign data and can therefore be applied to a wider range of statistical parameters,
                  such as CIs. These properties mean that re-sampling methods have greater statistical power and flexibility than traditional
                  nonparametric approaches. All this points to a future where re-sampling methods will largely rule the day. Get ready.
               </p>
               <p>The basic idea of bootstrapping is to use a sample dataset of modest size to simulate an entire population. An example is
                  provided by carrying out calculations to derive a 95% CI of the mean using the data that were analyzed in <a href="#figure5">Figure 5A</a> (<a href="#sec2-2" title="2.2.&nbsp;Understanding the t-test: a brief foray into some statistical theory">Section 2.2</a>; <a href="#figure19">Figure 19</a>). Here, 55 data points comprise the original sample of measured GFP intensities. In the first round of bootstrapping, the
                  55 data points are randomly <span class="emphasis"><em>re-sampled with replacement</em></span> to obtain a new data set of 55 values (<a href="#figure19">Figure 19</a>). Notice that, by sampling randomly with replacement, some of the data points from the original dataset are missing, whereas
                  others are repeated two or more times. A mean is then calculated for the re-sampled set, along with other statistical parameters
                  that may be of interest. Round one is done. The results of rounds two and three are also shown for clarity (<a href="#figure19">Figure 19</a>). Now repeat 3,997 more times. At this point, there should be 4,000 means obtained entirely through re-sampling. Next, imagine
                  lining up the 4,000 means from lowest to highest, top to bottom (<a href="#figure19">Figure 19</a>). We can partition off the lowest (top of list) 2.5% of values by drawing a line between the mean values at positions 100
                  and 101. We can also do this for the highest (bottom of list) 2.5% of values by drawing a line between the means at positions
                  3,900 and 3,901. To get a 95% CI, we simply report the mean values at positions 101 (14.86890) and 3900 (19.96438). Put another
                  way, had we carried out just 40 iterations<sup><a name="fn68" href="#ftn.fn68">68</a></sup> instead of 4,000, the 95% CI would range from the second highest to the second lowest number. Thus, at its core, bootstrapping
                  is conceptually very simple<sup><a name="fn69" href="#ftn.fn69">69</a></sup>. The fact that it's a bear computationally matters only to your computer.
               </p>
               <div class="mediaobject" align="center"><a name="figure19"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig19.jpg"><img src="statisticalanalysis_fig19_s.jpg" border="2" align="middle" alt=" figure 19"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 19. Illustration of the bootstrap process.&nbsp;</b>The original dataset is the one used for Section 2.2, Figure 5A.
                     </p>
                  </div>
               </div>
               <p>The 4,000 individual means obtained through re-sampling can also be used to construct a histogram, which you may notice looks
                  strikingly similar to a normal curve (<a href="#figure20">Figure 20</a>). Not surprisingly, the apex of the curve corresponds very closely to the mean value of the original 55 data points (17.32;
                  <a href="#figure5">Figures 5A</a> and <a href="#figure19">19</a>). Furthermore, if we examine the values at approximately two SDs (1.96 to be exact) to either side of the red vertical lines
                  in <a href="#figure20">Figure 20</a>, we obtain 14.7 and 19.9, which again correspond closely to the 95% CI obtained by bootstrapping (14.9, 20.0) as well as
                  the 95% CI obtained through traditional approaches (14.6 and 20.0; <a href="#figure5">Figure 5A</a>). Thus, at some level, bootstrapping dovetails with some of the concepts covered in earlier sections, such as the idea of
                  obtaining normal distributions through theoretical repeated sampling (<a href="#sec2-6" title="2.6.&nbsp;Are the data normal enough?">Section 2.6</a>). In fact, one of the methods to check the validity of data obtained from bootstrap methods is to generate histograms such
                  as the one in <a href="#figure20">Figure 20</a>.
               </p>
               <div class="mediaobject" align="center"><a name="figure20"></a><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="432">
<tr>
  <td align="center"><p><a href="statisticalanalysis_fig20.jpg"><img src="statisticalanalysis_fig20_s.jpg" border="2" align="middle" alt=" figure 20"></a>
  </td>
</tr>
                  </table>
                  <div class="caption">
                     <p><b>Figure 20. Bootstrapped sampling distribution of the mean.&nbsp;</b>A normal curve (blue line) has been inserted for reference. Red vertical lines indicate 95% CI boundaries (&#8764;2 SDs to either
                        side of the mean)
                     </p>
                  </div>
               </div>
               <p>Re-sampling methods can also be applied to test for differences in means between two independent samples, a process generically
                  referred to as <span class="emphasis"><em>permutation tests</em></span>. In this case, data from the two samples are first combined into one set. For example, samples with 22 and 58 data points
                  would be combined to give a single sample of 80. Next, <span class="emphasis"><em>re-sampling without replacement</em></span> is carried out to generate two new samples of size 22 and 58. Because the re-sampling is done without regard to which group
                  the original data points came from, this will result in a random re-shuffling of the data points into the two groups. Next,
                  means are obtained for each of the new sample sets and the difference between these means is calculated. Round one is now
                  done. The original set of 80 is then resampled thousands more times. A <span class="emphasis"><em>P</em></span>-value for the difference between means is then derived by noting the proportion of times that the two resampled sets gave
                  mean differences that were as large or larger than the difference between the original datasets. If a <span class="emphasis"><em>P</em></span>-value that was &#8804; 0.05 resulted, the difference would typically be deemed statistically significant. Thus, much like the bootstrap
                  calculation of the 95% CI, the re-sampling method is actually more simple and intuitive to grasp then the classical statistical
                  methods, which rely heavily on theory<sup><a name="fn70" href="#ftn.fn70">70</a></sup>.
               </p>
               <p>Because bootstrapping takes a single experiment with a limited sample size and effectively turns it into many thousands of
                  experiments, some skeptical scientists may suspect bootstrapping to constitute a form of cheating. Of course this isn't true.
                  That said, whenever you make any inference regarding the population as a whole from a single sample of that population, some
                  assumptions are required. In the case of using classical statistical methods, such as those that rely on the <span class="emphasis"><em>t</em></span>, <span class="emphasis"><em>z</em></span>, and other distributions, we are assuming that the sample statistic that we are estimating has a roughly normal distribution.
                  Moreover, the conclusions we derive using these methods are based on the impossible scenario that we carry out the experiment
                  an infinite number of times. Thus, although we tend to accept these standard methods, assumptions are certainly inherent.
                  Bootstrapping is no different. Like the classical methods, it uses the sample data to estimate population parameters. However,
                  instead of using theoretical distributions, such as <span class="emphasis"><em>z</em></span> and <span class="emphasis"><em>t</em></span>, we use re-sampling of the data to predict the behavior of the sample statistic. Moreover, in the case of bootstrapping,
                  we don't have to assume that the distribution of our statistic is normal. Therefore, bootstrapping actually requires fewer
                  assumptions than classical methods and is consequently more reliable. Of course, if the experiment wasn't conducted well (non-random
                  or inaccurate sampling methods) or if the numbers are very low, then neither classical nor bootstrapping methods will give
                  reliable results. Statistics cannot validate or otherwise rescue a scientifically flawed experiment.
               </p>
            </div>
            <div class="footnotes"><br><hr width="100" align="left">
               <div class="footnote">
                  <p><sup><a name="ftn.fn60" href="#fn60">60</a></sup>An online document describing this issue is available at: firstclinical.com/journal/2007/0703_Power.pdf. In addition, a recent
                     critical analysis of this issue is provided by <a href="#bib4">Bacchetti (2010)</a>.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn61" href="#fn61">61</a></sup>The median is essentially a trimmed mean where the trimming approaches 100%!
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn62" href="#fn62">62</a></sup>More accurately, the tests assume that the populations are normal enough and that the sample size is large enough such that
                     the distribution of the calculated statistic itself will be normal. This was discussed in <a href="#sec1" title="1.&nbsp;The basics">Section 1</a>.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn63" href="#fn63">63</a></sup>Note that there are subtle variations on this theme, which (depending on the text or source) may go by the same name. These
                     can be used to test for differences in additional statistical parameters such the median.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn64" href="#fn64">64</a></sup>More accurately, nonparametric tests will be less powerful than parametric tests if both tests were to be simultaneously carried
                     out on a dataset that was normal. The diminished power of nonparametric tests in these situations is particularly exacerbated
                     if sample sizes are small. Obviously, if the data were indeed normal, one would hopefully be aware of this and would apply
                     a parametric test. Conversely, nonparametric tests can actually be more powerful than parametric tests when applied to data
                     that are truly non-Gaussian. Of course, if the data are far from Guassian, then the parametric tests likely wouldn't even
                     be valid. Thus, each type of test is actually &#8220;better&#8221; or &#8220;best&#8221; when it is used for its intended purpose.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn65" href="#fn65">65</a></sup>For example, strains A and B might have six and twelve animals remaining on day 5, respectively. If a total of three animals
                     died by the next day (two from strain A and one from strain B) the expected number of deaths for strain B would be twice that
                     of A, since the population on day 5 was twice that of strain A. Namely, two deaths would be expected for strain B and one
                     for strain A. Thus, the difference between expected and observed deaths for strains A and B would be 1 (2&#8722;1=1) and &#8722;1 (1&#8722;2=&#8722;1),
                     respectively.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn66" href="#fn66">66</a></sup>The final calculation also takes into account sample size and the variance for each sample.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn67" href="#fn67">67</a></sup>Note that, although not as often used, there are also parametric bootstrapping methods.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn68" href="#fn68">68</a></sup>This is a bad idea in practice. For some statistical parameters, such as SE, several hundred repetitions may be sufficient
                     to give reliable results. For others, such as CIs, several thousand or more repetitions may be necessary. Moreover, because
                     it takes a computer only two more seconds to carry out 4,000 repetitions than it takes for 300, there is no particular reason
                     to scrimp.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn69" href="#fn69">69</a></sup>Note that this version of the procedure, the percentile bootstrap, differs slightly from the standard bootstrapping method,
                     the bias corrected and accelerated bootstrap (BCa). Differences are due to a potential for slight bias in the percentile bootstrapping
                     procedure that are not worth discussing in this context. Also, don't be unduly put off by the term &#8220;bias&#8221;. SD is also a &#8220;biased&#8221;
                     statistical parameter, as are many others. The BCa method compensates for this bias and also adjusts for skewness when necessary.
                  </p>
               </div>
               <div class="footnote">
                  <p><sup><a name="ftn.fn70" href="#fn70">70</a></sup>A brief disclaimer. Like everything else in statistics, there are some caveats to bootstrapping along with limitations and
                     guidelines that one should become familiar with before diving into the deep end.
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec8"></a>7.&nbsp;Acknowledgments
                     </h2>
                  </div>
               </div>
            </div>
            <p>We greatly appreciate input from Naomi Ward on the contents of this review. We are particularly indebted to Amy Fluet for
               editing this ungainly monolith and providing much useful critique. We also thank Oliver Walter for encouraging this project
               and both anonymous reviewers for taking on this task and providing constructive comments. This work was supported by NIH grant
               GM066868.
            </p>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec9"></a>8.&nbsp;References
                     </h2>
                  </div>
               </div>
            </div>
            <p></p>
            <div class="bibliography">
               <div class="bibliomixed"><a name="bib1"></a><p class="bibliomixed"><span class="bibliomisc">Agarwal, P., and States, D.J. (1996). A Bayesian evolutionary distance for parametrically aligned sequences. J. Comput. Biol.
                        <span class="emphasis"><em>3</em></span>, 1&#8211;17.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=8697232&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1089/cmb.1996.3.1" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib2"></a><p class="bibliomixed"><span class="bibliomisc">Agresti, A., and Coull, B.A. (1998). Approximate is better than Exact for Interval Estimation of Binomial Proportions. The
                        American Statistician <span class="emphasis"><em>52</em></span>, 119&#8211;126.</span>
                     <a href="http://www.tandfonline.com/doi/abs/10.1080/00031305.1998.10480550#.UaBymOuJTFc" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib3"></a><p class="bibliomixed"><span class="bibliomisc">Agresti, A., and Caffo, B. (2000). Simple and effective confidence intervals for proportions and differences of proportions
                        result from adding two successes and two failures. The American Statistician <span class="emphasis"><em>54</em></span>, 280&#8211;288.</span>
                     <a href="http://www.tandfonline.com/doi/abs/10.1080/00031305.2000.10474560#.UaByFeuJTFc" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib4"></a><p class="bibliomixed"><span class="bibliomisc">Bacchetti P. (2010). Current sample size conventions: flaws, harms, and alternatives. BMC Med. <span class="emphasis"><em>8</em></span>, 17.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=20307281&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1186/1741-7015-8-17" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib5"></a><p class="bibliomixed"><span class="bibliomisc">Benjamini, Y., and Hochberg. Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple
                        testing. J. Roy. Statist. Soc. Ser. B Stat. Methodol. <span class="emphasis"><em>57</em></span>, pp. 289&#8211;300.</span>
                     <a href="http://www.stat.purdue.edu/%E2%88%BCdoerge/BIOINFORM.D/FALL06/Benjamini%20and%20Y%20FDR.pdf" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib6"></a><p class="bibliomixed"><span class="bibliomisc">Brown, L.D., Cai, T., and DasGupta, A. (2001). Interval estimation for a binomial proportion. Statist. Sci. <span class="emphasis"><em>16</em></span>, 101&#8211;133.</span>
                     <a href="http://www.ic.unicamp.br/%E2%88%BCwainer/cursos/2s2009/1009213286.pdf" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib7"></a><p class="bibliomixed"><span class="bibliomisc">Burge, C., Campbell, A.M., and Karlin, S. (1992). Over- and under-representation of short oligonucleotides in DNA sequences.
                        Proc. Natl. Acad. Sci. U. S. A. <span class="emphasis"><em>89</em></span>, 1358-1362.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=1741388&amp;dopt=Abstract" target="_blank">Abstract</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib8"></a><p class="bibliomixed"><span class="bibliomisc">Carroll, P.M., Dougherty, B., Ross-Macdonald, P., Browman, K., and FitzGerald, K. (2003) Model systems in drug discovery:
                        chemical genetics meets genomics. Pharmacol. Ther. <span class="emphasis"><em>99</em></span>, 183-220.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=12888112&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/http://dx.doi.org/10.1016/S0163-7258(03)00059-7" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib9"></a><p class="bibliomixed"><span class="bibliomisc">Doitsidou, M., Flames, N., Lee, A.C., Boyanov, A., and Hobert, O. (2008). Automated screening for mutants affecting dopaminergic-neuron
                        specification in <span class="emphasis"><em>C. elegans</em></span>. Nat. Methods <span class="emphasis"><em>10</em></span>, 869&#8211;72.</span>
                     <a href="http://dx.doi.org/10.1038/nmeth.1250" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib10"></a><p class="bibliomixed"><span class="bibliomisc">Gassmann, M., Grenacher, B., Rohde, B., and Vogel, J. (2009). Quantifying Western blots: pitfalls of densitometry. Electrophoresis
                        <span class="emphasis"><em>11</em></span>, 1845&#8211;1855.</span>
                     <a href="http://dx.doi.org/10.1002/elps.200800720" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib11"></a><p class="bibliomixed"><span class="bibliomisc">Houser, J. (2007). How many are enough? Statistical power analysis and sample size estimation in clinical research. J. Clin.
                        Res. Best Pract. <span class="emphasis"><em>3</em></span>, 1-5.</span>
                     <a href="http://firstclinical.com/journal/2007/0703_Power.pdf" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib12"></a><p class="bibliomixed"><span class="bibliomisc">Hoogewijs, D., De Henau, S., Dewilde, S., Moens, L., Couvreur, M., Borgonie, G., Vinogradov, S.N., Roy, S.W., and Vanfleteren,
                        J.R. (2008). The <span class="emphasis"><em>Caenorhabditis</em></span> globin gene family reveals extensive nematode-specific radiation and diversification. BMC Evol. Biol. <span class="emphasis"><em>8</em></span>, 279.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=18844991&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1186/1471-2148-8-279" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib13"></a><p class="bibliomixed"><span class="bibliomisc">Jones, L.V., and Tukey, J. (2000). A sensible formulation of the significance test. Psychol. Methods <span class="emphasis"><em>5</em></span>, 411&#8211;414.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=11194204&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/ 10.1037/1082-989X.5.4.411" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib14"></a><p class="bibliomixed"><span class="bibliomisc">Kamath, R.S., Fraser, A.G., Dong, Y., Poulin, G., Durbin, R., Gotta, M., Kanapin, A., Le Bot, N., Moreno, S., Sohrmann, M.,
                        Welchman, D.P., Zipperlen, P., and Ahringer, J. (2003). Systematic functional analysis of the <span class="emphasis"><em>Caenorhabditis elegans</em></span> genome using RNAi. Nature <span class="emphasis"><em>16</em></span>, 231&#8211;237.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=12529635&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1038/nature01278" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib15"></a><p class="bibliomixed"><span class="bibliomisc">Knill, D.C, and Pouget, A. (2004). The Bayesian brain: the role of uncertainty in neural coding and computation. Trends Neurosci.
                        <span class="emphasis"><em>12</em></span>, 712-719.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=15541511&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/http://dx.doi.org/10.1016/j.tins.2004.10.007" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib16"></a><p class="bibliomixed"><span class="bibliomisc">Nagele, P. (2003). Misuse of standard error of the mean (SEM) when reporting variability of a sample. A critical evaluation
                        of four anaesthesia journals. Br. J. Anaesth. <span class="emphasis"><em>4</em></span>, 514-516.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=12644429&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1093/bja/aeg087" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib17"></a><p class="bibliomixed"><span class="bibliomisc">Ramsey, F.L. and Schafer, D.W. (2013). The Statistical Sleuth: a Course in Methods of Data Analysis, Third Edition. (Boston:
                        Brooks/Cole Cengage Learning).</span>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib18"></a><p class="bibliomixed"><span class="bibliomisc">Shaham, S. 2007. Counting mutagenized genomes and optimizing genetic screens in <span class="emphasis"><em>Caenorhabditis elegans</em></span>. PLoS One <span class="emphasis"><em>2</em></span>, e1117.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=17989770&amp;dopt=Abstract" target="_blank">Abstract</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib19"></a><p class="bibliomixed"><span class="bibliomisc">Suciu, G.P., Lemeshow, S., and Moeschberger, M. (2004). Hand Book of Statistics, Volume 23: Advances in Survival Analysis,
                        N. Balakrishnan and C.R. Rao, eds. (Amsterdam: Elsevier B. V.), pp. 251-261.</span>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib20"></a><p class="bibliomixed"><span class="bibliomisc">Sulston, J.E., and Horvitz, H.R. (1977). Post-embryonic cell lineages of the nematode, <span class="emphasis"><em>Caenorhabditis elegans</em></span>. Dev. Biol. <span class="emphasis"><em>56</em></span>, 110&#8211;156.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=838129&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/http://dx.doi.org/10.1016/0012-1606(77)90158-0" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib21"></a><p class="bibliomixed"><span class="bibliomisc">Sun, X., and Hong, P. (2007). Computational modeling of <span class="emphasis"><em>Caenorhabditis elegans</em></span> vulval induction. Bioinformatics <span class="emphasis"><em>23</em></span>, i499-507.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=17646336&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1093/bioinformatics/btm214" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib22"></a><p class="bibliomixed"><span class="bibliomisc">Vilares, I., and Kording, K. (2011). Bayesian models: the structure of the world, uncertainty, behavior, and the brain. Ann.
                        N.Y. Acad. Sci. <span class="emphasis"><em>1224</em></span>, 22&#8211;39.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=21486294&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1111/j.1749-6632.2011.05965.x" target="_blank">Article</a>
                     
                  </p>
               </div>
               <div class="bibliomixed"><a name="bib23"></a><p class="bibliomixed"><span class="bibliomisc">Zhong, W., and Sternberg, P.W. (2006). Genome-wide prediction of <span class="emphasis"><em>C. elegans</em></span> genetic interactions. Science <span class="emphasis"><em>311</em></span>, 1481&#8211;1484.</span>
                     <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=16527984&amp;dopt=Abstract" target="_blank">Abstract</a>
                     <a href="http://dx.doi.org/10.1126/science.1123287" target="_blank">Article</a>
                     
                  </p>
               </div>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec10"></a>9.&nbsp;Appendix A: Microsoft Excel tools
                     </h2>
                  </div>
               </div>
            </div>
            <p>Note: these tools require the Microsoft Windows operating system. Information about running Windows on a Mac (Apple Inc.)
               can be found at <a href="http://store.apple.com/us/browse/guide/windows" target="_top">http://store.apple.com/us/browse/guide/windows</a>.
            </p>
            <div class="orderedlist">
               <ol type="1">
                  <li>
                     <p><span class="bold"><strong><a href="PowercalcTool1mean.xls" target="_blank">PowercalcTool 1mean.xls</a></strong></span></p>
                  </li>
                  <li>
                     <p><span class="bold"><strong><a href="PowercalcTool2mean.xls" target="_blank">PowercalcTool 2mean.xls</a></strong></span></p>
                  </li>
                  <li>
                     <p><span class="bold"><strong><a href="PowercalcToolprop.xls" target="_blank">PowercalcTool prop.xls</a></strong></span></p>
                  </li>
                  <li>
                     <p><span class="bold"><strong><a href="RatiosTool.xls" target="_blank">RatiosTool.xls</a></strong></span></p>
                  </li>
               </ol>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec11"></a>10.&nbsp;Appendix B: Recomended reading
                     </h2>
                  </div>
               </div>
            </div>
            <div class="orderedlist">
               <ol type="1">
                  <li>
                     <p><span class="bold"><strong>Motulsky, H. (2010). Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking, Second Edition (New York: Oxford
                              University Press).</strong></span></p>
                     <p>This is <span class="emphasis"><em>easily</em></span> my favorite biostatistics book. It covers a lot of ground, explains the issues and concepts clearly, and provides lots of
                        good practical advice. It's also <span class="emphasis"><em>very</em></span> readable. Some may want more equations or theory, but for what it is, it's excellent. (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>Triola M. F., and Triola M.M. (2006). Biostatistics for the Biological and Health Sciences (Boston:</strong></span> <span class="bold"><strong>Pearson Addison-Wesley).</strong></span></p>
                     <p>This is a comprehensive and admirably readable conventional statistical text book aimed primarily at advanced undergraduates
                        and beginning graduate students. It provides <span class="emphasis"><em>many</em></span> great illustrations of the concepts as well as tons of worked examples. The main author (M. F. Triola) has been writing statistical
                        texts for many years and has honed his craft well. This particular version is very similar to Elementary Statistics (10<sup>th</sup> edition) by the same author. (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>Gonick, L. and Smith, W. (1993). The Cartoon Guide to Statistics (New York: HarperPerennial).</strong></span></p>
                     <p>Don't let the name fool you. This book is quite dense and contains the occasional calculus equation. It's an enjoyable read
                        and explains some concepts very well. The humor is corny, but provides some needed levity. (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>van Emden, H. (2008). Statistics for Terrified Biologists (Malden, MA: Blackwell Publishing).</strong></span></p>
                     <p>This paperback explains certain concepts in statistics very well. In particular, its treatment of the theory behind the T
                        test, statistical correlation, and ANOVA is excellent. The downside is that the author operates under the premise that calculations
                        are still done by hand and so uses many shortcut formulas (algebraic variations) that tend to obscure what's really going
                        on. In addition, almost half of the text is dedicated to ANOVA, which may not be hugely relevant for the worm field. (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>Sokal R. R., and Rohlf, F. J. (2012). Biometry: The Principles and Practice of Statisticsin Biological Research, Fourth Edition
                              (New York: W. H. Freeman and Co.).</strong></span></p>
                     <p>One of the &#8220;bibles&#8221; in the field of biostatistics. Weighing in at 937 pages, this book is chock full of information. Unfortunately,
                        like many holy texts, this book will not be that easy for most of us to read and digest. (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong><a href="#bib17">Ramsey, F.L. and Schafer, D.W. (2013)</a>. The Statistical Sleuth: a Course in Methods of Data Analysis, Third Edition. (Boston: Brooks/Cole Cengage Learning).</strong></span></p>
                     <p>Contemporary statistics for a mature audience (meaning those who do real research), done with lots of examples and explanation,
                        and minimal math. (KG)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>Fay, D.S., and Gerow, K. (2013). A biologist's guide to statistical thinking and analysis.</strong></span> <span class="bold"><strong><span class="emphasis"><em>WormBook</em></span></strong></span><span class="bold"><strong>, ed. The</strong></span> <span class="bold"><strong><span class="emphasis"><em>C. elegans</em></span></strong></span> <span class="bold"><strong>Research Community, WormBook,</strong></span></p>
                     <p>Quite simply a tour de farce, written by two authors with impeachable credentials and a keen eye for misguiding na&iuml;ve biologists.</p>
                  </li>
               </ol>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec12"></a>11.&nbsp;Appendix C: Useful programs for statistical calculations
                     </h2>
                  </div>
               </div>
            </div>
            <div class="orderedlist">
               <ol type="1">
                  <li>
                     <p><span class="bold"><strong><span class="emphasis"><em>Minitab</em></span></strong></span></p>
                     <p>A comprehensive and reasonably intuitive program. This program has many useful features and is frequently updated. The downside
                        for Mac users is that you will have to either use a PC (unthinkable) or run it on parallels (or some similar program) that
                        lets you employ a PC interface on your Mac (shudder). (DF)
                     </p>
                  </li>
                  <li>
                     <p><span class="bold"><strong>Microsoft Excel</strong></span></p>
                     <p>Excel excels at doing arithmetic, but is not meant to be a statistics package. It does most simple things (t-tests) sort of
                        reasonably well, but the interface is clunky. Minitab (or any other stats package) would be preferred. (KG)
                     </p>
                  </li>
               </ol>
            </div>
         </div>
         <div class="sect1" lang="en">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title" style="clear: both"><a name="sec13"></a>12.&nbsp;Appendix D: Useful websites for statistical calculations
                     </h2>
                  </div>
               </div>
            </div>
            <p>There are many such sites, which have the advantage of being free and generally easy to use. Of course, what is available
               at the time of this writing could change rapidly and without notice. As with anything else that's free on the web, rely on
               at your own risk!
            </p>
            <div class="orderedlist">
               <ol type="1">
                  <li>
                     <p><a href="http://easycalculation.com/statistics/statistics.php" target="_top">http://easycalculation.com/statistics/statistics.php</a></p>
                  </li>
                  <li>
                     <p><a href="http://www.graphpad.com/quickcalcs/" target="_top">http://www.graphpad.com/quickcalcs/</a></p>
                  </li>
                  <li>
                     <p><a href="http://stattrek.com/" target="_top">http://stattrek.com/</a></p>
                  </li>
                  <li>
                     <p><a href="http://www.numberempire.com/statisticscalculator.php" target="_top">http://www.numberempire.com/statisticscalculator.php</a></p>
                  </li>
                  <li>
                     <p><a href="http://www.danielsoper.com/statcalc3/default.aspx" target="_top">http://www.danielsoper.com/statcalc3/default.aspx</a></p>
                  </li>
                  <li>
                     <p><a href="http://vassarstats.net/" target="_top">http://vassarstats.net/</a></p>
                  </li>
               </ol>
            </div>
         </div>
         <div class="footnotes"><br><hr width="100" align="left">
            <div class="footnote">
               <p><sup><a name="ftn.d0e6" href="#d0e6">*</a></sup>Edited by Oliver Hobert. Last revised January 14, 2013, Published July 9, 2013. This chapter should be cited as: Fay D.S.
                  and Gerow K. A biologist's guide to statistical thinking and analysis (July 9, 2013), <span class="emphasis"><em>WormBook</em></span>, ed. The <span class="emphasis"><em>C. elegans</em></span> Research Community, WormBook, doi/10.1895/wormbook.1.159.1, <a href="http://www.wormbook.org" target="_top">http://www.wormbook.org</a>.
               </p>
               <p><span class="bold"><strong>Copyright:</strong></span> &copy; 2013 David S. Fay and Ken Gerow. This is an open-access article distributed under the terms of the Creative Commons Attribution
                  License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source
                  are credited.
               </p>
            </div>
            <div class="footnote">
               <p><sup><a name="ftn.d0e29" href="#d0e29">&sect;</a></sup>To whom correspondence should be addressed. Email: <a href="mailto:davidfay@uwyo.edu" target="_top">davidfay@uwyo.edu</a></p>
	   <p><img src="somerights20.gif" alt="Creative Commons License" align="middle" border="0"> All WormBook content, except where otherwise noted, is licensed under a <a href="http://creativecommons.org/licenses/by/2.5/" title="Creative Commons Attribution License" target="_blank">Creative Commons Attribution License</a>.</p>
            </div>

   <!--#include virtual="/ssi/footer.html" --></body>
</html>